{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \napproved_list = [\"adhd_2018_features_tfidf_256.csv\",\n                \"bpd_2019_features_tfidf_256.csv\",\n                \"schizophrenia_2019_features_tfidf_256.csv\",\n                \"bipolarreddit_2018_features_tfidf_256.csv\",\n                \"anxiety_2019_features_tfidf_256.csv\",\n                \"depression_2019_features_tfidf_256.csv\",\n                \"EDAnonymous_2019_features_tfidf_256.csv\"]\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-27T23:35:42.106755Z","iopub.execute_input":"2023-07-27T23:35:42.107114Z","iopub.status.idle":"2023-07-27T23:35:42.163148Z","shell.execute_reply.started":"2023-07-27T23:35:42.107086Z","shell.execute_reply":"2023-07-27T23:35:42.162261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade gensim","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:35:42.164683Z","iopub.execute_input":"2023-07-27T23:35:42.165801Z","iopub.status.idle":"2023-07-27T23:35:52.661262Z","shell.execute_reply.started":"2023-07-27T23:35:42.165764Z","shell.execute_reply":"2023-07-27T23:35:52.660561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nsp = spacy.load(\"en_core_web_sm\")\nimport os \nimport csv\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom IPython.display import display, HTML\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nimport gensim\nfrom scipy import spatial\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.decomposition import PCA\nimport re\nfrom statistics import mean\n# to make our plot outputs appear and be stored within the notebook:\n%matplotlib inline ","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:35:52.662426Z","iopub.execute_input":"2023-07-27T23:35:52.662798Z","iopub.status.idle":"2023-07-27T23:36:07.849704Z","shell.execute_reply.started":"2023-07-27T23:35:52.662765Z","shell.execute_reply":"2023-07-27T23:36:07.848278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"posts_dict = {}\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename in approved_list:\n            curr_df = pd.read_csv('/kaggle/input/mental-health-support-feature-analysis/' + filename)\n#             print(curr_df.columns)\n#             print(curr_df.post)\n            print(len(list(curr_df.post)))\n            disorder = filename.split(\"_\")[0]\n            print(\"DISORDER: \" + disorder)\n            if disorder == \"bipolarreddit\":\n                disorder = \"bipolar\"\n            elif disorder == \"EDAnonymous\":\n                disorder = \"eating_disorder\"\n            posts_dict[disorder] = list(curr_df.post)\n    \nprint(\"FINISHED LOOPING THROUGH FILES\")\nfor k in posts_dict.keys():\n    print(f\"Key: {k}\\nLength: {len(posts_dict[k])}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:36:07.851694Z","iopub.execute_input":"2023-07-27T23:36:07.852496Z","iopub.status.idle":"2023-07-27T23:36:12.197292Z","shell.execute_reply.started":"2023-07-27T23:36:07.852468Z","shell.execute_reply":"2023-07-27T23:36:12.196253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['disorder', 'post'])\ncount = 0\nfor k in posts_dict.keys():\n    for post in posts_dict[k]:\n        df.loc[count] = [k, post]\n        count += 1\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:36:12.198552Z","iopub.execute_input":"2023-07-27T23:36:12.199526Z","iopub.status.idle":"2023-07-27T23:38:17.099986Z","shell.execute_reply.started":"2023-07-27T23:36:12.199473Z","shell.execute_reply":"2023-07-27T23:38:17.098722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head)\nshortened_set = df.sample(frac=0.3)\nshortened_set = shortened_set.reset_index().drop('index', axis=1)\nprint(shortened_set.head)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:38:17.101613Z","iopub.execute_input":"2023-07-27T23:38:17.101913Z","iopub.status.idle":"2023-07-27T23:38:17.131871Z","shell.execute_reply.started":"2023-07-27T23:38:17.101888Z","shell.execute_reply":"2023-07-27T23:38:17.131080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortened_set['post'] = [sp(shortened_set['post'].iloc[i]) for i in range(shortened_set.shape[0])]\ndisplay(shortened_set.head)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:38:17.132817Z","iopub.execute_input":"2023-07-27T23:38:17.133625Z","iopub.status.idle":"2023-07-27T23:46:32.871711Z","shell.execute_reply.started":"2023-07-27T23:38:17.133595Z","shell.execute_reply":"2023-07-27T23:46:32.870351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortened_set['tokens'] = [[word.text for word in shortened_set['post'].iloc[i]] for i in range(shortened_set.shape[0])]","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:46:32.872994Z","iopub.execute_input":"2023-07-27T23:46:32.873233Z","iopub.status.idle":"2023-07-27T23:46:35.143240Z","shell.execute_reply.started":"2023-07-27T23:46:32.873212Z","shell.execute_reply":"2023-07-27T23:46:35.142052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortened_set['lemmas'] = [[word.lemma_ for word in shortened_set['post'].iloc[i]] for i in range(shortened_set .shape[0])]\ndisplay(shortened_set)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:46:35.146689Z","iopub.execute_input":"2023-07-27T23:46:35.147133Z","iopub.status.idle":"2023-07-27T23:46:37.076479Z","shell.execute_reply.started":"2023-07-27T23:46:35.147100Z","shell.execute_reply":"2023-07-27T23:46:37.075172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(shortened_set.shape[0]):\n    shortened_set['lemmas'].iloc[i] = [t for t in shortened_set['lemmas'].iloc[i] if t not in '''!()-[]{};:\\'\"\\,<>./?@#$%^&*_~\\|–—\\“’`''']# and t.isalpha() ]\nshortened_set","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:46:37.078084Z","iopub.execute_input":"2023-07-27T23:46:37.078475Z","iopub.status.idle":"2023-07-27T23:46:42.299040Z","shell.execute_reply.started":"2023-07-27T23:46:37.078443Z","shell.execute_reply":"2023-07-27T23:46:42.298150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nshortened_set[\"lemmas_no_stop\"] = ''\nfor i in range(shortened_set.shape[0]):\n    shortened_set['lemmas_no_stop'].iloc[i] = [t for t in shortened_set['lemmas'].iloc[i] if t.lower() not in stop_words and t.lower() not in [\"i\", \"is\", \"be\", \"am\", \"are\", \"the\", \"reddit\", \"i'm\", \"i’m\"]]\nshortened_set","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:46:42.300262Z","iopub.execute_input":"2023-07-27T23:46:42.301173Z","iopub.status.idle":"2023-07-27T23:46:47.766394Z","shell.execute_reply.started":"2023-07-27T23:46:42.301145Z","shell.execute_reply":"2023-07-27T23:46:47.765363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_vocab(model, top_n = None):\n    count = 0\n    if top_n is not None:\n        for index, word in enumerate(model.wv.index_to_key):\n            count+= 1\n            if count < top_n:\n                print(f\"WORD #{index}/{len(model.wv.index_to_key)} IS: {word}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:46:47.767459Z","iopub.execute_input":"2023-07-27T23:46:47.767781Z","iopub.status.idle":"2023-07-27T23:46:47.772479Z","shell.execute_reply.started":"2023-07-27T23:46:47.767754Z","shell.execute_reply":"2023-07-27T23:46:47.771721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbow_models = {}\n# 100 dimensions\ncbow_w3_f1_100 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_100\"] = cbow_w3_f1_100\ncbow_w5_f1_100 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_100\"] = cbow_w5_f1_100\ncbow_w10_f1_100 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_100\"] = cbow_w10_f1_100\ncbow_w3_f1_100_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_100_mc0\"] = cbow_w3_f1_100_mc0\ncbow_w5_f1_100_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=100, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_100_mc0\"] = cbow_w5_f1_100_mc0\ncbow_w10_f1_100_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_100_mc0\"] = cbow_w10_f1_100_mc0\ncbow_w10_f1_100_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_100_neg2\"] = cbow_w10_f1_100_neg2\ncbow_w10_f1_100_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_100_neg5\"] = cbow_w10_f1_100_neg5\ncbow_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_100_ns_half_neg2\"] = cbow_w10_f1_100_ns_half_neg2\ncbow_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_100_ns_half_neg5\"] = cbow_w10_f1_100_ns_half_neg5\ncbow_w10_f1_100_neg2_ws2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_100_neg2_ws2\"] = cbow_w10_f1_100_neg2_ws2\ncbow_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=100, window = 5, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_100_neg5_ws5\"] = cbow_w10_f1_100_neg5_ws5\nprint(\"FINISHED TRAINING 100 DIMENSION---WHITE\")\n\n# 300 dimensions\ncbow_w3_f1_300 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_300\"] = cbow_w3_f1_300\ncbow_w5_f1_300 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_300\"] = cbow_w5_f1_300\ncbow_w10_f1_300 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_300\"] = cbow_w10_f1_300\ncbow_w3_f1_300_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_300_mc0\"] = cbow_w3_f1_300_mc0\ncbow_w5_f1_300_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=300, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_300_mc0\"] = cbow_w5_f1_300_mc0\ncbow_w10_f1_300_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_300_mc0\"] = cbow_w10_f1_300_mc0\ncbow_w10_f1_300_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_300_neg2\"] = cbow_w10_f1_300_neg2\ncbow_w10_f1_300_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_300_neg5\"] = cbow_w10_f1_300_neg5\ncbow_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_300_ns_half_neg2\"] = cbow_w10_f1_300_ns_half_neg2\ncbow_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_300_ns_half_neg5\"] = cbow_w10_f1_300_ns_half_neg5\ncbow_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_300_neg2_ws2\"] = cbow_w10_f1_300_neg2_ws2\ncbow_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=300, window = 5, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_300_neg5_ws5\"] = cbow_w10_f1_300_neg5_ws5\nprint(\"FINISHED TRAINING 300 DIMENSION---WHITE\")\n\n# 500 dimensions\ncbow_w3_f1_500 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_500\"] = cbow_w3_f1_500\ncbow_w5_f1_500 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_500\"] = cbow_w5_f1_500\ncbow_w10_f1_500 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_500\"] = cbow_w10_f1_500\ncbow_w3_f1_500_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_500_mc0\"] = cbow_w3_f1_500_mc0\ncbow_w5_f1_500_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=500, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_500_mc0\"] = cbow_w5_f1_500_mc0\ncbow_w10_f1_500_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_500_mc0\"] = cbow_w10_f1_500_mc0\ncbow_w10_f1_500_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_500_neg2\"] = cbow_w10_f1_500_neg2\ncbow_w10_f1_500_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_500_neg5\"] = cbow_w10_f1_500_neg5\ncbow_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_500_ns_half_neg2\"] = cbow_w10_f1_500_ns_half_neg2\ncbow_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_500_ns_half_neg5\"] = cbow_w10_f1_500_ns_half_neg5\ncbow_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_500_neg2_ws2\"] = cbow_w10_f1_500_neg2_ws2\ncbow_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=500, window = 5, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_500_neg5_ws5\"] = cbow_w10_f1_500_neg5_ws5\nprint(\"FINISHED TRAINING 500 DIMENSION---WHITE\")\n\n# 700 dimensions\ncbow_w3_f1_700 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_700\"] = cbow_w3_f1_700\ncbow_w5_f1_700 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_700\"] = cbow_w5_f1_700\ncbow_w10_f1_700 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_700\"] = cbow_w10_f1_700\ncbow_w3_f1_700_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_700_mc0\"] = cbow_w3_f1_700_mc0\ncbow_w5_f1_700_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=700, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_700_mc0\"] = cbow_w5_f1_700_mc0\ncbow_w10_f1_700_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_700_mc0\"] = cbow_w10_f1_700_mc0\ncbow_w10_f1_700_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_700_neg2\"] = cbow_w10_f1_700_neg2\ncbow_w10_f1_700_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_700_neg5\"] = cbow_w10_f1_700_neg5\ncbow_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_700_ns_half_neg2\"] = cbow_w10_f1_700_ns_half_neg2\ncbow_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_700_ns_half_neg5\"] = cbow_w10_f1_700_ns_half_neg5\ncbow_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_700_neg2_ws2\"] = cbow_w10_f1_700_neg2_ws2\ncbow_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=700, window = 5, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_700_neg5_ws5\"] = cbow_w10_f1_700_neg5_ws5\nprint(\"FINISHED TRAINING 700 DIMENSION---WHITE\")\n\n# 1000 dimensions\ncbow_w3_f1_1000 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_1000\"] = cbow_w3_f1_1000\ncbow_w5_f1_1000 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_1000\"] = cbow_w5_f1_1000\ncbow_w10_f1_1000 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_1000\"] = cbow_w10_f1_1000\ncbow_w3_f1_1000_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 0)\ncbow_models[\"cbow_w3_f1_1000_mc0\"] = cbow_w3_f1_1000_mc0\ncbow_w5_f1_1000_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=1000, window = 5, sg = 0)\ncbow_models[\"cbow_w5_f1_1000_mc0\"] = cbow_w5_f1_1000_mc0\ncbow_w10_f1_1000_mc0 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 0)\ncbow_models[\"cbow_w10_f1_1000_mc0\"] = cbow_w10_f1_1000_mc0\ncbow_w10_f1_1000_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_1000_neg2\"] = cbow_w10_f1_1000_neg2\ncbow_w10_f1_1000_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_1000_neg5\"] = cbow_w10_f1_1000_neg5\ncbow_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_1000_ns_half_neg2\"] = cbow_w10_f1_1000_ns_half_neg2\ncbow_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.5)\ncbow_models[\"cbow_w10_f1_1000_ns_half_neg5\"] = cbow_w10_f1_1000_ns_half_neg5\ncbow_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=2, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_1000_neg2_ws2\"] = cbow_w10_f1_1000_neg2_ws2\ncbow_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(shortened_set['lemmas_no_stop'], min_count=1, vector_size=1000, window = 5, sg = 0, negative=5, ns_exponent=0.75)\ncbow_models[\"cbow_w10_f1_1000_neg5_ws5\"] = cbow_w10_f1_1000_neg5_ws5\nprint(\"FINISHED TRAINING 1000 DIMENSION---WHITE\")\nprint(\"FINISHED TRAINING ALL WORD2VEC MODELS\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T23:46:47.773673Z","iopub.execute_input":"2023-07-27T23:46:47.773895Z","iopub.status.idle":"2023-07-28T00:00:10.258141Z","shell.execute_reply.started":"2023-07-27T23:46:47.773873Z","shell.execute_reply":"2023-07-28T00:00:10.257184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"synonyms = dict({\"smart\": \"intelligent\", \"happy\": \"cheerful\", \"afraid\": \"anxious\", \"large\": \"big\", \"insane\": \"crazy\", \"weird\" : \"strange\", \"bad\" : \"terrible\", })","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:00:10.259614Z","iopub.execute_input":"2023-07-28T00:00:10.259958Z","iopub.status.idle":"2023-07-28T00:00:10.264876Z","shell.execute_reply.started":"2023-07-28T00:00:10.259930Z","shell.execute_reply":"2023-07-28T00:00:10.263909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_synonymity_average = dict()\ncbow_names = list(cbow_models.keys())\ncbow_vals = list(cbow_models.values())\nfor i in range(len(cbow_names)):\n    print(\"Model\", cbow_names[i])\n    average_synonimity = 0\n    synonymities = list()\n    for s1 in synonyms:\n        print(\"\\tSimilarity between\", s1, \"and\", synonyms[s1],  \"in\", cbow_names[i], cbow_vals[i].wv.similarity(s1, synonyms[s1]))\n    synonymities.append(cbow_vals[i].wv.similarity(s1, synonyms[s1]))\n    average_synonimity = mean(synonymities)\n    print(average_synonimity)\n    models_synonymity_average[cbow_names[i]] = average_synonimity\nmodels_synonymity_average","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:00:10.266015Z","iopub.execute_input":"2023-07-28T00:00:10.266284Z","iopub.status.idle":"2023-07-28T00:00:10.308259Z","shell.execute_reply.started":"2023-07-28T00:00:10.266261Z","shell.execute_reply":"2023-07-28T00:00:10.306640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_chosen_name = max(models_synonymity_average, key=models_synonymity_average.get)\nprint(model_chosen_name)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:00:10.309652Z","iopub.execute_input":"2023-07-28T00:00:10.310769Z","iopub.status.idle":"2023-07-28T00:00:10.317589Z","shell.execute_reply.started":"2023-07-28T00:00:10.310741Z","shell.execute_reply":"2023-07-28T00:00:10.316322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Winning model\nprint_vocab(cbow_models[\"cbow_w10_f1_500_ns_half_neg2\"], top_n = 20)\n# the following will find words used in similar context to listed words, but does not\n# necessarily qualify or define the words in the list\nfor word in ['black', 'white', 'misdiagnosis', 'misdiagnosed', 'ethnicity', 'ethnic', 'indian', 'asian', 'caucasian', 'african', 'mexican', 'chinese', 'muslim', 'spanish', 'english', 'native', 'racist', 'bigot', 'bigoted', 'discrimination', 'racism', 'stereotype', 'stereotypical', 'family', 'culture', 'cultural', 'aggression', 'hostility', 'hostile', 'involuntary', 'forced', 'aggressive', 'hostile', 'violent', 'restrained', 'stereotyped', 'bias', 'biased', 'nhs', 'religious', 'condescending', 'disbelief', 'immigrant', 'foreign', 'foreigners', 'indigenous', 'ghetto', 'slang', 'aave']:\n    try:\n        print(f\"\\nWORD: {word.upper()}\")\n        print(cbow_w10_f1_500_ns_half_neg2, cbow_w10_f1_500_ns_half_neg2.wv.similar_by_word(word, 30))\n        print(\"\\n\")\n    except:\n        print(f\"WORD: {word}\")\n        print(f\"no instance of {word} found\\n\\n\")\n        \n# TODO: print concordances for each word in the list","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:00:10.318931Z","iopub.execute_input":"2023-07-28T00:00:10.319386Z","iopub.status.idle":"2023-07-28T00:00:10.598710Z","shell.execute_reply.started":"2023-07-28T00:00:10.319216Z","shell.execute_reply":"2023-07-28T00:00:10.597915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # # SECTION II: Separate by Disorder Category + Cosine Similarity","metadata":{}},{"cell_type":"code","source":"df_depression = df[df['disorder'] == 'depression'].sample(frac=0.19689)\ndf_anxiety = df[df['disorder'] == 'anxiety'].sample(frac=0.5)\ndisplay(df_depression)\ndisplay(df_anxiety)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:00:10.602399Z","iopub.execute_input":"2023-07-28T00:00:10.603037Z","iopub.status.idle":"2023-07-28T00:00:10.679049Z","shell.execute_reply.started":"2023-07-28T00:00:10.603007Z","shell.execute_reply":"2023-07-28T00:00:10.678270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_depression['disorder'] = [sp(df_depression['post'].iloc[i]) for i in range(df_depression.shape[0])]\n\ndf_anxiety['disorder'] = [sp(df_anxiety['post'].iloc[i]) for i in range(df_anxiety.shape[0])]\n\ndf_depression['tokens'] = [[word.text for word in sp(df_depression['post'].iloc[i])] for i in range(df_depression.shape[0])]\ndisplay(df_depression)\ndf_anxiety['tokens'] = [[word.text for word in sp(df_anxiety['post'].iloc[i])] for i in range(df_anxiety.shape[0])]\n\ndf_depression['lemmas'] = [[word.lemma_ for word in sp(df_depression['post'].iloc[i])] for i in range(df_depression .shape[0])]\ndisplay(df_depression)\ndf_anxiety['lemmas'] = [[word.lemma_ for word in sp(df_anxiety['post'].iloc[i])] for i in range(df_anxiety .shape[0])]\n\nfor i in range(df_depression.shape[0]):\n    df_depression['lemmas'].iloc[i] = [t for t in df_depression['lemmas'].iloc[i] if t not in '''!()-[]{};:\\'\"\\,<>./?@#$%^&*_~\\|–—\\“’`''']\ndisplay(df_depression)\nfor i in range(df_anxiety.shape[0]):\n    df_anxiety['lemmas'].iloc[i] = [t for t in df_anxiety['lemmas'].iloc[i] if t not in '''!()-[]{};:\\'\"\\,<>./?@#$%^&*_~\\|–—\\“’`''']\n    \nstop_words = set(stopwords.words('english'))\ndf_depression[\"lemmas_no_stop\"] = ''\nfor i in range(df_depression.shape[0]):\n    df_depression['lemmas_no_stop'].iloc[i] = [t for t in df_depression['lemmas'].iloc[i] if t.lower() not in stop_words and t.lower() not in [\"i\", \"is\", \"be\", \"am\", \"are\", \"the\", \"reddit\", \"i'm\", \"i’m\"]]\ndisplay(df_depression)\ndf_anxiety[\"lemmas_no_stop\"] = ''\nfor i in range(df_anxiety.shape[0]):\n    df_anxiety['lemmas_no_stop'].iloc[i] = [t for t in df_anxiety['lemmas'].iloc[i] if t.lower() not in stop_words and t.lower() not in [\"i\", \"is\", \"be\", \"am\", \"are\", \"the\", \"reddit\", \"i'm\", \"i’m\"]]\n\ndisplay(df_depression)\ndisplay(df_anxiety)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:00:10.682394Z","iopub.execute_input":"2023-07-28T00:00:10.684145Z","iopub.status.idle":"2023-07-28T00:17:49.277479Z","shell.execute_reply.started":"2023-07-28T00:00:10.684115Z","shell.execute_reply":"2023-07-28T00:17:49.276453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dep = gensim.models.Word2Vec(df_depression['lemmas_no_stop'], min_count=1, vector_size=300, window = 5, sg = 1)\nmodel_anx = gensim.models.Word2Vec(df_anxiety['lemmas_no_stop'], min_count=1, vector_size=300, window = 5, sg = 1)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:17:49.278735Z","iopub.execute_input":"2023-07-28T00:17:49.279058Z","iopub.status.idle":"2023-07-28T00:18:12.970621Z","shell.execute_reply.started":"2023-07-28T00:17:49.279035Z","shell.execute_reply":"2023-07-28T00:18:12.969742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_vocab(model_dep, 10)\nprint_vocab(model_anx, 10)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:18:12.971727Z","iopub.execute_input":"2023-07-28T00:18:12.972095Z","iopub.status.idle":"2023-07-28T00:18:12.985351Z","shell.execute_reply.started":"2023-07-28T00:18:12.972072Z","shell.execute_reply":"2023-07-28T00:18:12.984471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# measure quality of embeddings:\n    # standard way to evaluate word2vec is to use a fixed list like https://fh295.github.io/simlex.html\n    # calculate cosine similarities between pairs of words as calculated by each model, figure out best model from that\n# next step, split corpus by condition/other defining factor:\n    # \n# HOW TO CHOOSE WORDS:\n    # focus on words that we know are of particular interest:\n        # culture, family\n        # aggressiveness\n        # could use https://wordnet.princeton.edu/ to\n         # find synonyms and related words for concepts\n         # like culture/family/aggressiveness\n        # check synonyms in word embeddings for words like aggressive\n         # to see whether they're different for different races\n         # such as hostile (pos) vs assertive (neg)\n        # http://sag.art.uniroma2.it/demo-software/distributional-polarity-lexicon/\n# how to write code in order to produce results I can thoroughly analyse\n\n# strong dissertation advice\n    # write a dissertation which is understandable by\n     # people who know nothing about the computational\n     # side, but also explain the technological methodology\n     # well and ensure it's sound\n    # RICH literature review, talk about what's not been done\n     # one aspect = using non-chronological semantic shift (comp ling innovation)\n     # ethnicity + mental health using semantic shift = new angle\n    # one thing which is overlooked: leave enough time to reflect on results\n     # need multiple weeks to do so","metadata":{"execution":{"iopub.status.busy":"2023-07-28T00:18:12.986344Z","iopub.execute_input":"2023-07-28T00:18:12.986739Z","iopub.status.idle":"2023-07-28T00:18:13.006028Z","shell.execute_reply.started":"2023-07-28T00:18:12.986718Z","shell.execute_reply":"2023-07-28T00:18:13.004731Z"},"trusted":true},"execution_count":null,"outputs":[]}]}