{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82d9fef-65b7-4ce7-8692-05233456d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.11/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.11/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (6.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "import spacy\n",
    "!python -m spacy download en\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "import csv\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import display, HTML\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "!pip install -U gensim\n",
    "import gensim\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "from statistics import mean\n",
    "import json\n",
    "# to make our plot outputs appear and be stored within the notebook:\n",
    "%matplotlib inline \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de3348f-7471-4fc0-a45e-3af243d63eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1054565/2352958704.py:2: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,804) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  text_df = pd.read_csv(\"18_093_attachment_text_Aug21.csv\", encoding=\"ISO-8859-1\")[[\"Date\", \"Attachment_text\", \"cn_doc_id\", \"Summary\", \"BRCID\"]]\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.read_excel(\"output.xls\")\n",
    "text_df = pd.read_csv(\"18_093_attachment_text_Aug21.csv\", encoding=\"ISO-8859-1\")[[\"Date\", \"Attachment_text\", \"cn_doc_id\", \"Summary\", \"BRCID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1fa423-69b9-4d32-a245-7a626ea050aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = pd.merge(output_df, text_df, left_on='CN_Doc_ID', right_on='cn_doc_id', how='right')\n",
    "complete_data.sort_values(\"ethnicitycleaned\", axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6ea772-710d-4aae-9884-0d27c499bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethnicity\n",
      "British (A)                    588\n",
      "Any other ethnic group (S)     240\n",
      "                              ... \n",
      "Chinese (R)                     14\n",
      "White and Black African (E)     10\n",
      "Name: count, Length: 16, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "complete_data.rename(columns={\"ethnicitycleaned\": \"Ethnicity\", \"Attachment_text\": \"Patient_Summary\", \"age\": \"Age\", \"Gender_ID\": \"Gender\", \"diagnosis_date\": \"Diagnosis_Date\", \"document_date\": \"Document_Date\", \"CN_Doc_ID\": \"unique_id\"}, inplace=True)\n",
    "complete_data = complete_data.dropna(subset=[\"Ethnicity\", \"Patient_Summary\"])\n",
    "# display(complete_data)\n",
    "print(complete_data[\"Ethnicity\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38607aba-56c1-4dc9-8268-85f3b46d9a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethnicity\n",
      "White          743\n",
      "Black          393\n",
      "              ... \n",
      "Mixed Black     31\n",
      "Other           19\n",
      "Name: count, Length: 8, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "complete_data[\"Ethnicity\"] = complete_data[\"Ethnicity\"].replace(\"Any other black background (P)\", \"Black\").replace(\"African (N)\", \"Black\").replace(\"Caribbean (M)\", \"Black\").replace(\"White and Black African (E)\", \"Mixed Black\").replace(\"White and black Caribbean (D)\", \"Mixed Black\")\n",
    "complete_data[\"Ethnicity\"] = complete_data[\"Ethnicity\"].replace(\"Indian (H)\", \"South Asian\").replace(\"Pakistani (J)\", \"Asian\").replace(\"Bangladeshi (K)\", \"Asian\")\n",
    "complete_data[\"Ethnicity\"] = complete_data[\"Ethnicity\"].replace(\"Chinese (R)\", \"Asian\").replace(\"Any other Asian background (L)\", \"Asian\")\n",
    "complete_data[\"Ethnicity\"] = complete_data[\"Ethnicity\"].replace(\"Any other white background (C)\", \"White\").replace(\"Irish (B)\", \"White\").replace(\"British (A)\", \"White\")\n",
    "complete_data[\"Ethnicity\"] = complete_data[\"Ethnicity\"].replace(\"Any other mixed background (G)\", \"Other\")\n",
    "print(complete_data[\"Ethnicity\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c093698e-af7a-4488-816e-c244ee444fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data[\"Patient_Summary\"] = complete_data[\"Patient_Summary\"].replace(\"dr \".casefold(), \"doctor \")\n",
    "complete_data[\"Patient_Summary\"] = complete_data[\"Patient_Summary\"].replace(\"bipolar disorder\".casefold(), \"bipolar_disorder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17baf0b-fac6-443b-b71d-e44601cc4ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10402\n",
      "5502\n",
      "434\n",
      "1470\n"
     ]
    }
   ],
   "source": [
    "white_data = complete_data[complete_data[\"Ethnicity\"] == \"White\"]\n",
    "black_data = complete_data[complete_data[\"Ethnicity\"] == \"Black\"]\n",
    "mixed_data = complete_data[complete_data[\"Ethnicity\"] == \"Mixed Black\"]\n",
    "asian_data = complete_data[complete_data[\"Ethnicity\"] == \"Asian\"]\n",
    "\n",
    "print(white_data.size)\n",
    "print(black_data.size)\n",
    "print(mixed_data.size)\n",
    "print(asian_data.size)\n",
    "\n",
    "# display(white_data)\n",
    "# display(black_data)\n",
    "# display(asian_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c5dfd1a-c381-4bee-be7d-0dc194afe226",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# list of words to check for:\n",
    "# [family, husband, wife, behaviour, behaved, supportive, anger, paranoid, schizophrenia, symptoms, suicidal, cancel, warning, childcare, personality]\n",
    "# search for concordances of verbs like \"I [do not] believe\", \"I [do not] think\", \"I [do not] find\"\n",
    "\n",
    "# Analyse the types of document within the files\n",
    "\n",
    "# check for home and car ownership, can potentially add column to mention this\n",
    "# is the language used in clinical notes different for ethnic minorities who own these vs those who don't?\n",
    "white_home_ownership_df = white_data[white_data[\"Patient_Summary\"].str.contains(\"House\", case=False)]\n",
    "white_car_ownership_df = white_data[white_data[\"Patient_Summary\"].str.contains(\"Car\", case=False)]\n",
    "black_home_ownership_df = black_data[black_data[\"Patient_Summary\"].str.contains(\"House\", case=False)]\n",
    "black_car_ownership_df = black_data[black_data[\"Patient_Summary\"].str.contains(\"Car\", case=False)]\n",
    "mixed_home_ownership_df = mixed_data[mixed_data[\"Patient_Summary\"].str.contains(\"House\", case=False)]\n",
    "mixed_car_ownership_df = mixed_data[mixed_data[\"Patient_Summary\"].str.contains(\"Car\", case=False)]\n",
    "asian_home_ownership_df = asian_data[asian_data[\"Patient_Summary\"].str.contains(\"House\", case=False)]\n",
    "asian_car_ownership_df = asian_data[asian_data[\"Patient_Summary\"].str.contains(\"Car\", case=False)]\n",
    "\n",
    "white_ownership_df = pd.concat([white_home_ownership_df, white_car_ownership_df])\n",
    "black_ownership_df = pd.concat([black_home_ownership_df, black_car_ownership_df])\n",
    "asian_ownership_df = pd.concat([asian_home_ownership_df, asian_car_ownership_df])\n",
    "\n",
    "\n",
    "diagnosis_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Diagnosis\", case=False)]\n",
    "diagnosed_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Diagnosed\", case=False)]\n",
    "assessment_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Assessment\", case=False)]\n",
    "symptoms_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Symptoms\", case=False)]\n",
    "paranoid_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"paranoid\", case=False)]\n",
    "schizo_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Schizophrenia\", case=False)]\n",
    "schizo2_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Schizophrenic\", case=False)]\n",
    "dementia_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Dementia\", case=False)]\n",
    "sui_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Suicidal\", case=False)]\n",
    "sui2_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Suicide\", case=False)]\n",
    "hallucinations_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Hallucinations\", case=False)]\n",
    "\n",
    "d_df = pd.concat([diagnosis_df, diagnosed_df, assessment_df, symptoms_df, schizo_df, schizo2_df, dementia_df, sui_df, sui2_df, hallucinations_df])\n",
    "d_df.drop_duplicates(inplace=True)\n",
    "print(\"------------DIAGNOSE MERGED------------\")\n",
    "print(d_df[\"Ethnicity\"].value_counts())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "family_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Family\", case=False)]\n",
    "child_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Child\", case=False)]\n",
    "parent_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Parent\", case=False)]\n",
    "father_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Father\", case=False)]\n",
    "mother_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Mother\", case=False)]\n",
    "son_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Son\", case=False)]\n",
    "daughter_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Daughter\", case=False)]\n",
    "husband_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Husband\", case=False)]\n",
    "wife_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Wife\", case=False)]\n",
    "grandparents_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Grandparents\", case=False)]\n",
    "grandmother_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Grandmother\", case=False)]\n",
    "grandfather_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Grandfather\", case=False)]\n",
    "elder_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"Elder\", case=False)]\n",
    "childcare_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"childcare\", case=False)]\n",
    "\n",
    "fam_comb_df = pd.concat([family_df, child_df, parent_df, father_df, mother_df, son_df, daughter_df, husband_df, wife_df, grandparents_df, grandmother_df, grandfather_df, elder_df, childcare_df])\n",
    "fam_comb_df.drop_duplicates(inplace=True)\n",
    "print(\"------------FAMILY MERGED------------\")\n",
    "print(fam_comb_df[\"Ethnicity\"].value_counts())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "culture_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"culture\", case=False)]\n",
    "cultural_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"cultural\", case=False)]\n",
    "religious_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"religious\", case=False)]\n",
    "religion_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"religion\", case=False)]\n",
    "conservative_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"conservative\", case=False)]\n",
    "liberal_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"liberal\", case=False)]\n",
    "traditional_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"traditional\", case=False)]\n",
    "patriarchal_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"patriarchal\", case=False)]\n",
    "immigrant_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"immigrant\", case=False)]\n",
    "immigration_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"immigration\", case=False)]\n",
    "\n",
    "cult_comb_df = pd.concat([culture_df, cultural_df, religious_df, religion_df, conservative_df, liberal_df, traditional_df, patriarchal_df, immigrant_df, immigration_df])\n",
    "cult_comb_df.drop_duplicates(inplace=True)\n",
    "print(\"------------CULTURE MERGED------------\")\n",
    "print(cult_comb_df[\"Ethnicity\"].value_counts())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "anger_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"anger\", case=False)]\n",
    "angry_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"angry\", case=False)]\n",
    "hostile_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"hostile\", case=False)]\n",
    "violent_df = complete_data[complete_data[\"Patient_Summary\"].str.contains(\"violent\", case=False)]\n",
    "\n",
    "behav_df = pd.concat([anger_df, angry_df, hostile_df, violent_df])\n",
    "behav_df.drop_duplicates(inplace=True)\n",
    "print(\"------------BEHAVIOUR MERGED------------\")\n",
    "print(cult_comb_df[\"Ethnicity\"].value_counts())\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "764cdde9-41d8-405b-9444-270eedfb399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running spacy\n",
      "Finished running tokens\n",
      "Finished running lemmas\n"
     ]
    }
   ],
   "source": [
    "white_data['Patient_Summary_sp'] = [sp(white_data['Patient_Summary'].iloc[i]) for i in range(white_data.shape[0])]\n",
    "black_data['Patient_Summary_sp'] = [sp(black_data['Patient_Summary'].iloc[i]) for i in range(black_data.shape[0])]\n",
    "mixed_data['Patient_Summary_sp'] = [sp(mixed_data['Patient_Summary'].iloc[i]) for i in range(mixed_data.shape[0])]\n",
    "asian_data['Patient_Summary_sp'] = [sp(asian_data['Patient_Summary'].iloc[i]) for i in range(asian_data.shape[0])]\n",
    "print(\"Finished running spacy\")\n",
    "\n",
    "white_data['tokens'] = [[word.text for word in white_data['Patient_Summary_sp'].iloc[i]] for i in range(white_data.shape[0])]\n",
    "black_data['tokens'] = [[word.text for word in black_data['Patient_Summary_sp'].iloc[i]] for i in range(black_data.shape[0])]\n",
    "mixed_data['tokens'] = [[word.text for word in mixed_data['Patient_Summary_sp'].iloc[i]] for i in range(mixed_data.shape[0])]\n",
    "asian_data['tokens'] = [[word.text for word in asian_data['Patient_Summary_sp'].iloc[i]] for i in range(asian_data.shape[0])]\n",
    "print(\"Finished running tokens\")\n",
    "white_data['lemmas'] = [[word.lemma_ for word in white_data['Patient_Summary_sp'].iloc[i]] for i in range(white_data.shape[0])]\n",
    "black_data['lemmas'] = [[word.lemma_ for word in black_data['Patient_Summary_sp'].iloc[i]] for i in range(black_data.shape[0])]\n",
    "mixed_data['lemmas'] = [[word.lemma_ for word in mixed_data['Patient_Summary_sp'].iloc[i]] for i in range(mixed_data.shape[0])]\n",
    "asian_data['lemmas'] = [[word.lemma_ for word in asian_data['Patient_Summary_sp'].iloc[i]] for i in range(asian_data.shape[0])]\n",
    "print(\"Finished running lemmas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dc6c164-2e86-46cf-a4ef-1b157864febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(white_data.shape[0]):\n",
    "    white_data['lemmas'].iloc[i] = [t for t in white_data['lemmas'].iloc[i] if t.isalnum()]\n",
    "for i in range(black_data.shape[0]):\n",
    "    black_data['lemmas'].iloc[i] = [t for t in black_data['lemmas'].iloc[i] if t.isalnum()]\n",
    "for i in range(mixed_data.shape[0]):\n",
    "    mixed_data['lemmas'].iloc[i] = [t for t in mixed_data['lemmas'].iloc[i] if t.isalnum()]\n",
    "for i in range(asian_data.shape[0]):\n",
    "    asian_data['lemmas'].iloc[i] = [t for t in asian_data['lemmas'].iloc[i] if t.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04d08161-57c8-4186-aca4-42cd5dc5d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED REMOVING STOPWORDS\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "white_data[\"lemmas_no_stop\"] = ''\n",
    "black_data['lemmas_no_stop'] = ''\n",
    "mixed_data['lemmas_no_stop'] = ''\n",
    "asian_data['lemmas_no_stop'] = ''\n",
    "for i in range(white_data.shape[0]):\n",
    "    white_data['lemmas_no_stop'].iloc[i] = [t for t in white_data['lemmas'].iloc[i] if t.lower() not in stop_words and not t.isnumeric() and t.lower() not in [\"i\", \"the\", \"SLaM\", \"i'm\", \"i’m\", \"zzzzz\", \"qqqqq\"]]\n",
    "for i in range(black_data.shape[0]):\n",
    "    black_data['lemmas_no_stop'].iloc[i] = [t for t in black_data['lemmas'].iloc[i] if t.lower() not in stop_words and not t.isnumeric() and t.lower() not in [\"i\", \"the\", \"SLaM\", \"i'm\", \"i’m\", \"zzzzz\", \"qqqqq\"]]\n",
    "for i in range(mixed_data.shape[0]):\n",
    "    mixed_data['lemmas_no_stop'].iloc[i] = [t for t in mixed_data['lemmas'].iloc[i] if t.lower() not in stop_words and not t.isnumeric() and t.lower() not in [\"i\", \"the\", \"SLaM\", \"i'm\", \"i’m\", \"zzzzz\", \"qqqqq\"]]\n",
    "for i in range(asian_data.shape[0]):\n",
    "    asian_data['lemmas_no_stop'].iloc[i] = [t for t in asian_data['lemmas'].iloc[i] if t.lower() not in stop_words and not t.isnumeric() and t.lower() not in [\"i\", \"the\", \"SLaM\", \"i'm\", \"i’m\", \"zzzzz\", \"qqqqq\"]]\n",
    "\n",
    "print(\"FINISHED REMOVING STOPWORDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bb1fc-429b-4572-888a-575372526bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_data.to_csv(index=False)\n",
    "black_data.to_csv(index=False)\n",
    "mixed_data.to_csv(index=False)\n",
    "asian_data.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b61180-a8c3-4b03-9dca-6feee7ea5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vocab(model, top_n = None):\n",
    "    count = 0\n",
    "    if top_n is not None:\n",
    "        for index, word in enumerate(model.wv.index_to_key):\n",
    "            count+= 1\n",
    "            if count < top_n:\n",
    "                print(f\"WORD #{index}/{len(model.wv.index_to_key)} IS: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce80020-dc0f-4406-8d47-969d43ab9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(white_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b6bc981-6de0-43e2-af63-cd984e58d9f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINED ALL 100-DIMENSION MODELS---WHITE\n",
      "TRAINED ALL 300-DIMENSION MODELS---WHITE\n",
      "TRAINED ALL 500-DIMENSION MODELS---WHITE\n",
      "TRAINED ALL 700-DIMENSION MODELS---WHITE\n",
      "TRAINED ALL 1000-DIMENSION MODELS---WHITE\n"
     ]
    }
   ],
   "source": [
    "white_cbow_models = {}\n",
    "# 100 dimensions\n",
    "cbow_w3_f1_100 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_100\"] = cbow_w3_f1_100\n",
    "cbow_w5_f1_100 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_100\"] = cbow_w5_f1_100\n",
    "cbow_w10_f1_100 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100\"] = cbow_w10_f1_100\n",
    "cbow_w3_f1_100_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_100_mc0\"] = cbow_w3_f1_100_mc0\n",
    "cbow_w5_f1_100_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_100_mc0\"] = cbow_w5_f1_100_mc0\n",
    "cbow_w10_f1_100_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_mc0\"] = cbow_w10_f1_100_mc0\n",
    "cbow_w10_f1_100_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_mc2\"] = cbow_w10_f1_100_mc2\n",
    "cbow_w10_f1_100_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_neg2\"] = cbow_w10_f1_100_neg2\n",
    "cbow_w10_f1_100_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_neg5\"] = cbow_w10_f1_100_neg5\n",
    "cbow_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_ns_half_neg2\"] = cbow_w10_f1_100_ns_half_neg2\n",
    "cbow_w10_f1_100_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_ns_half_neg3\"] = cbow_w10_f1_100_ns_half_neg3\n",
    "cbow_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_ns_half_neg5\"] = cbow_w10_f1_100_ns_half_neg5\n",
    "cbow_w10_f1_100_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_neg2_ws2\"] = cbow_w10_f1_100_neg2_ws2\n",
    "cbow_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_100_neg5_ws5\"] = cbow_w10_f1_100_neg5_ws5\n",
    "print(\"TRAINED ALL 100-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# 300 dimensions\n",
    "cbow_w3_f1_300 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_300\"] = cbow_w3_f1_300\n",
    "cbow_w5_f1_300 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_300\"] = cbow_w5_f1_300\n",
    "cbow_w10_f1_300 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300\"] = cbow_w10_f1_300\n",
    "cbow_w3_f1_300_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_300_mc0\"] = cbow_w3_f1_300_mc0\n",
    "cbow_w5_f1_300_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_300_mc0\"] = cbow_w5_f1_300_mc0\n",
    "cbow_w10_f1_300_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_mc0\"] = cbow_w10_f1_300_mc0\n",
    "cbow_w10_f1_300_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_mc2\"] = cbow_w10_f1_300_mc2\n",
    "cbow_w10_f1_300_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_neg2\"] = cbow_w10_f1_300_neg2\n",
    "cbow_w10_f1_300_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_neg5\"] = cbow_w10_f1_300_neg5\n",
    "cbow_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_ns_half_neg2\"] = cbow_w10_f1_300_ns_half_neg2\n",
    "cbow_w10_f1_300_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_ns_half_neg3\"] = cbow_w10_f1_300_ns_half_neg3\n",
    "cbow_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_ns_half_neg5\"] = cbow_w10_f1_300_ns_half_neg5\n",
    "cbow_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_neg2_ws2\"] = cbow_w10_f1_300_neg2_ws2\n",
    "cbow_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_300_neg5_ws5\"] = cbow_w10_f1_300_neg5_ws5\n",
    "print(\"TRAINED ALL 300-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# 500 dimensions\n",
    "cbow_w3_f1_500 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_500\"] = cbow_w3_f1_500\n",
    "cbow_w5_f1_500 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_500\"] = cbow_w5_f1_500\n",
    "cbow_w10_f1_500 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500\"] = cbow_w10_f1_500\n",
    "cbow_w3_f1_500_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_500_mc0\"] = cbow_w3_f1_500_mc0\n",
    "cbow_w5_f1_500_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_500_mc0\"] = cbow_w5_f1_500_mc0\n",
    "cbow_w10_f1_500_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_mc0\"] = cbow_w10_f1_500_mc0\n",
    "cbow_w10_f1_500_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_mc2\"] = cbow_w10_f1_500_mc2\n",
    "cbow_w10_f1_500_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_neg2\"] = cbow_w10_f1_500_neg2\n",
    "cbow_w10_f1_500_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_neg5\"] = cbow_w10_f1_500_neg5\n",
    "cbow_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_ns_half_neg2\"] = cbow_w10_f1_500_ns_half_neg2\n",
    "cbow_w10_f1_500_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_ns_half_neg3\"] = cbow_w10_f1_500_ns_half_neg3\n",
    "cbow_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_ns_half_neg5\"] = cbow_w10_f1_500_ns_half_neg5\n",
    "cbow_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_neg2_ws2\"] = cbow_w10_f1_500_neg2_ws2\n",
    "cbow_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_500_neg5_ws5\"] = cbow_w10_f1_500_neg5_ws5\n",
    "print(\"TRAINED ALL 500-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# 700 dimensions\n",
    "cbow_w3_f1_700 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_700\"] = cbow_w3_f1_700\n",
    "cbow_w5_f1_700 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_700\"] = cbow_w5_f1_700\n",
    "cbow_w10_f1_700 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700\"] = cbow_w10_f1_700\n",
    "cbow_w3_f1_700_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_700_mc0\"] = cbow_w3_f1_700_mc0\n",
    "cbow_w5_f1_700_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_700_mc0\"] = cbow_w5_f1_700_mc0\n",
    "cbow_w10_f1_700_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_mc0\"] = cbow_w10_f1_700_mc0\n",
    "cbow_w10_f1_700_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_mc2\"] = cbow_w10_f1_700_mc2\n",
    "cbow_w10_f1_700_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_neg2\"] = cbow_w10_f1_700_neg2\n",
    "cbow_w10_f1_700_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_neg5\"] = cbow_w10_f1_700_neg5\n",
    "cbow_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_ns_half_neg2\"] = cbow_w10_f1_700_ns_half_neg2\n",
    "cbow_w10_f1_700_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_ns_half_neg3\"] = cbow_w10_f1_700_ns_half_neg3\n",
    "cbow_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_ns_half_neg5\"] = cbow_w10_f1_700_ns_half_neg5\n",
    "cbow_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_neg2_ws2\"] = cbow_w10_f1_700_neg2_ws2\n",
    "cbow_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_700_neg5_ws5\"] = cbow_w10_f1_700_neg5_ws5\n",
    "print(\"TRAINED ALL 700-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# 1000 dimensions\n",
    "cbow_w3_f1_1000 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_1000\"] = cbow_w3_f1_1000\n",
    "cbow_w2_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w2_f1_1000_ns_half_neg5\"] = cbow_w2_f1_1000_ns_half_neg5\n",
    "cbow_w5_f1_1000 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_1000\"] = cbow_w5_f1_1000\n",
    "cbow_w10_f1_1000 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000\"] = cbow_w10_f1_1000\n",
    "cbow_w3_f1_1000_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_1000_mc0\"] = cbow_w3_f1_1000_mc0\n",
    "\n",
    "cbow_w3_f1_1000_mc0_e20 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 20)\n",
    "white_cbow_models[\"cbow_w3_f1_1000_mc0_e20\"] = cbow_w3_f1_1000_mc0_e20\n",
    "\n",
    "cbow_w3_f1_1000_mc1 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w3_f1_1000_mc1\"] = cbow_w3_f1_1000_mc1\n",
    "\n",
    "cbow_w5_f1_1000_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w5_f1_1000_mc0\"] = cbow_w5_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_mc0\"] = cbow_w10_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_mc2\"] = cbow_w10_f1_1000_mc2\n",
    "cbow_w10_f1_1000_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_neg2\"] = cbow_w10_f1_1000_neg2\n",
    "cbow_w10_f1_1000_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_neg5\"] = cbow_w10_f1_1000_neg5\n",
    "cbow_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_ns_half_neg2\"] = cbow_w10_f1_1000_ns_half_neg2\n",
    "cbow_w10_f1_1000_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_ns_half_neg3\"] = cbow_w10_f1_1000_ns_half_neg3\n",
    "cbow_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_ns_half_neg5\"] = cbow_w10_f1_1000_ns_half_neg5\n",
    "cbow_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_neg2_ws2\"] = cbow_w10_f1_1000_neg2_ws2\n",
    "cbow_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "white_cbow_models[\"cbow_w10_f1_1000_neg5_ws5\"] = cbow_w10_f1_1000_neg5_ws5\n",
    "print(\"TRAINED ALL 1000-DIMENSION MODELS---WHITE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "84c69f9c-3aee-4bda-8d5c-dc62a04b151d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow_w3_f1_100: 0.7611773610115051\n",
      "cbow_w5_f1_100: 0.7406727075576782\n",
      "cbow_w10_f1_100: 0.5874452590942383\n",
      "cbow_w3_f1_100_mc0: 0.7618361711502075\n",
      "cbow_w5_f1_100_mc0: 0.7310590744018555\n",
      "cbow_w10_f1_100_mc0: 0.5897625684738159\n",
      "cbow_w10_f1_100_mc2: 0.5358892679214478\n",
      "cbow_w10_f1_100_neg2: 0.6089947819709778\n",
      "cbow_w10_f1_100_neg5: 0.5832439064979553\n",
      "cbow_w10_f1_100_ns_half_neg2: 0.730128288269043\n",
      "cbow_w10_f1_100_ns_half_neg3: 0.7315319776535034\n",
      "cbow_w10_f1_100_ns_half_neg5: 0.6801722645759583\n",
      "cbow_w10_f1_100_neg2_ws2: 0.7331801056861877\n",
      "cbow_w10_f1_100_neg5_ws5: 0.7454401254653931\n",
      "cbow_w3_f1_300: 0.7681673765182495\n",
      "cbow_w5_f1_300: 0.7526143789291382\n",
      "cbow_w10_f1_300: 0.6059569716453552\n",
      "cbow_w3_f1_300_mc0: 0.781477153301239\n",
      "cbow_w5_f1_300_mc0: 0.7532844543457031\n",
      "cbow_w10_f1_300_mc0: 0.6045530438423157\n",
      "cbow_w10_f1_300_mc2: 0.5557360053062439\n",
      "cbow_w10_f1_300_neg2: 0.6129350066184998\n",
      "cbow_w10_f1_300_neg5: 0.6120279431343079\n",
      "cbow_w10_f1_300_ns_half_neg2: 0.7666320204734802\n",
      "cbow_w10_f1_300_ns_half_neg3: 0.7634136080741882\n",
      "cbow_w10_f1_300_ns_half_neg5: 0.7038425803184509\n",
      "cbow_w10_f1_300_neg2_ws2: 0.7384196519851685\n",
      "cbow_w10_f1_300_neg5_ws5: 0.7543522715568542\n",
      "cbow_w3_f1_500: 0.7881327867507935\n",
      "cbow_w5_f1_500: 0.75732421875\n",
      "cbow_w10_f1_500: 0.627325177192688\n",
      "cbow_w3_f1_500_mc0: 0.7890646457672119\n",
      "cbow_w5_f1_500_mc0: 0.7636292576789856\n",
      "cbow_w10_f1_500_mc0: 0.6144166588783264\n",
      "cbow_w10_f1_500_mc2: 0.5686644315719604\n",
      "cbow_w10_f1_500_neg2: 0.6403743624687195\n",
      "cbow_w10_f1_500_neg5: 0.6125699877738953\n",
      "cbow_w10_f1_500_ns_half_neg2: 0.776944637298584\n",
      "cbow_w10_f1_500_ns_half_neg3: 0.7661823630332947\n",
      "cbow_w10_f1_500_ns_half_neg5: 0.718858540058136\n",
      "cbow_w10_f1_500_neg2_ws2: 0.7482343912124634\n",
      "cbow_w10_f1_500_neg5_ws5: 0.7655541300773621\n",
      "cbow_w3_f1_700: 0.7830698490142822\n",
      "cbow_w5_f1_700: 0.7642658948898315\n",
      "cbow_w10_f1_700: 0.6106078028678894\n",
      "cbow_w3_f1_700_mc0: 0.7850682735443115\n",
      "cbow_w5_f1_700_mc0: 0.756257176399231\n",
      "cbow_w10_f1_700_mc0: 0.6233637928962708\n",
      "cbow_w10_f1_700_mc2: 0.5744780898094177\n",
      "cbow_w10_f1_700_neg2: 0.6131613850593567\n",
      "cbow_w10_f1_700_neg5: 0.6224501729011536\n",
      "cbow_w10_f1_700_ns_half_neg2: 0.80196213722229\n",
      "cbow_w10_f1_700_ns_half_neg3: 0.7782728672027588\n",
      "cbow_w10_f1_700_ns_half_neg5: 0.6989750862121582\n",
      "cbow_w10_f1_700_neg2_ws2: 0.7401030659675598\n",
      "cbow_w10_f1_700_neg5_ws5: 0.7698792219161987\n",
      "cbow_w3_f1_1000: 0.7896590232849121\n",
      "cbow_w2_f1_1000_ns_half_neg5: 0.7795820832252502\n",
      "cbow_w5_f1_1000: 0.7653828859329224\n",
      "cbow_w10_f1_1000: 0.6267661452293396\n",
      "cbow_w3_f1_1000_mc0: 0.7838717699050903\n",
      "cbow_w3_f1_1000_mc0_e20: 0.5644679665565491\n",
      "cbow_w3_f1_1000_mc1: 0.784527599811554\n",
      "cbow_w5_f1_1000_mc0: 0.7583214044570923\n",
      "cbow_w10_f1_1000_mc0: 0.6242848038673401\n",
      "cbow_w10_f1_1000_mc2: 0.6010136008262634\n",
      "cbow_w10_f1_1000_neg2: 0.6516352891921997\n",
      "cbow_w10_f1_1000_neg5: 0.6218345165252686\n",
      "cbow_w10_f1_1000_ns_half_neg2: 0.7822591066360474\n",
      "cbow_w10_f1_1000_ns_half_neg3: 0.772535502910614\n",
      "cbow_w10_f1_1000_ns_half_neg5: 0.7361496090888977\n",
      "cbow_w10_f1_1000_neg2_ws2: 0.7431641221046448\n",
      "cbow_w10_f1_1000_neg5_ws5: 0.7755081057548523\n",
      "WHITE CBOW MODEL: cbow_w10_f1_700_ns_half_neg2\n",
      "\tSYNONIMITY: 0.80196213722229\n"
     ]
    }
   ],
   "source": [
    "# white cbow save & calculate avg\n",
    "synonyms = dict({\"medication\": \"medicine\",\n",
    "                 \"therapy\": \"treatment\",\n",
    "                 \"simple\": \"easy\",\n",
    "                 \"appointment\": \"engagement\",\n",
    "                 \"road\": \"route\",\n",
    "                 \"family\": \"household\",\n",
    "                 \"history\": \"account\",\n",
    "                 \"mood\": \"temper\",\n",
    "                 \"information\": \"info\",\n",
    "                 \"treatment\": \"intervention\",\n",
    "                 \"social\": \"sociable\",\n",
    "                 \"anxious\": \"nervous\",\n",
    "                 \"confidential\": \"private\",\n",
    "                 \"feeling\": \"belief\",\n",
    "                 \"feeling\": \"opinion\",\n",
    "                 \"feeling\": \"impression\",\n",
    "                 \"aware\": \"mindful\",\n",
    "                 \"difficult\": \"hard\",\n",
    "                 \"clinician\": \"doctor\",\n",
    "                 \"clinician\": \"therapist\"\n",
    "                })\n",
    "\n",
    "white_cbow_models_synonymity_average = {}\n",
    "white_cbow_names = list(white_cbow_models.keys())\n",
    "white_cbow_vals = list(white_cbow_models.values())\n",
    "for i in range(len(white_cbow_names)):\n",
    "    average_synonimity = 0\n",
    "    synonymities = list()\n",
    "    for s1 in synonyms:\n",
    "        synonymities.append(white_cbow_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "    average_synonimity = mean(synonymities)\n",
    "    white_cbow_models_synonymity_average[white_cbow_names[i]] = average_synonimity\n",
    "\n",
    "for i in white_cbow_models_synonymity_average.keys():\n",
    "    print(f\"{i}: {white_cbow_models_synonymity_average[i]}\")\n",
    "white_cbow_model_chosen = max(white_cbow_models_synonymity_average, key=white_cbow_models_synonymity_average.get)\n",
    "print(f\"WHITE CBOW MODEL: {white_cbow_model_chosen}\")\n",
    "print(f\"\\tSYNONIMITY: {white_cbow_models_synonymity_average[white_cbow_model_chosen]}\")\n",
    "white_cbow_models[white_cbow_model_chosen].save(os.path.join('white_models', white_cbow_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "934652c4-d4a5-4057-875f-b5aea2e50047",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINED ALL 100-DIMENSION MODELS---BLACK\n",
      "TRAINED ALL 300-DIMENSION MODELS---BLACK\n",
      "TRAINED ALL 500-DIMENSION MODELS---BLACK\n",
      "TRAINED ALL 700-DIMENSION MODELS---BLACK\n",
      "TRAINED ALL 1000-DIMENSION MODELS---BLACK\n"
     ]
    }
   ],
   "source": [
    "black_cbow_models = {}\n",
    "# 100 dimensions\n",
    "cbow_w3_f1_100 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_100\"] = cbow_w3_f1_100\n",
    "cbow_w5_f1_100 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_100\"] = cbow_w5_f1_100\n",
    "cbow_w10_f1_100 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100\"] = cbow_w10_f1_100\n",
    "cbow_w3_f1_100_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_100_mc0\"] = cbow_w3_f1_100_mc0\n",
    "cbow_w5_f1_100_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_100_mc0\"] = cbow_w5_f1_100_mc0\n",
    "cbow_w10_f1_100_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_mc0\"] = cbow_w10_f1_100_mc0\n",
    "cbow_w10_f1_100_mc2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=2, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_mc2\"] = cbow_w10_f1_100_mc2\n",
    "cbow_w10_f1_100_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_neg2\"] = cbow_w10_f1_100_neg2\n",
    "cbow_w10_f1_100_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_neg5\"] = cbow_w10_f1_100_neg5\n",
    "cbow_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_ns_half_neg2\"] = cbow_w10_f1_100_ns_half_neg2\n",
    "cbow_w10_f1_100_ns_half_neg3 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_ns_half_neg3\"] = cbow_w10_f1_100_ns_half_neg3\n",
    "cbow_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_ns_half_neg5\"] = cbow_w10_f1_100_ns_half_neg5\n",
    "cbow_w10_f1_100_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_neg2_ws2\"] = cbow_w10_f1_100_neg2_ws2\n",
    "cbow_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_100_neg5_ws5\"] = cbow_w10_f1_100_neg5_ws5\n",
    "print(\"TRAINED ALL 100-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# 300 dimensions\n",
    "cbow_w3_f1_300 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_300\"] = cbow_w3_f1_300\n",
    "cbow_w5_f1_300 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_300\"] = cbow_w5_f1_300\n",
    "cbow_w10_f1_300 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300\"] = cbow_w10_f1_300\n",
    "cbow_w3_f1_300_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_300_mc0\"] = cbow_w3_f1_300_mc0\n",
    "cbow_w5_f1_300_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_300_mc0\"] = cbow_w5_f1_300_mc0\n",
    "cbow_w10_f1_300_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_mc0\"] = cbow_w10_f1_300_mc0\n",
    "cbow_w10_f1_300_mc2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=2, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_mc2\"] = cbow_w10_f1_300_mc2\n",
    "cbow_w10_f1_300_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_neg2\"] = cbow_w10_f1_300_neg2\n",
    "cbow_w10_f1_300_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_neg5\"] = cbow_w10_f1_300_neg5\n",
    "cbow_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_ns_half_neg2\"] = cbow_w10_f1_300_ns_half_neg2\n",
    "cbow_w10_f1_300_ns_half_neg3 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_ns_half_neg3\"] = cbow_w10_f1_300_ns_half_neg3\n",
    "cbow_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_ns_half_neg5\"] = cbow_w10_f1_300_ns_half_neg5\n",
    "cbow_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_neg2_ws2\"] = cbow_w10_f1_300_neg2_ws2\n",
    "cbow_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_300_neg5_ws5\"] = cbow_w10_f1_300_neg5_ws5\n",
    "print(\"TRAINED ALL 300-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# 500 dimensions\n",
    "cbow_w3_f1_500 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_500\"] = cbow_w3_f1_500\n",
    "cbow_w5_f1_500 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_500\"] = cbow_w5_f1_500\n",
    "cbow_w10_f1_500 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500\"] = cbow_w10_f1_500\n",
    "cbow_w3_f1_500_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_500_mc0\"] = cbow_w3_f1_500_mc0\n",
    "cbow_w5_f1_500_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_500_mc0\"] = cbow_w5_f1_500_mc0\n",
    "cbow_w10_f1_500_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_mc0\"] = cbow_w10_f1_500_mc0\n",
    "cbow_w10_f1_500_mc2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=2, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_mc2\"] = cbow_w10_f1_500_mc2\n",
    "cbow_w10_f1_500_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_neg2\"] = cbow_w10_f1_500_neg2\n",
    "cbow_w10_f1_500_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_neg5\"] = cbow_w10_f1_500_neg5\n",
    "cbow_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_ns_half_neg2\"] = cbow_w10_f1_500_ns_half_neg2\n",
    "cbow_w10_f1_500_ns_half_neg3 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_ns_half_neg3\"] = cbow_w10_f1_500_ns_half_neg3\n",
    "cbow_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_ns_half_neg5\"] = cbow_w10_f1_500_ns_half_neg5\n",
    "cbow_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_neg2_ws2\"] = cbow_w10_f1_500_neg2_ws2\n",
    "cbow_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_500_neg5_ws5\"] = cbow_w10_f1_500_neg5_ws5\n",
    "print(\"TRAINED ALL 500-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# 700 dimensions\n",
    "cbow_w3_f1_700 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_700\"] = cbow_w3_f1_700\n",
    "cbow_w5_f1_700 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_700\"] = cbow_w5_f1_700\n",
    "cbow_w10_f1_700 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700\"] = cbow_w10_f1_700\n",
    "cbow_w3_f1_700_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_700_mc0\"] = cbow_w3_f1_700_mc0\n",
    "cbow_w5_f1_700_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_700_mc0\"] = cbow_w5_f1_700_mc0\n",
    "cbow_w10_f1_700_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_mc0\"] = cbow_w10_f1_700_mc0\n",
    "cbow_w10_f1_700_mc2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=2, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_mc2\"] = cbow_w10_f1_700_mc2\n",
    "cbow_w10_f1_700_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_neg2\"] = cbow_w10_f1_700_neg2\n",
    "cbow_w10_f1_700_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_neg5\"] = cbow_w10_f1_700_neg5\n",
    "cbow_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_ns_half_neg2\"] = cbow_w10_f1_700_ns_half_neg2\n",
    "cbow_w10_f1_700_ns_half_neg3 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_ns_half_neg3\"] = cbow_w10_f1_700_ns_half_neg3\n",
    "cbow_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_ns_half_neg5\"] = cbow_w10_f1_700_ns_half_neg5\n",
    "cbow_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_neg2_ws2\"] = cbow_w10_f1_700_neg2_ws2\n",
    "cbow_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_700_neg5_ws5\"] = cbow_w10_f1_700_neg5_ws5\n",
    "print(\"TRAINED ALL 700-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# 1000 dimensions\n",
    "cbow_w3_f1_1000 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_1000\"] = cbow_w3_f1_1000\n",
    "cbow_w2_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w2_f1_1000_ns_half_neg5\"] = cbow_w2_f1_1000_ns_half_neg5\n",
    "cbow_w5_f1_1000 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_1000\"] = cbow_w5_f1_1000\n",
    "cbow_w10_f1_1000 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000\"] = cbow_w10_f1_1000\n",
    "cbow_w3_f1_1000_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_1000_mc0\"] = cbow_w3_f1_1000_mc0\n",
    "\n",
    "cbow_w3_f1_1000_mc0_e20 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 20)\n",
    "black_cbow_models[\"cbow_w3_f1_1000_mc0_e20\"] = cbow_w3_f1_1000_mc0_e20\n",
    "\n",
    "cbow_w3_f1_1000_mc1 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w3_f1_1000_mc1\"] = cbow_w3_f1_1000_mc1\n",
    "\n",
    "cbow_w5_f1_1000_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w5_f1_1000_mc0\"] = cbow_w5_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_mc0\"] = cbow_w10_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=2, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_mc2\"] = cbow_w10_f1_1000_mc2\n",
    "cbow_w10_f1_1000_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_neg2\"] = cbow_w10_f1_1000_neg2\n",
    "cbow_w10_f1_1000_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_neg5\"] = cbow_w10_f1_1000_neg5\n",
    "cbow_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_ns_half_neg2\"] = cbow_w10_f1_1000_ns_half_neg2\n",
    "cbow_w10_f1_1000_ns_half_neg3 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_ns_half_neg3\"] = cbow_w10_f1_1000_ns_half_neg3\n",
    "cbow_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_ns_half_neg5\"] = cbow_w10_f1_1000_ns_half_neg5\n",
    "cbow_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_neg2_ws2\"] = cbow_w10_f1_1000_neg2_ws2\n",
    "cbow_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "black_cbow_models[\"cbow_w10_f1_1000_neg5_ws5\"] = cbow_w10_f1_1000_neg5_ws5\n",
    "print(\"TRAINED ALL 1000-DIMENSION MODELS---BLACK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "932ee0d3-e60b-4b8a-ac40-a32a1244e862",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACK CBOW MODEL: cbow_w10_f1_100_ns_half_neg2\n",
      "\tSYNONIMITY: 0.9570443630218506\n"
     ]
    }
   ],
   "source": [
    "# black cbow save & calculate avg\n",
    "# synonyms = dict({\"appointment\": \"engagement\",\n",
    "#                  \"road\": \"route\",\n",
    "#                  \"family\": \"household\",\n",
    "#                  \"history\": \"account\",\n",
    "#                  \"mood\": \"temper\",\n",
    "#                  \"treatment\": \"intervention\",\n",
    "#                  \"confidential\": \"private\",\n",
    "#                  \"feeling\": \"belief\",\n",
    "#                  \"feeling\": \"opinion\",\n",
    "#                  \"feeling\": \"impression\",\n",
    "#                 })\n",
    "synonyms = dict({\"appointment\": \"engagement\",\n",
    "                 \"family\": \"household\",\n",
    "                 \"history\": \"account\",\n",
    "                 \"treatment\": \"intervention\",\n",
    "                 \"difficult\": \"difficulty\",\n",
    "                 \"clinician\": \"doctor\",\n",
    "                 \"clinician\": \"therapist\",\n",
    "                })\n",
    "black_cbow_models_synonymity_average = dict()\n",
    "black_cbow_names = list(black_cbow_models.keys())\n",
    "black_cbow_vals = list(black_cbow_models.values())\n",
    "for i in range(len(black_cbow_names)):\n",
    "    average_synonimity = 0\n",
    "    synonymities = list()\n",
    "    for s1 in synonyms:\n",
    "        synonymities.append(black_cbow_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "    average_synonimity = mean(synonymities)\n",
    "    black_cbow_models_synonymity_average[black_cbow_names[i]] = average_synonimity\n",
    "\n",
    "black_cbow_model_chosen = max(black_cbow_models_synonymity_average, key=black_cbow_models_synonymity_average.get)\n",
    "print(f\"BLACK CBOW MODEL: {black_cbow_model_chosen}\")\n",
    "print(f\"\\tSYNONIMITY: {black_cbow_models_synonymity_average[black_cbow_model_chosen]}\")\n",
    "black_cbow_models[black_cbow_model_chosen].save(os.path.join('black_models', black_cbow_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "feb942fb-ce8c-4b40-8c0f-f4c69ee0e4a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINED ALL 100-DIMENSION MODELS---MIXED\n",
      "TRAINED ALL 300-DIMENSION MODELS---MIXED\n",
      "TRAINED ALL 500-DIMENSION MODELS---MIXED\n",
      "TRAINED ALL 700-DIMENSION MODELS---MIXED\n",
      "TRAINED ALL 1000-DIMENSION MODELS---MIXED\n"
     ]
    }
   ],
   "source": [
    "mixed_cbow_models = {}\n",
    "# 100 dimensions\n",
    "cbow_w3_f1_100 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_100\"] = cbow_w3_f1_100\n",
    "cbow_w5_f1_100 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_100\"] = cbow_w5_f1_100\n",
    "cbow_w10_f1_100 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100\"] = cbow_w10_f1_100\n",
    "cbow_w3_f1_100_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_100_mc0\"] = cbow_w3_f1_100_mc0\n",
    "cbow_w5_f1_100_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_100_mc0\"] = cbow_w5_f1_100_mc0\n",
    "cbow_w10_f1_100_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_mc0\"] = cbow_w10_f1_100_mc0\n",
    "cbow_w10_f1_100_mc2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=2, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_mc2\"] = cbow_w10_f1_100_mc2\n",
    "cbow_w10_f1_100_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_neg2\"] = cbow_w10_f1_100_neg2\n",
    "cbow_w10_f1_100_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_neg5\"] = cbow_w10_f1_100_neg5\n",
    "cbow_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_ns_half_neg2\"] = cbow_w10_f1_100_ns_half_neg2\n",
    "cbow_w10_f1_100_ns_half_neg3 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_ns_half_neg3\"] = cbow_w10_f1_100_ns_half_neg3\n",
    "cbow_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_ns_half_neg5\"] = cbow_w10_f1_100_ns_half_neg5\n",
    "cbow_w10_f1_100_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_neg2_ws2\"] = cbow_w10_f1_100_neg2_ws2\n",
    "cbow_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_100_neg5_ws5\"] = cbow_w10_f1_100_neg5_ws5\n",
    "print(\"TRAINED ALL 100-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# 300 dimensions\n",
    "cbow_w3_f1_300 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_300\"] = cbow_w3_f1_300\n",
    "cbow_w5_f1_300 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_300\"] = cbow_w5_f1_300\n",
    "cbow_w10_f1_300 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300\"] = cbow_w10_f1_300\n",
    "cbow_w3_f1_300_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_300_mc0\"] = cbow_w3_f1_300_mc0\n",
    "cbow_w5_f1_300_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_300_mc0\"] = cbow_w5_f1_300_mc0\n",
    "cbow_w10_f1_300_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_mc0\"] = cbow_w10_f1_300_mc0\n",
    "cbow_w10_f1_300_mc2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=2, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_mc2\"] = cbow_w10_f1_300_mc2\n",
    "cbow_w10_f1_300_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_neg2\"] = cbow_w10_f1_300_neg2\n",
    "cbow_w10_f1_300_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_neg5\"] = cbow_w10_f1_300_neg5\n",
    "cbow_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_ns_half_neg2\"] = cbow_w10_f1_300_ns_half_neg2\n",
    "cbow_w10_f1_300_ns_half_neg3 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_ns_half_neg3\"] = cbow_w10_f1_300_ns_half_neg3\n",
    "cbow_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_ns_half_neg5\"] = cbow_w10_f1_300_ns_half_neg5\n",
    "cbow_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_neg2_ws2\"] = cbow_w10_f1_300_neg2_ws2\n",
    "cbow_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_300_neg5_ws5\"] = cbow_w10_f1_300_neg5_ws5\n",
    "print(\"TRAINED ALL 300-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# 500 dimensions\n",
    "cbow_w3_f1_500 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_500\"] = cbow_w3_f1_500\n",
    "cbow_w5_f1_500 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_500\"] = cbow_w5_f1_500\n",
    "cbow_w10_f1_500 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500\"] = cbow_w10_f1_500\n",
    "cbow_w3_f1_500_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_500_mc0\"] = cbow_w3_f1_500_mc0\n",
    "cbow_w5_f1_500_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_500_mc0\"] = cbow_w5_f1_500_mc0\n",
    "cbow_w10_f1_500_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_mc0\"] = cbow_w10_f1_500_mc0\n",
    "cbow_w10_f1_500_mc2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=2, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_mc2\"] = cbow_w10_f1_500_mc2\n",
    "cbow_w10_f1_500_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_neg2\"] = cbow_w10_f1_500_neg2\n",
    "cbow_w10_f1_500_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_neg5\"] = cbow_w10_f1_500_neg5\n",
    "cbow_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_ns_half_neg2\"] = cbow_w10_f1_500_ns_half_neg2\n",
    "cbow_w10_f1_500_ns_half_neg3 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_ns_half_neg3\"] = cbow_w10_f1_500_ns_half_neg3\n",
    "cbow_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_ns_half_neg5\"] = cbow_w10_f1_500_ns_half_neg5\n",
    "cbow_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_neg2_ws2\"] = cbow_w10_f1_500_neg2_ws2\n",
    "cbow_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_500_neg5_ws5\"] = cbow_w10_f1_500_neg5_ws5\n",
    "print(\"TRAINED ALL 500-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# 700 dimensions\n",
    "cbow_w3_f1_700 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_700\"] = cbow_w3_f1_700\n",
    "cbow_w5_f1_700 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_700\"] = cbow_w5_f1_700\n",
    "cbow_w10_f1_700 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700\"] = cbow_w10_f1_700\n",
    "cbow_w3_f1_700_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_700_mc0\"] = cbow_w3_f1_700_mc0\n",
    "cbow_w5_f1_700_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_700_mc0\"] = cbow_w5_f1_700_mc0\n",
    "cbow_w10_f1_700_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_mc0\"] = cbow_w10_f1_700_mc0\n",
    "cbow_w10_f1_700_mc2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=2, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_mc2\"] = cbow_w10_f1_700_mc2\n",
    "cbow_w10_f1_700_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_neg2\"] = cbow_w10_f1_700_neg2\n",
    "cbow_w10_f1_700_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_neg5\"] = cbow_w10_f1_700_neg5\n",
    "cbow_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_ns_half_neg2\"] = cbow_w10_f1_700_ns_half_neg2\n",
    "cbow_w10_f1_700_ns_half_neg3 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_ns_half_neg3\"] = cbow_w10_f1_700_ns_half_neg3\n",
    "cbow_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_ns_half_neg5\"] = cbow_w10_f1_700_ns_half_neg5\n",
    "cbow_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_neg2_ws2\"] = cbow_w10_f1_700_neg2_ws2\n",
    "cbow_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_700_neg5_ws5\"] = cbow_w10_f1_700_neg5_ws5\n",
    "print(\"TRAINED ALL 700-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# 1000 dimensions\n",
    "cbow_w3_f1_1000 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_1000\"] = cbow_w3_f1_1000\n",
    "cbow_w2_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w2_f1_1000_ns_half_neg5\"] = cbow_w2_f1_1000_ns_half_neg5\n",
    "cbow_w5_f1_1000 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_1000\"] = cbow_w5_f1_1000\n",
    "cbow_w10_f1_1000 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000\"] = cbow_w10_f1_1000\n",
    "cbow_w3_f1_1000_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_1000_mc0\"] = cbow_w3_f1_1000_mc0\n",
    "\n",
    "cbow_w3_f1_1000_mc0_e20 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 20)\n",
    "mixed_cbow_models[\"cbow_w3_f1_1000_mc0_e20\"] = cbow_w3_f1_1000_mc0_e20\n",
    "\n",
    "cbow_w3_f1_1000_mc1 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w3_f1_1000_mc1\"] = cbow_w3_f1_1000_mc1\n",
    "\n",
    "cbow_w5_f1_1000_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w5_f1_1000_mc0\"] = cbow_w5_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_mc0\"] = cbow_w10_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=2, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_mc2\"] = cbow_w10_f1_1000_mc2\n",
    "cbow_w10_f1_1000_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_neg2\"] = cbow_w10_f1_1000_neg2\n",
    "cbow_w10_f1_1000_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_neg5\"] = cbow_w10_f1_1000_neg5\n",
    "cbow_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_ns_half_neg2\"] = cbow_w10_f1_1000_ns_half_neg2\n",
    "cbow_w10_f1_1000_ns_half_neg3 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_ns_half_neg3\"] = cbow_w10_f1_1000_ns_half_neg3\n",
    "cbow_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_ns_half_neg5\"] = cbow_w10_f1_1000_ns_half_neg5\n",
    "cbow_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_neg2_ws2\"] = cbow_w10_f1_1000_neg2_ws2\n",
    "cbow_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "mixed_cbow_models[\"cbow_w10_f1_1000_neg5_ws5\"] = cbow_w10_f1_1000_neg5_ws5\n",
    "print(\"TRAINED ALL 1000-DIMENSION MODELS---MIXED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29d08640-5b61-41a1-8a16-0b39ad93b840",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIXED CBOW MODEL: cbow_w10_f1_1000_mc2\n",
      "\tSYNONIMITY: 0.9995271563529968\n"
     ]
    }
   ],
   "source": [
    "# mixed cbow save & calculate avg\n",
    "synonyms = dict({\"appointment\": \"engagement\",\n",
    "                 \"family\": \"household\",\n",
    "                 \"history\": \"account\",\n",
    "                 \"treatment\": \"intervention\",\n",
    "                 \"difficult\": \"difficulty\",\n",
    "                })\n",
    "\n",
    "mixed_cbow_models_synonymity_average = dict()\n",
    "mixed_cbow_names = list(mixed_cbow_models.keys())\n",
    "mixed_cbow_vals = list(mixed_cbow_models.values())\n",
    "for i in range(len(mixed_cbow_names)):\n",
    "    average_synonimity = 0\n",
    "    synonymities = list()\n",
    "    for s1 in synonyms:\n",
    "        synonymities.append(mixed_cbow_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "    average_synonimity = mean(synonymities)\n",
    "    mixed_cbow_models_synonymity_average[mixed_cbow_names[i]] = average_synonimity\n",
    "\n",
    "mixed_cbow_model_chosen = max(mixed_cbow_models_synonymity_average, key=mixed_cbow_models_synonymity_average.get)\n",
    "print(f\"MIXED CBOW MODEL: {mixed_cbow_model_chosen}\")\n",
    "print(f\"\\tSYNONIMITY: {mixed_cbow_models_synonymity_average[mixed_cbow_model_chosen]}\")\n",
    "mixed_cbow_models[mixed_cbow_model_chosen].save(os.path.join('mixed_models', mixed_cbow_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19d83854-f9c6-4b30-8b23-455fe772a56a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINED ALL 100-DIMENSION MODELS---ASIAN\n",
      "TRAINED ALL 300-DIMENSION MODELS---ASIAN\n",
      "TRAINED ALL 500-DIMENSION MODELS---ASIAN\n",
      "TRAINED ALL 700-DIMENSION MODELS---ASIAN\n",
      "TRAINED ALL 1000-DIMENSION MODELS---ASIAN\n"
     ]
    }
   ],
   "source": [
    "asian_cbow_models = {}\n",
    "# 100 dimensions\n",
    "cbow_w3_f1_100 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_100\"] = cbow_w3_f1_100\n",
    "cbow_w5_f1_100 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_100\"] = cbow_w5_f1_100\n",
    "cbow_w10_f1_100 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100\"] = cbow_w10_f1_100\n",
    "cbow_w3_f1_100_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_100_mc0\"] = cbow_w3_f1_100_mc0\n",
    "cbow_w5_f1_100_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_100_mc0\"] = cbow_w5_f1_100_mc0\n",
    "cbow_w10_f1_100_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_mc0\"] = cbow_w10_f1_100_mc0\n",
    "cbow_w10_f1_100_mc2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=2, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_mc2\"] = cbow_w10_f1_100_mc2\n",
    "cbow_w10_f1_100_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_neg2\"] = cbow_w10_f1_100_neg2\n",
    "cbow_w10_f1_100_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_neg5\"] = cbow_w10_f1_100_neg5\n",
    "cbow_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_ns_half_neg2\"] = cbow_w10_f1_100_ns_half_neg2\n",
    "cbow_w10_f1_100_ns_half_neg3 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_ns_half_neg3\"] = cbow_w10_f1_100_ns_half_neg3\n",
    "cbow_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_ns_half_neg5\"] = cbow_w10_f1_100_ns_half_neg5\n",
    "cbow_w10_f1_100_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_neg2_ws2\"] = cbow_w10_f1_100_neg2_ws2\n",
    "cbow_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_100_neg5_ws5\"] = cbow_w10_f1_100_neg5_ws5\n",
    "print(\"TRAINED ALL 100-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# 300 dimensions\n",
    "cbow_w3_f1_300 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_300\"] = cbow_w3_f1_300\n",
    "cbow_w5_f1_300 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_300\"] = cbow_w5_f1_300\n",
    "cbow_w10_f1_300 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300\"] = cbow_w10_f1_300\n",
    "cbow_w3_f1_300_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_300_mc0\"] = cbow_w3_f1_300_mc0\n",
    "cbow_w5_f1_300_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_300_mc0\"] = cbow_w5_f1_300_mc0\n",
    "cbow_w10_f1_300_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_mc0\"] = cbow_w10_f1_300_mc0\n",
    "cbow_w10_f1_300_mc2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=2, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_mc2\"] = cbow_w10_f1_300_mc2\n",
    "cbow_w10_f1_300_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_neg2\"] = cbow_w10_f1_300_neg2\n",
    "cbow_w10_f1_300_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_neg5\"] = cbow_w10_f1_300_neg5\n",
    "cbow_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_ns_half_neg2\"] = cbow_w10_f1_300_ns_half_neg2\n",
    "cbow_w10_f1_300_ns_half_neg3 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_ns_half_neg3\"] = cbow_w10_f1_300_ns_half_neg3\n",
    "cbow_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_ns_half_neg5\"] = cbow_w10_f1_300_ns_half_neg5\n",
    "cbow_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_neg2_ws2\"] = cbow_w10_f1_300_neg2_ws2\n",
    "cbow_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_300_neg5_ws5\"] = cbow_w10_f1_300_neg5_ws5\n",
    "print(\"TRAINED ALL 300-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# 500 dimensions\n",
    "cbow_w3_f1_500 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_500\"] = cbow_w3_f1_500\n",
    "cbow_w5_f1_500 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_500\"] = cbow_w5_f1_500\n",
    "cbow_w10_f1_500 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500\"] = cbow_w10_f1_500\n",
    "cbow_w3_f1_500_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_500_mc0\"] = cbow_w3_f1_500_mc0\n",
    "cbow_w5_f1_500_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_500_mc0\"] = cbow_w5_f1_500_mc0\n",
    "cbow_w10_f1_500_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_mc0\"] = cbow_w10_f1_500_mc0\n",
    "cbow_w10_f1_500_mc2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=2, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_mc2\"] = cbow_w10_f1_500_mc2\n",
    "cbow_w10_f1_500_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_neg2\"] = cbow_w10_f1_500_neg2\n",
    "cbow_w10_f1_500_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_neg5\"] = cbow_w10_f1_500_neg5\n",
    "cbow_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_ns_half_neg2\"] = cbow_w10_f1_500_ns_half_neg2\n",
    "cbow_w10_f1_500_ns_half_neg3 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_ns_half_neg3\"] = cbow_w10_f1_500_ns_half_neg3\n",
    "cbow_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_ns_half_neg5\"] = cbow_w10_f1_500_ns_half_neg5\n",
    "cbow_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_neg2_ws2\"] = cbow_w10_f1_500_neg2_ws2\n",
    "cbow_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_500_neg5_ws5\"] = cbow_w10_f1_500_neg5_ws5\n",
    "print(\"TRAINED ALL 500-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# 700 dimensions\n",
    "cbow_w3_f1_700 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_700\"] = cbow_w3_f1_700\n",
    "cbow_w5_f1_700 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_700\"] = cbow_w5_f1_700\n",
    "cbow_w10_f1_700 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700\"] = cbow_w10_f1_700\n",
    "cbow_w3_f1_700_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_700_mc0\"] = cbow_w3_f1_700_mc0\n",
    "cbow_w5_f1_700_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_700_mc0\"] = cbow_w5_f1_700_mc0\n",
    "cbow_w10_f1_700_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_mc0\"] = cbow_w10_f1_700_mc0\n",
    "cbow_w10_f1_700_mc2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=2, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_mc2\"] = cbow_w10_f1_700_mc2\n",
    "cbow_w10_f1_700_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_neg2\"] = cbow_w10_f1_700_neg2\n",
    "cbow_w10_f1_700_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_neg5\"] = cbow_w10_f1_700_neg5\n",
    "cbow_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_ns_half_neg2\"] = cbow_w10_f1_700_ns_half_neg2\n",
    "cbow_w10_f1_700_ns_half_neg3 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_ns_half_neg3\"] = cbow_w10_f1_700_ns_half_neg3\n",
    "cbow_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_ns_half_neg5\"] = cbow_w10_f1_700_ns_half_neg5\n",
    "cbow_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_neg2_ws2\"] = cbow_w10_f1_700_neg2_ws2\n",
    "cbow_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_700_neg5_ws5\"] = cbow_w10_f1_700_neg5_ws5\n",
    "print(\"TRAINED ALL 700-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# 1000 dimensions\n",
    "cbow_w3_f1_1000 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_1000\"] = cbow_w3_f1_1000\n",
    "cbow_w2_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w2_f1_1000_ns_half_neg5\"] = cbow_w2_f1_1000_ns_half_neg5\n",
    "cbow_w5_f1_1000 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_1000\"] = cbow_w5_f1_1000\n",
    "cbow_w10_f1_1000 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000\"] = cbow_w10_f1_1000\n",
    "cbow_w3_f1_1000_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_1000_mc0\"] = cbow_w3_f1_1000_mc0\n",
    "\n",
    "cbow_w3_f1_1000_mc0_e20 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 20)\n",
    "asian_cbow_models[\"cbow_w3_f1_1000_mc0_e20\"] = cbow_w3_f1_1000_mc0_e20\n",
    "\n",
    "cbow_w3_f1_1000_mc1 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w3_f1_1000_mc1\"] = cbow_w3_f1_1000_mc1\n",
    "\n",
    "cbow_w5_f1_1000_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w5_f1_1000_mc0\"] = cbow_w5_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_mc0\"] = cbow_w10_f1_1000_mc0\n",
    "cbow_w10_f1_1000_mc2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=2, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_mc2\"] = cbow_w10_f1_1000_mc2\n",
    "cbow_w10_f1_1000_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_neg2\"] = cbow_w10_f1_1000_neg2\n",
    "cbow_w10_f1_1000_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_neg5\"] = cbow_w10_f1_1000_neg5\n",
    "cbow_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_ns_half_neg2\"] = cbow_w10_f1_1000_ns_half_neg2\n",
    "cbow_w10_f1_1000_ns_half_neg3 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_ns_half_neg3\"] = cbow_w10_f1_1000_ns_half_neg3\n",
    "cbow_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_ns_half_neg5\"] = cbow_w10_f1_1000_ns_half_neg5\n",
    "cbow_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_neg2_ws2\"] = cbow_w10_f1_1000_neg2_ws2\n",
    "cbow_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "asian_cbow_models[\"cbow_w10_f1_1000_neg5_ws5\"] = cbow_w10_f1_1000_neg5_ws5\n",
    "print(\"TRAINED ALL 1000-DIMENSION MODELS---ASIAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2e048e3-9070-4250-8097-e2a71cb23b1f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIAN CBOW MODEL: cbow_w10_f1_1000_neg5\n",
      "\tSYNONIMITY: 0.9991270899772644\n"
     ]
    }
   ],
   "source": [
    "# asian cbow save & calculate avg\n",
    "synonyms = dict({\"family\": \"household\",\n",
    "                 \"history\": \"account\",\n",
    "                 \"treatment\": \"intervention\",\n",
    "                 \"social\": \"sociable\",\n",
    "                 \"feeling\": \"belief\",\n",
    "                 \"feeling\": \"opinion\",\n",
    "                 \"clinician\": \"doctor\",\n",
    "                 \"clinician\": \"therapist\",\n",
    "                })\n",
    "\n",
    "asian_cbow_models_synonymity_average = dict()\n",
    "asian_cbow_names = list(asian_cbow_models.keys())\n",
    "asian_cbow_vals = list(asian_cbow_models.values())\n",
    "for i in range(len(asian_cbow_names)):\n",
    "    average_synonimity = 0\n",
    "    synonymities = list()\n",
    "    for s1 in synonyms:\n",
    "        synonymities.append(asian_cbow_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "    average_synonimity = mean(synonymities)\n",
    "    asian_cbow_models_synonymity_average[asian_cbow_names[i]] = average_synonimity\n",
    "\n",
    "asian_cbow_model_chosen = max(asian_cbow_models_synonymity_average, key=asian_cbow_models_synonymity_average.get)\n",
    "print(f\"ASIAN CBOW MODEL: {asian_cbow_model_chosen}\")\n",
    "print(f\"\\tSYNONIMITY: {asian_cbow_models_synonymity_average[asian_cbow_model_chosen]}\")\n",
    "asian_cbow_models[asian_cbow_model_chosen].save(os.path.join('asian_models', asian_cbow_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ed327-84c3-4022-8891-3b1f9868fd1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# white_sg_models = {}\n",
    "# # 100 dimensions\n",
    "# sg_w3_f1_100 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_100\"] = sg_w3_f1_100\n",
    "# sg_w5_f1_100 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_100\"] = sg_w5_f1_100\n",
    "# sg_w10_f1_100 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100\"] = sg_w10_f1_100\n",
    "# sg_w3_f1_100_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_100_mc0\"] = sg_w3_f1_100_mc0\n",
    "# sg_w5_f1_100_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_100_mc0\"] = sg_w5_f1_100_mc0\n",
    "# sg_w10_f1_100_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_mc0\"] = sg_w10_f1_100_mc0\n",
    "# sg_w10_f1_100_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=100, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_mc2\"] = sg_w10_f1_100_mc2\n",
    "# sg_w10_f1_100_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_neg2\"] = sg_w10_f1_100_neg2\n",
    "# sg_w10_f1_100_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_neg5\"] = sg_w10_f1_100_neg5\n",
    "# sg_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_ns_half_neg2\"] = sg_w10_f1_100_ns_half_neg2\n",
    "# sg_w10_f1_100_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_ns_half_neg3\"] = sg_w10_f1_100_ns_half_neg3\n",
    "# sg_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_ns_half_neg5\"] = sg_w10_f1_100_ns_half_neg5\n",
    "# sg_w10_f1_100_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_neg2_ws2\"] = sg_w10_f1_100_neg2_ws2\n",
    "# sg_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_100_neg5_ws5\"] = sg_w10_f1_100_neg5_ws5\n",
    "# print(\"TRAINED ALL 100-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# # 300 dimensions\n",
    "# sg_w3_f1_300 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_300\"] = sg_w3_f1_300\n",
    "# sg_w5_f1_300 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_300\"] = sg_w5_f1_300\n",
    "# sg_w10_f1_300 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300\"] = sg_w10_f1_300\n",
    "# sg_w3_f1_300_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_300_mc0\"] = sg_w3_f1_300_mc0\n",
    "# sg_w5_f1_300_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_300_mc0\"] = sg_w5_f1_300_mc0\n",
    "# sg_w10_f1_300_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_mc0\"] = sg_w10_f1_300_mc0\n",
    "# sg_w10_f1_300_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=300, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_mc2\"] = sg_w10_f1_300_mc2\n",
    "# sg_w10_f1_300_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_neg2\"] = sg_w10_f1_300_neg2\n",
    "# sg_w10_f1_300_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_neg5\"] = sg_w10_f1_300_neg5\n",
    "# sg_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_ns_half_neg2\"] = sg_w10_f1_300_ns_half_neg2\n",
    "# sg_w10_f1_300_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_ns_half_neg3\"] = sg_w10_f1_300_ns_half_neg3\n",
    "# sg_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_ns_half_neg5\"] = sg_w10_f1_300_ns_half_neg5\n",
    "# sg_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_neg2_ws2\"] = sg_w10_f1_300_neg2_ws2\n",
    "# sg_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_300_neg5_ws5\"] = sg_w10_f1_300_neg5_ws5\n",
    "# print(\"TRAINED ALL 300-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# # 500 dimensions\n",
    "# sg_w3_f1_500 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_500\"] = sg_w3_f1_500\n",
    "# sg_w5_f1_500 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_500\"] = sg_w5_f1_500\n",
    "# sg_w10_f1_500 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500\"] = sg_w10_f1_500\n",
    "# sg_w3_f1_500_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_500_mc0\"] = sg_w3_f1_500_mc0\n",
    "# sg_w5_f1_500_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_500_mc0\"] = sg_w5_f1_500_mc0\n",
    "# sg_w10_f1_500_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_mc0\"] = sg_w10_f1_500_mc0\n",
    "# sg_w10_f1_500_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=500, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_mc2\"] = sg_w10_f1_500_mc2\n",
    "# sg_w10_f1_500_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_neg2\"] = sg_w10_f1_500_neg2\n",
    "# sg_w10_f1_500_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_neg5\"] = sg_w10_f1_500_neg5\n",
    "# sg_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_ns_half_neg2\"] = sg_w10_f1_500_ns_half_neg2\n",
    "# sg_w10_f1_500_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_ns_half_neg3\"] = sg_w10_f1_500_ns_half_neg3\n",
    "# sg_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_ns_half_neg5\"] = sg_w10_f1_500_ns_half_neg5\n",
    "# sg_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_neg2_ws2\"] = sg_w10_f1_500_neg2_ws2\n",
    "# sg_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_500_neg5_ws5\"] = sg_w10_f1_500_neg5_ws5\n",
    "# print(\"TRAINED ALL 500-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# # 700 dimensions\n",
    "# sg_w3_f1_700 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_700\"] = sg_w3_f1_700\n",
    "# sg_w5_f1_700 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_700\"] = sg_w5_f1_700\n",
    "# sg_w10_f1_700 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700\"] = sg_w10_f1_700\n",
    "# sg_w3_f1_700_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_700_mc0\"] = sg_w3_f1_700_mc0\n",
    "# sg_w5_f1_700_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_700_mc0\"] = sg_w5_f1_700_mc0\n",
    "# sg_w10_f1_700_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_mc0\"] = sg_w10_f1_700_mc0\n",
    "# sg_w10_f1_700_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=700, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_mc2\"] = sg_w10_f1_700_mc2\n",
    "# sg_w10_f1_700_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_neg2\"] = sg_w10_f1_700_neg2\n",
    "# sg_w10_f1_700_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_neg5\"] = sg_w10_f1_700_neg5\n",
    "# sg_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_ns_half_neg2\"] = sg_w10_f1_700_ns_half_neg2\n",
    "# sg_w10_f1_700_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_ns_half_neg3\"] = sg_w10_f1_700_ns_half_neg3\n",
    "# sg_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_ns_half_neg5\"] = sg_w10_f1_700_ns_half_neg5\n",
    "# sg_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_neg2_ws2\"] = sg_w10_f1_700_neg2_ws2\n",
    "# sg_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_700_neg5_ws5\"] = sg_w10_f1_700_neg5_ws5\n",
    "# print(\"TRAINED ALL 700-DIMENSION MODELS---WHITE\")\n",
    "\n",
    "# # 1000 dimensions\n",
    "# sg_w3_f1_1000 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_1000\"] = sg_w3_f1_1000\n",
    "# sg_w2_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w2_f1_1000_ns_half_neg5\"] = sg_w2_f1_1000_ns_half_neg5\n",
    "# sg_w5_f1_1000 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_1000\"] = sg_w5_f1_1000\n",
    "# sg_w10_f1_1000 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000\"] = sg_w10_f1_1000\n",
    "# sg_w3_f1_1000_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_1000_mc0\"] = sg_w3_f1_1000_mc0\n",
    "\n",
    "# sg_w3_f1_1000_mc0_e20 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 0, epochs = 20)\n",
    "# white_sg_models[\"sg_w3_f1_1000_mc0_e20\"] = sg_w3_f1_1000_mc0_e20\n",
    "\n",
    "# sg_w3_f1_1000_mc1 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w3_f1_1000_mc1\"] = sg_w3_f1_1000_mc1\n",
    "\n",
    "# sg_w5_f1_1000_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w5_f1_1000_mc0\"] = sg_w5_f1_1000_mc0\n",
    "# sg_w10_f1_1000_mc0 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_mc0\"] = sg_w10_f1_1000_mc0\n",
    "# sg_w10_f1_1000_mc2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=2, vector_size=1000, window = 10, sg = 0, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_mc2\"] = sg_w10_f1_1000_mc2\n",
    "# sg_w10_f1_1000_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_neg2\"] = sg_w10_f1_1000_neg2\n",
    "# sg_w10_f1_1000_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_neg5\"] = sg_w10_f1_1000_neg5\n",
    "# sg_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_ns_half_neg2\"] = sg_w10_f1_1000_ns_half_neg2\n",
    "# sg_w10_f1_1000_ns_half_neg3 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=3, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_ns_half_neg3\"] = sg_w10_f1_1000_ns_half_neg3\n",
    "# sg_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 0, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_ns_half_neg5\"] = sg_w10_f1_1000_ns_half_neg5\n",
    "# sg_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 0, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_neg2_ws2\"] = sg_w10_f1_1000_neg2_ws2\n",
    "# sg_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(white_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 0, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# white_sg_models[\"sg_w10_f1_1000_neg5_ws5\"] = sg_w10_f1_1000_neg5_ws5\n",
    "# print(\"TRAINED ALL 1000-DIMENSION MODELS---WHITE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cbe5a-11b6-4687-a057-dd2b8b2e15d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # white sg save & calculate avg\n",
    "# synonyms = dict({\"assessment\": \"appraisal\",\n",
    "#                  \"medication\": \"medicine\",\n",
    "#                  \"appointment\": \"engagement\",\n",
    "#                  \"road\": \"route\",\n",
    "#                  \"family\": \"household\",\n",
    "#                  \"history\": \"account\",\n",
    "#                  \"mood\": \"temper\",\n",
    "#                  \"information\": \"info\",\n",
    "#                  \"treatment\": \"intervention\",\n",
    "#                  \"social\": \"sociable\",\n",
    "#                  \"anxious\": \"nervous\",\n",
    "#                  \"confidential\": \"private\",\n",
    "#                  \"feeling\": \"belief\",\n",
    "#                  \"feeling\": \"opinion\"\n",
    "#                 })\n",
    "\n",
    "# white_sg_models_synonymity_average = {}\n",
    "# white_sg_names = list(white_sg_models.keys())\n",
    "# white_sg_vals = list(white_sg_models.values())\n",
    "# for i in range(len(white_sg_names)):\n",
    "#     average_synonimity = 0\n",
    "#     synonymities = list()\n",
    "#     for s1 in synonyms:\n",
    "#         synonymities.append(white_sg_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "#     average_synonimity = mean(synonymities)\n",
    "#     white_sg_models_synonymity_average[white_sg_names[i]] = average_synonimity\n",
    "\n",
    "# white_sg_model_chosen = max(white_sg_models_synonymity_average, key=white_sg_models_synonymity_average.get)\n",
    "# print(f\"WHITE SG MODEL: {white_sg_model_chosen}\")\n",
    "# print(f\"\\tSYNONIMITY: {white_sg_models_synonymity_average[white_sg_model_chosen]}\")\n",
    "# white_sg_models[white_sg_model_chosen].save(os.path.join('white_models', white_sg_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172fcbe-d8cd-4e9f-9367-3fd106a393c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# black_sg_models = {}\n",
    "# # 100 dimensions\n",
    "# sg_w3_f1_100 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_100\"] = sg_w3_f1_100\n",
    "# sg_w5_f1_100 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_100\"] = sg_w5_f1_100\n",
    "# sg_w10_f1_100 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_100\"] = sg_w10_f1_100\n",
    "# sg_w3_f1_100_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_100_mc0\"] = sg_w3_f1_100_mc0\n",
    "# sg_w5_f1_100_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_100_mc0\"] = sg_w5_f1_100_mc0\n",
    "# sg_w10_f1_100_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_100_mc0\"] = sg_w10_f1_100_mc0\n",
    "# sg_w10_f1_100_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_100_neg2\"] = sg_w10_f1_100_neg2\n",
    "# sg_w10_f1_100_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_100_neg5\"] = sg_w10_f1_100_neg5\n",
    "# sg_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_100_ns_half_neg2\"] = sg_w10_f1_100_ns_half_neg2\n",
    "# sg_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_100_ns_half_neg5\"] = sg_w10_f1_100_ns_half_neg5\n",
    "# sg_w2_f1_100_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w2_f1_100_neg2_ws2\"] = sg_w2_f1_100_neg2_ws2\n",
    "# sg_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_100_neg5_ws5\"] = sg_w10_f1_100_neg5_ws5\n",
    "# print(\"TRAINED ALL 100-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# # 300 dimensions\n",
    "# sg_w3_f1_300 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_300\"] = sg_w3_f1_300\n",
    "# sg_w5_f1_300 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_300\"] = sg_w5_f1_300\n",
    "# sg_w10_f1_300 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300\"] = sg_w10_f1_300\n",
    "# sg_w3_f1_300_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_300_mc0\"] = sg_w3_f1_300_mc0\n",
    "# sg_w5_f1_300_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_300_mc0\"] = sg_w5_f1_300_mc0\n",
    "# sg_w10_f1_300_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300_mc0\"] = sg_w10_f1_300_mc0\n",
    "# sg_w10_f1_300_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300_neg2\"] = sg_w10_f1_300_neg2\n",
    "# sg_w10_f1_300_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300_neg5\"] = sg_w10_f1_300_neg5\n",
    "# sg_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300_ns_half_neg2\"] = sg_w10_f1_300_ns_half_neg2\n",
    "# sg_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300_ns_half_neg5\"] = sg_w10_f1_300_ns_half_neg5\n",
    "# sg_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300_neg2_ws2\"] = sg_w10_f1_300_neg2_ws2\n",
    "# sg_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_300_neg5_ws5\"] = sg_w10_f1_300_neg5_ws5\n",
    "# print(\"TRAINED ALL 300-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# # 500 dimensions\n",
    "# sg_w3_f1_500 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_500\"] = sg_w3_f1_500\n",
    "# sg_w5_f1_500 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_500\"] = sg_w5_f1_500\n",
    "# sg_w10_f1_500 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500\"] = sg_w10_f1_500\n",
    "# sg_w3_f1_500_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_500_mc0\"] = sg_w3_f1_500_mc0\n",
    "# sg_w5_f1_500_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_500_mc0\"] = sg_w5_f1_500_mc0\n",
    "# sg_w10_f1_500_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500_mc0\"] = sg_w10_f1_500_mc0\n",
    "# sg_w10_f1_500_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500_neg2\"] = sg_w10_f1_500_neg2\n",
    "# sg_w10_f1_500_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500_neg5\"] = sg_w10_f1_500_neg5\n",
    "# sg_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500_ns_half_neg2\"] = sg_w10_f1_500_ns_half_neg2\n",
    "# sg_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500_ns_half_neg5\"] = sg_w10_f1_500_ns_half_neg5\n",
    "# sg_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500_neg2_ws2\"] = sg_w10_f1_500_neg2_ws2\n",
    "# sg_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_500_neg5_ws5\"] = sg_w10_f1_500_neg5_ws5\n",
    "# print(\"TRAINED ALL 500-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# # 700 dimensions\n",
    "# sg_w3_f1_700 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_700\"] = sg_w3_f1_700\n",
    "# sg_w5_f1_700 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_700\"] = sg_w5_f1_700\n",
    "# sg_w10_f1_700 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700\"] = sg_w10_f1_700\n",
    "# sg_w3_f1_700_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_700_mc0\"] = sg_w3_f1_700_mc0\n",
    "# sg_w5_f1_700_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_700_mc0\"] = sg_w5_f1_700_mc0\n",
    "# sg_w10_f1_700_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700_mc0\"] = sg_w10_f1_700_mc0\n",
    "# sg_w10_f1_700_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700_neg2\"] = sg_w10_f1_700_neg2\n",
    "# sg_w10_f1_700_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700_neg5\"] = sg_w10_f1_700_neg5\n",
    "# sg_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700_ns_half_neg2\"] = sg_w10_f1_700_ns_half_neg2\n",
    "# sg_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700_ns_half_neg5\"] = sg_w10_f1_700_ns_half_neg5\n",
    "# sg_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700_neg2_ws2\"] = sg_w10_f1_700_neg2_ws2\n",
    "# sg_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_700_neg5_ws5\"] = sg_w10_f1_700_neg5_ws5\n",
    "# print(\"TRAINED ALL 700-DIMENSION MODELS---BLACK\")\n",
    "\n",
    "# # 1000 dimensions\n",
    "# sg_w3_f1_1000 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_1000\"] = sg_w3_f1_1000\n",
    "# sg_w5_f1_1000 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_1000\"] = sg_w5_f1_1000\n",
    "# sg_w10_f1_1000 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000\"] = sg_w10_f1_1000\n",
    "# sg_w3_f1_1000_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w3_f1_1000_mc0\"] = sg_w3_f1_1000_mc0\n",
    "# sg_w5_f1_1000_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w5_f1_1000_mc0\"] = sg_w5_f1_1000_mc0\n",
    "# sg_w10_f1_1000_mc0 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 1, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000_mc0\"] = sg_w10_f1_1000_mc0\n",
    "# sg_w10_f1_1000_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000_neg2\"] = sg_w10_f1_1000_neg2\n",
    "# sg_w10_f1_1000_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000_neg5\"] = sg_w10_f1_1000_neg5\n",
    "# sg_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000_ns_half_neg2\"] = sg_w10_f1_1000_ns_half_neg2\n",
    "# sg_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000_ns_half_neg5\"] = sg_w10_f1_1000_ns_half_neg5\n",
    "# sg_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000_neg2_ws2\"] = sg_w10_f1_1000_neg2_ws2\n",
    "# sg_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(black_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# black_sg_models[\"sg_w10_f1_1000_neg5_ws5\"] = sg_w10_f1_1000_neg5_ws5\n",
    "# print(\"TRAINED ALL 1000-DIMENSION MODELS---BLACK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b998a-bd72-4230-af9e-431623545e11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # black sg save & calculate avg\n",
    "# synonyms = dict({\"medication\": \"medicine\",\n",
    "#                  \"appointment\": \"engagement\",\n",
    "#                  \"road\": \"route\",\n",
    "#                  \"family\": \"household\",\n",
    "#                  \"history\": \"account\",\n",
    "#                  \"mood\": \"temper\",\n",
    "#                  \"information\": \"info\",\n",
    "#                  \"treatment\": \"intervention\",\n",
    "#                  \"anxious\": \"nervous\",\n",
    "#                  \"confidential\": \"private\",\n",
    "#                  \"feeling\": \"belief\",\n",
    "#                  \"feeling\": \"opinion\"\n",
    "#                 })\n",
    "\n",
    "# black_sg_models_synonymity_average = {}\n",
    "# black_sg_names = list(black_sg_models.keys())\n",
    "# black_sg_vals = list(black_sg_models.values())\n",
    "# for i in range(len(black_sg_names)):\n",
    "#     average_synonimity = 0\n",
    "#     synonymities = list()\n",
    "#     for s1 in synonyms:\n",
    "#         synonymities.append(black_sg_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "#     average_synonimity = mean(synonymities)\n",
    "#     black_sg_models_synonymity_average[black_sg_names[i]] = average_synonimity\n",
    "\n",
    "# black_sg_model_chosen = max(black_sg_models_synonymity_average, key=black_sg_models_synonymity_average.get)\n",
    "# print(f\"BLACK SG MODEL: {black_sg_model_chosen}\")\n",
    "# print(f\"\\tSYNONIMITY: {black_sg_models_synonymity_average[black_sg_model_chosen]}\")\n",
    "# black_sg_vals[black_sg_model_chosen].save(os.path.join('black_models', black_sg_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380b59b-9707-4f01-9657-a69dde0f2eed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# mixed_sg_models = {}\n",
    "# # 100 dimensions\n",
    "# sg_w3_f1_100 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_100\"] = sg_w3_f1_100\n",
    "# sg_w5_f1_100 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_100\"] = sg_w5_f1_100\n",
    "# sg_w10_f1_100 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_100\"] = sg_w10_f1_100\n",
    "# sg_w3_f1_100_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_100_mc0\"] = sg_w3_f1_100_mc0\n",
    "# sg_w5_f1_100_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_100_mc0\"] = sg_w5_f1_100_mc0\n",
    "# sg_w10_f1_100_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_100_mc0\"] = sg_w10_f1_100_mc0\n",
    "# sg_w10_f1_100_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_100_neg2\"] = sg_w10_f1_100_neg2\n",
    "# sg_w10_f1_100_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_100_neg5\"] = sg_w10_f1_100_neg5\n",
    "# sg_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_100_ns_half_neg2\"] = sg_w10_f1_100_ns_half_neg2\n",
    "# sg_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_100_ns_half_neg5\"] = sg_w10_f1_100_ns_half_neg5\n",
    "# sg_w2_f1_100_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w2_f1_100_neg2_ws2\"] = sg_w2_f1_100_neg2_ws2\n",
    "# sg_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_100_neg5_ws5\"] = sg_w10_f1_100_neg5_ws5\n",
    "# print(\"TRAINED ALL 100-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# # 300 dimensions\n",
    "# sg_w3_f1_300 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_300\"] = sg_w3_f1_300\n",
    "# sg_w5_f1_300 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_300\"] = sg_w5_f1_300\n",
    "# sg_w10_f1_300 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300\"] = sg_w10_f1_300\n",
    "# sg_w3_f1_300_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_300_mc0\"] = sg_w3_f1_300_mc0\n",
    "# sg_w5_f1_300_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_300_mc0\"] = sg_w5_f1_300_mc0\n",
    "# sg_w10_f1_300_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300_mc0\"] = sg_w10_f1_300_mc0\n",
    "# sg_w10_f1_300_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300_neg2\"] = sg_w10_f1_300_neg2\n",
    "# sg_w10_f1_300_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300_neg5\"] = sg_w10_f1_300_neg5\n",
    "# sg_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300_ns_half_neg2\"] = sg_w10_f1_300_ns_half_neg2\n",
    "# sg_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300_ns_half_neg5\"] = sg_w10_f1_300_ns_half_neg5\n",
    "# sg_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300_neg2_ws2\"] = sg_w10_f1_300_neg2_ws2\n",
    "# sg_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_300_neg5_ws5\"] = sg_w10_f1_300_neg5_ws5\n",
    "# print(\"TRAINED ALL 300-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# # 500 dimensions\n",
    "# sg_w3_f1_500 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_500\"] = sg_w3_f1_500\n",
    "# sg_w5_f1_500 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_500\"] = sg_w5_f1_500\n",
    "# sg_w10_f1_500 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500\"] = sg_w10_f1_500\n",
    "# sg_w3_f1_500_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_500_mc0\"] = sg_w3_f1_500_mc0\n",
    "# sg_w5_f1_500_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_500_mc0\"] = sg_w5_f1_500_mc0\n",
    "# sg_w10_f1_500_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500_mc0\"] = sg_w10_f1_500_mc0\n",
    "# sg_w10_f1_500_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500_neg2\"] = sg_w10_f1_500_neg2\n",
    "# sg_w10_f1_500_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500_neg5\"] = sg_w10_f1_500_neg5\n",
    "# sg_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500_ns_half_neg2\"] = sg_w10_f1_500_ns_half_neg2\n",
    "# sg_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500_ns_half_neg5\"] = sg_w10_f1_500_ns_half_neg5\n",
    "# sg_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500_neg2_ws2\"] = sg_w10_f1_500_neg2_ws2\n",
    "# sg_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_500_neg5_ws5\"] = sg_w10_f1_500_neg5_ws5\n",
    "# print(\"TRAINED ALL 500-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# # 700 dimensions\n",
    "# sg_w3_f1_700 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_700\"] = sg_w3_f1_700\n",
    "# sg_w5_f1_700 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_700\"] = sg_w5_f1_700\n",
    "# sg_w10_f1_700 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700\"] = sg_w10_f1_700\n",
    "# sg_w3_f1_700_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_700_mc0\"] = sg_w3_f1_700_mc0\n",
    "# sg_w5_f1_700_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_700_mc0\"] = sg_w5_f1_700_mc0\n",
    "# sg_w10_f1_700_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700_mc0\"] = sg_w10_f1_700_mc0\n",
    "# sg_w10_f1_700_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700_neg2\"] = sg_w10_f1_700_neg2\n",
    "# sg_w10_f1_700_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700_neg5\"] = sg_w10_f1_700_neg5\n",
    "# sg_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700_ns_half_neg2\"] = sg_w10_f1_700_ns_half_neg2\n",
    "# sg_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700_ns_half_neg5\"] = sg_w10_f1_700_ns_half_neg5\n",
    "# sg_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700_neg2_ws2\"] = sg_w10_f1_700_neg2_ws2\n",
    "# sg_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_700_neg5_ws5\"] = sg_w10_f1_700_neg5_ws5\n",
    "# print(\"TRAINED ALL 700-DIMENSION MODELS---MIXED\")\n",
    "\n",
    "# # 1000 dimensions\n",
    "# sg_w3_f1_1000 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_1000\"] = sg_w3_f1_1000\n",
    "# sg_w5_f1_1000 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_1000\"] = sg_w5_f1_1000\n",
    "# sg_w10_f1_1000 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000\"] = sg_w10_f1_1000\n",
    "# sg_w3_f1_1000_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w3_f1_1000_mc0\"] = sg_w3_f1_1000_mc0\n",
    "# sg_w5_f1_1000_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w5_f1_1000_mc0\"] = sg_w5_f1_1000_mc0\n",
    "# sg_w10_f1_1000_mc0 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 1, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000_mc0\"] = sg_w10_f1_1000_mc0\n",
    "# sg_w10_f1_1000_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000_neg2\"] = sg_w10_f1_1000_neg2\n",
    "# sg_w10_f1_1000_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000_neg5\"] = sg_w10_f1_1000_neg5\n",
    "# sg_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000_ns_half_neg2\"] = sg_w10_f1_1000_ns_half_neg2\n",
    "# sg_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000_ns_half_neg5\"] = sg_w10_f1_1000_ns_half_neg5\n",
    "# sg_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000_neg2_ws2\"] = sg_w10_f1_1000_neg2_ws2\n",
    "# sg_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(mixed_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# mixed_sg_models[\"sg_w10_f1_1000_neg5_ws5\"] = sg_w10_f1_1000_neg5_ws5\n",
    "# print(\"TRAINED ALL 1000-DIMENSION MODELS---MIXED\")\n",
    "# print(\"FINISHED TRAINING SKIP-GRAM WORD2VEC MODELS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49085d-d107-4aca-9814-798314dccd63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # mixed sg save & calculate avg\n",
    "# synonyms = dict({\"medication\": \"medicine\",\n",
    "#                  \"appointment\": \"engagement\",\n",
    "#                  \"family\": \"household\",\n",
    "#                  \"history\": \"account\",\n",
    "#                  \"information\": \"info\",\n",
    "#                  \"treatment\": \"intervention\"\n",
    "#                 })\n",
    "\n",
    "# mixed_sg_models_synonymity_average = dict()\n",
    "# mixed_sg_names = list(mixed_sg_models.keys())\n",
    "# mixed_sg_vals = list(mixed_sg_models.values())\n",
    "# for i in range(len(mixed_sg_names)):\n",
    "#     average_synonimity = 0\n",
    "#     synonymities = list()\n",
    "#     for s1 in synonyms:\n",
    "#         synonymities.append(mixed_sg_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "#     average_synonimity = mean(synonymities)\n",
    "#     mixed_sg_models_synonymity_average[mixed_sg_names[i]] = average_synonimity\n",
    "\n",
    "# mixed_sg_model_chosen = max(mixed_sg_models_synonymity_average, key=mixed_sg_models_synonymity_average.get)\n",
    "# print(f\"MIXED SG MODEL: {mixed_sg_model_chosen}\")\n",
    "# print(f\"\\tSYNONIMITY: {mixed_sg_models_synonymity_average[mixed_sg_model_chosen]}\")\n",
    "# mixed_sg_vals[mixed_sg_model_chosen].save(os.path.join('mixed_models', mixed_sg_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b7336-930a-4a28-9c4e-f74548e20c7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# asian_sg_models = {}\n",
    "# # 100 dimensions\n",
    "# sg_w3_f1_100 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_100\"] = sg_w3_f1_100\n",
    "# sg_w5_f1_100 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_100\"] = sg_w5_f1_100\n",
    "# sg_w10_f1_100 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_100\"] = sg_w10_f1_100\n",
    "# sg_w3_f1_100_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_100_mc0\"] = sg_w3_f1_100_mc0\n",
    "# sg_w5_f1_100_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_100_mc0\"] = sg_w5_f1_100_mc0\n",
    "# sg_w10_f1_100_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=100, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_100_mc0\"] = sg_w10_f1_100_mc0\n",
    "# sg_w10_f1_100_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_100_neg2\"] = sg_w10_f1_100_neg2\n",
    "# sg_w10_f1_100_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_100_neg5\"] = sg_w10_f1_100_neg5\n",
    "# sg_w10_f1_100_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_100_ns_half_neg2\"] = sg_w10_f1_100_ns_half_neg2\n",
    "# sg_w10_f1_100_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_100_ns_half_neg5\"] = sg_w10_f1_100_ns_half_neg5\n",
    "# sg_w2_f1_100_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w2_f1_100_neg2_ws2\"] = sg_w2_f1_100_neg2_ws2\n",
    "# sg_w10_f1_100_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=100, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_100_neg5_ws5\"] = sg_w10_f1_100_neg5_ws5\n",
    "# print(\"TRAINED ALL 100-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# # 300 dimensions\n",
    "# sg_w3_f1_300 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_300\"] = sg_w3_f1_300\n",
    "# sg_w5_f1_300 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_300\"] = sg_w5_f1_300\n",
    "# sg_w10_f1_300 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300\"] = sg_w10_f1_300\n",
    "# sg_w3_f1_300_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_300_mc0\"] = sg_w3_f1_300_mc0\n",
    "# sg_w5_f1_300_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_300_mc0\"] = sg_w5_f1_300_mc0\n",
    "# sg_w10_f1_300_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=300, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300_mc0\"] = sg_w10_f1_300_mc0\n",
    "# sg_w10_f1_300_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300_neg2\"] = sg_w10_f1_300_neg2\n",
    "# sg_w10_f1_300_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300_neg5\"] = sg_w10_f1_300_neg5\n",
    "# sg_w10_f1_300_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300_ns_half_neg2\"] = sg_w10_f1_300_ns_half_neg2\n",
    "# sg_w10_f1_300_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300_ns_half_neg5\"] = sg_w10_f1_300_ns_half_neg5\n",
    "# sg_w10_f1_300_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300_neg2_ws2\"] = sg_w10_f1_300_neg2_ws2\n",
    "# sg_w10_f1_300_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=300, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_300_neg5_ws5\"] = sg_w10_f1_300_neg5_ws5\n",
    "# print(\"TRAINED ALL 300-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# # 500 dimensions\n",
    "# sg_w3_f1_500 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_500\"] = sg_w3_f1_500\n",
    "# sg_w5_f1_500 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_500\"] = sg_w5_f1_500\n",
    "# sg_w10_f1_500 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500\"] = sg_w10_f1_500\n",
    "# sg_w3_f1_500_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_500_mc0\"] = sg_w3_f1_500_mc0\n",
    "# sg_w5_f1_500_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_500_mc0\"] = sg_w5_f1_500_mc0\n",
    "# sg_w10_f1_500_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=500, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500_mc0\"] = sg_w10_f1_500_mc0\n",
    "# sg_w10_f1_500_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500_neg2\"] = sg_w10_f1_500_neg2\n",
    "# sg_w10_f1_500_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500_neg5\"] = sg_w10_f1_500_neg5\n",
    "# sg_w10_f1_500_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500_ns_half_neg2\"] = sg_w10_f1_500_ns_half_neg2\n",
    "# sg_w10_f1_500_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500_ns_half_neg5\"] = sg_w10_f1_500_ns_half_neg5\n",
    "# sg_w10_f1_500_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500_neg2_ws2\"] = sg_w10_f1_500_neg2_ws2\n",
    "# sg_w10_f1_500_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=500, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_500_neg5_ws5\"] = sg_w10_f1_500_neg5_ws5\n",
    "# print(\"TRAINED ALL 500-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# # 700 dimensions\n",
    "# sg_w3_f1_700 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_700\"] = sg_w3_f1_700\n",
    "# sg_w5_f1_700 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_700\"] = sg_w5_f1_700\n",
    "# sg_w10_f1_700 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700\"] = sg_w10_f1_700\n",
    "# sg_w3_f1_700_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_700_mc0\"] = sg_w3_f1_700_mc0\n",
    "# sg_w5_f1_700_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_700_mc0\"] = sg_w5_f1_700_mc0\n",
    "# sg_w10_f1_700_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=700, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700_mc0\"] = sg_w10_f1_700_mc0\n",
    "# sg_w10_f1_700_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700_neg2\"] = sg_w10_f1_700_neg2\n",
    "# sg_w10_f1_700_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700_neg5\"] = sg_w10_f1_700_neg5\n",
    "# sg_w10_f1_700_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700_ns_half_neg2\"] = sg_w10_f1_700_ns_half_neg2\n",
    "# sg_w10_f1_700_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700_ns_half_neg5\"] = sg_w10_f1_700_ns_half_neg5\n",
    "# sg_w10_f1_700_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700_neg2_ws2\"] = sg_w10_f1_700_neg2_ws2\n",
    "# sg_w10_f1_700_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=700, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_700_neg5_ws5\"] = sg_w10_f1_700_neg5_ws5\n",
    "# print(\"TRAINED ALL 700-DIMENSION MODELS---ASIAN\")\n",
    "\n",
    "# # 1000 dimensions\n",
    "# sg_w3_f1_1000 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_1000\"] = sg_w3_f1_1000\n",
    "# sg_w5_f1_1000 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_1000\"] = sg_w5_f1_1000\n",
    "# sg_w10_f1_1000 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000\"] = sg_w10_f1_1000\n",
    "# sg_w3_f1_1000_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 2, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w3_f1_1000_mc0\"] = sg_w3_f1_1000_mc0\n",
    "# sg_w5_f1_1000_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 3, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w5_f1_1000_mc0\"] = sg_w5_f1_1000_mc0\n",
    "# sg_w10_f1_1000_mc0 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=0, vector_size=1000, window = 10, sg = 1, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000_mc0\"] = sg_w10_f1_1000_mc0\n",
    "# sg_w10_f1_1000_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000_neg2\"] = sg_w10_f1_1000_neg2\n",
    "# sg_w10_f1_1000_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000_neg5\"] = sg_w10_f1_1000_neg5\n",
    "# sg_w10_f1_1000_ns_half_neg2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=2, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000_ns_half_neg2\"] = sg_w10_f1_1000_ns_half_neg2\n",
    "# sg_w10_f1_1000_ns_half_neg5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 10, sg = 1, negative=5, ns_exponent=0.5, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000_ns_half_neg5\"] = sg_w10_f1_1000_ns_half_neg5\n",
    "# sg_w10_f1_1000_neg2_ws2 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 2, sg = 1, negative=2, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000_neg2_ws2\"] = sg_w10_f1_1000_neg2_ws2\n",
    "# sg_w10_f1_1000_neg5_ws5 = gensim.models.Word2Vec(asian_data['lemmas_no_stop'], min_count=1, vector_size=1000, window = 3, sg = 1, negative=5, ns_exponent=0.75, epochs = 10)\n",
    "# asian_sg_models[\"sg_w10_f1_1000_neg5_ws5\"] = sg_w10_f1_1000_neg5_ws5\n",
    "# print(\"TRAINED ALL 1000-DIMENSION MODELS---ASIAN\")\n",
    "# print(\"FINISHED TRAINING SKIP-GRAM WORD2VEC MODELS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aed510-0749-4530-9b6e-954c263326ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # asian sg save & calculate avg\n",
    "# synonyms = dict({\"medication\": \"medicine\",\n",
    "#                  \"appointment\": \"engagement\",\n",
    "#                  \"road\": \"route\",\n",
    "#                  \"family\": \"household\",\n",
    "#                  \"history\": \"account\",\n",
    "#                  \"information\": \"info\",\n",
    "#                  \"treatment\": \"intervention\",\n",
    "#                  \"social\": \"sociable\",\n",
    "#                  \"confidential\": \"private\",\n",
    "#                  \"feeling\": \"belief\",\n",
    "#                  \"feeling\": \"opinion\"\n",
    "#                 })\n",
    "\n",
    "# asian_sg_models_synonymity_average = dict()\n",
    "# asian_sg_names = list(asian_sg_models.keys())\n",
    "# asian_sg_vals = list(asian_sg_models.values())\n",
    "# for i in range(len(asian_sg_names)):\n",
    "#     average_synonimity = 0\n",
    "#     synonymities = list()\n",
    "#     for s1 in synonyms:\n",
    "#         synonymities.append(asian_sg_vals[i].wv.similarity(s1, synonyms[s1]))\n",
    "#     average_synonimity = mean(synonymities)\n",
    "#     asian_sg_models_synonymity_average[asian_sg_names[i]] = average_synonimity\n",
    "\n",
    "# asian_sg_model_chosen = max(asian_sg_models_synonymity_average, key=asian_sg_models_synonymity_average.get)\n",
    "# print(f\"ASIAN SG MODEL: {asian_sg_model_chosen}\")\n",
    "# print(f\"\\tSYNONIMITY: {asian_sg_models_synonymity_average[asian_sg_model_chosen]}\")\n",
    "# asian_sg_vals[asian_sg_model_chosen].save(os.path.join('asian_models', asian_sg_model_chosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cc47558-7524-43ce-97bb-2b7ecfca17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP 1 VOCAB: Symptoms\n",
    "symptoms = ['delusional', 'manic', 'suicidal', 'delusions', 'mania', 'suicide', 'disordered', 'eating', 'insomnia', 'addiction', 'addicted', 'weight']\n",
    "\n",
    "# GROUP 2 VOCAB: Conditions\n",
    "conditions = ['pregnant', 'pregnancy', 'schizophrenia', 'depression', 'anxiety', 'schizophrenic', 'depressed', 'anxious', 'bipolar_disorder', 'bipolar', 'autism', 'autistic', 'handicapped', 'handicap', 'learning', 'diabetes', 'obese', 'obesity', 'alcoholism', 'alcohol']\n",
    "\n",
    "# GROUP 3 VOCAB: Interpersonal\n",
    "interpersonal = ['family', 'relationship', 'mother', 'father', 'husband', 'wife', 'son', 'daughter', 'child', 'parent', 'parents', 'friends', 'support', 'marriage', 'partner', 'childcare']\n",
    "\n",
    "# GROUP 4 VOCAB: Behaviour\n",
    "behaviour = ['mood', 'behaviour', 'violent', 'agitated', 'moody', 'withdrawn', 'aggressive', 'submissive', 'quiet', 'speech', 'listen', 'sleep', 'irritable']\n",
    "\n",
    "# GROUP 5 VOCAB: Clinical\n",
    "clinical = ['appointment', 'assessment', 'cancel', 'reschedule', 'treatment', 'schedule', 'intervention', 'medication', 'medicate', 'medicine', 'doctor', 'therapist', 'clinician']\n",
    "\n",
    "# GROUP 6 VOCAB: Stereotypes\n",
    "stereotypes = ['articulate', 'english', 'native', 'foreign', 'immigrant', 'immigration', 'visa', 'ethnic', 'cultural', 'culture']\n",
    "\n",
    "# GROUP 7 VOCAB: Subjective\n",
    "subjective = ['stubborn', 'frustrating', 'defiant', 'obstinate', 'unwilling', 'risk', 'personality', 'opinion', 'believe', 'seem', 'claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5c90d4b0-d087-4fb2-8ece-fa3ca6a86d20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHITE CBOW TOP VOCAB: \n",
      "WORD #0/12958 IS: Dr\n",
      "WORD #1/12958 IS: please\n",
      "WORD #2/12958 IS: appointment\n",
      "WORD #3/12958 IS: feel\n",
      "WORD #4/12958 IS: service\n",
      "WORD #5/12958 IS: time\n",
      "WORD #6/12958 IS: contact\n",
      "WORD #7/12958 IS: would\n",
      "WORD #8/12958 IS: report\n",
      "WORD #9/12958 IS: Team\n",
      "WORD #10/12958 IS: Road\n",
      "WORD #11/12958 IS: also\n",
      "WORD #12/12958 IS: see\n",
      "WORD #13/12958 IS: support\n",
      "WORD #14/12958 IS: year\n",
      "WORD #15/12958 IS: assessment\n",
      "WORD #16/12958 IS: mood\n",
      "WORD #17/12958 IS: need\n",
      "WORD #18/12958 IS: mental\n",
      "WORD #19/12958 IS: attend\n",
      "WORD #20/12958 IS: health\n",
      "WORD #21/12958 IS: Mr\n",
      "WORD #22/12958 IS: work\n",
      "WORD #23/12958 IS: GP\n",
      "WORD #24/12958 IS: take\n",
      "WORD #25/12958 IS: risk\n",
      "WORD #26/12958 IS: medication\n",
      "WORD #27/12958 IS: child\n",
      "WORD #28/12958 IS: referral\n",
      "WORD #29/12958 IS: Hospital\n",
      "WORD #30/12958 IS: sincerely\n",
      "WORD #31/12958 IS: care\n",
      "WORD #32/12958 IS: history\n",
      "WORD #33/12958 IS: number\n",
      "WORD #34/12958 IS: well\n",
      "WORD #35/12958 IS: self\n",
      "WORD #36/12958 IS: Service\n",
      "WORD #37/12958 IS: plan\n",
      "WORD #38/12958 IS: refer\n",
      "WORD #39/12958 IS: follow\n",
      "WORD #40/12958 IS: Health\n",
      "WORD #41/12958 IS: low\n",
      "WORD #42/12958 IS: make\n",
      "WORD #43/12958 IS: Assessment\n",
      "WORD #44/12958 IS: Ms\n",
      "WORD #45/12958 IS: thought\n",
      "WORD #46/12958 IS: say\n",
      "WORD #47/12958 IS: mg\n",
      "WORD #48/12958 IS: help\n",
      "\n",
      "BLACK CBOW TOP VOCAB: \n",
      "WORD #0/9417 IS: please\n",
      "WORD #1/9417 IS: Dr\n",
      "WORD #2/9417 IS: report\n",
      "WORD #3/9417 IS: feel\n",
      "WORD #4/9417 IS: appointment\n",
      "WORD #5/9417 IS: service\n",
      "WORD #6/9417 IS: Team\n",
      "WORD #7/9417 IS: would\n",
      "WORD #8/9417 IS: contact\n",
      "WORD #9/9417 IS: time\n",
      "WORD #10/9417 IS: also\n",
      "WORD #11/9417 IS: support\n",
      "WORD #12/9417 IS: say\n",
      "WORD #13/9417 IS: child\n",
      "WORD #14/9417 IS: see\n",
      "WORD #15/9417 IS: health\n",
      "WORD #16/9417 IS: mental\n",
      "WORD #17/9417 IS: mood\n",
      "WORD #18/9417 IS: Road\n",
      "WORD #19/9417 IS: Health\n",
      "WORD #20/9417 IS: need\n",
      "WORD #21/9417 IS: risk\n",
      "WORD #22/9417 IS: referral\n",
      "WORD #23/9417 IS: year\n",
      "WORD #24/9417 IS: GP\n",
      "WORD #25/9417 IS: take\n",
      "WORD #26/9417 IS: Mental\n",
      "WORD #27/9417 IS: Hospital\n",
      "WORD #28/9417 IS: medication\n",
      "WORD #29/9417 IS: care\n",
      "WORD #30/9417 IS: attend\n",
      "WORD #31/9417 IS: mother\n",
      "WORD #32/9417 IS: refer\n",
      "WORD #33/9417 IS: go\n",
      "WORD #34/9417 IS: assessment\n",
      "WORD #35/9417 IS: low\n",
      "WORD #36/9417 IS: follow\n",
      "WORD #37/9417 IS: Ms\n",
      "WORD #38/9417 IS: work\n",
      "WORD #39/9417 IS: history\n",
      "WORD #40/9417 IS: make\n",
      "WORD #41/9417 IS: number\n",
      "WORD #42/9417 IS: state\n",
      "WORD #43/9417 IS: family\n",
      "WORD #44/9417 IS: sincerely\n",
      "WORD #45/9417 IS: well\n",
      "WORD #46/9417 IS: team\n",
      "WORD #47/9417 IS: currently\n",
      "WORD #48/9417 IS: thought\n",
      "\n",
      "MIXED CBOW TOP VOCAB: \n",
      "WORD #0/1452 IS: child\n",
      "WORD #1/1452 IS: report\n",
      "WORD #2/1452 IS: feel\n",
      "WORD #3/1452 IS: support\n",
      "WORD #4/1452 IS: Health\n",
      "WORD #5/1452 IS: assessment\n",
      "WORD #6/1452 IS: please\n",
      "WORD #7/1452 IS: would\n",
      "WORD #8/1452 IS: Team\n",
      "WORD #9/1452 IS: contact\n",
      "WORD #10/1452 IS: Dr\n",
      "WORD #11/1452 IS: attend\n",
      "WORD #12/1452 IS: appointment\n",
      "WORD #13/1452 IS: thought\n",
      "WORD #14/1452 IS: service\n",
      "WORD #15/1452 IS: take\n",
      "WORD #16/1452 IS: family\n",
      "WORD #17/1452 IS: also\n",
      "WORD #18/1452 IS: need\n",
      "WORD #19/1452 IS: mental\n",
      "WORD #20/1452 IS: plan\n",
      "WORD #21/1452 IS: history\n",
      "WORD #22/1452 IS: Road\n",
      "WORD #23/1452 IS: low\n",
      "WORD #24/1452 IS: go\n",
      "WORD #25/1452 IS: risk\n",
      "WORD #26/1452 IS: health\n",
      "WORD #27/1452 IS: well\n",
      "WORD #28/1452 IS: Mental\n",
      "WORD #29/1452 IS: self\n",
      "WORD #30/1452 IS: team\n",
      "WORD #31/1452 IS: time\n",
      "WORD #32/1452 IS: say\n",
      "WORD #33/1452 IS: however\n",
      "WORD #34/1452 IS: current\n",
      "WORD #35/1452 IS: sleep\n",
      "WORD #36/1452 IS: mood\n",
      "WORD #37/1452 IS: work\n",
      "WORD #38/1452 IS: harm\n",
      "WORD #39/1452 IS: ward\n",
      "WORD #40/1452 IS: refer\n",
      "WORD #41/1452 IS: Lewisham\n",
      "WORD #42/1452 IS: GP\n",
      "WORD #43/1452 IS: Hospital\n",
      "WORD #44/1452 IS: information\n",
      "WORD #45/1452 IS: see\n",
      "WORD #46/1452 IS: number\n",
      "WORD #47/1452 IS: help\n",
      "WORD #48/1452 IS: concern\n",
      "\n",
      "ASIAN CBOW TOP VOCAB: \n",
      "WORD #0/4398 IS: Dr\n",
      "WORD #1/4398 IS: feel\n",
      "WORD #2/4398 IS: please\n",
      "WORD #3/4398 IS: mental\n",
      "WORD #4/4398 IS: also\n",
      "WORD #5/4398 IS: child\n",
      "WORD #6/4398 IS: service\n",
      "WORD #7/4398 IS: appointment\n",
      "WORD #8/4398 IS: mg\n",
      "WORD #9/4398 IS: Mr\n",
      "WORD #10/4398 IS: time\n",
      "WORD #11/4398 IS: would\n",
      "WORD #12/4398 IS: report\n",
      "WORD #13/4398 IS: see\n",
      "WORD #14/4398 IS: care\n",
      "WORD #15/4398 IS: health\n",
      "WORD #16/4398 IS: contact\n",
      "WORD #17/4398 IS: Team\n",
      "WORD #18/4398 IS: medication\n",
      "WORD #19/4398 IS: need\n",
      "WORD #20/4398 IS: Road\n",
      "WORD #21/4398 IS: week\n",
      "WORD #22/4398 IS: family\n",
      "WORD #23/4398 IS: take\n",
      "WORD #24/4398 IS: husband\n",
      "WORD #25/4398 IS: thought\n",
      "WORD #26/4398 IS: help\n",
      "WORD #27/4398 IS: day\n",
      "WORD #28/4398 IS: attend\n",
      "WORD #29/4398 IS: mood\n",
      "WORD #30/4398 IS: referral\n",
      "WORD #31/4398 IS: risk\n",
      "WORD #32/4398 IS: state\n",
      "WORD #33/4398 IS: assessment\n",
      "WORD #34/4398 IS: refer\n",
      "WORD #35/4398 IS: well\n",
      "WORD #36/4398 IS: number\n",
      "WORD #37/4398 IS: say\n",
      "WORD #38/4398 IS: go\n",
      "WORD #39/4398 IS: team\n",
      "WORD #40/4398 IS: support\n",
      "WORD #41/4398 IS: work\n",
      "WORD #42/4398 IS: harm\n",
      "WORD #43/4398 IS: year\n",
      "WORD #44/4398 IS: self\n",
      "WORD #45/4398 IS: hour\n",
      "WORD #46/4398 IS: GP\n",
      "WORD #47/4398 IS: low\n",
      "WORD #48/4398 IS: sincerely\n"
     ]
    }
   ],
   "source": [
    "print(\"WHITE CBOW TOP VOCAB: \") # white_cbow_model_chosen\n",
    "print_vocab(white_cbow_models[white_cbow_model_chosen], top_n = 50)\n",
    "# print(\"\\nWHITE SG TOP VOCAB: \")\n",
    "# print_vocab(white_sg_models[white_sg_model_chosen], top_n = 50)\n",
    "\n",
    "print(\"\\nBLACK CBOW TOP VOCAB: \")\n",
    "print_vocab(black_cbow_models[black_cbow_model_chosen], top_n = 50)\n",
    "# print(\"\\nBLACK SG TOP VOCAB: \")\n",
    "# print_vocab(black_sg_models[black_sg_model_chosen], top_n = 50)\n",
    "\n",
    "print(\"\\nMIXED CBOW TOP VOCAB: \")\n",
    "print_vocab(mixed_cbow_models[mixed_cbow_model_chosen], top_n = 50)\n",
    "# print(\"\\nMIXED SG TOP VOCAB: \")\n",
    "# print_vocab(mixed_sg_models[mixed_sg_model_chosen], top_n = 50)\n",
    "\n",
    "print(\"\\nASIAN CBOW TOP VOCAB: \")\n",
    "print_vocab(asian_cbow_models[asian_cbow_model_chosen], top_n = 50)\n",
    "# print(\"\\nASIAN SG TOP VOCAB: \")\n",
    "# print_vocab(asian_sg_models[asian_sg_model_chosen], top_n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8bdca12a-fd1d-469b-931d-70886466d31e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORD: DELUSIONAL\n",
      "[('delusion', 0.9899833798408508), ('Thoughts', 0.9892183542251587), ('unexpected', 0.9886423945426941), ('perceptual', 0.9861484169960022), ('convince', 0.9851548671722412), ('abnormal', 0.9840434789657593), ('image', 0.983711302280426), ('perception', 0.9830425381660461), ('intact', 0.9825755953788757), ('fleeting', 0.9802474975585938), ('subjectively', 0.979609489440918), ('aggressive', 0.9795891046524048), ('pertain', 0.9790951609611511), ('grossly', 0.9776877760887146), ('Childhood', 0.9775781035423279), ('regretful', 0.9771857857704163), ('paranoia', 0.9770001173019409), ('psychomotor', 0.9763065576553345), ('fed', 0.9737668037414551), ('obsession', 0.973608136177063), ('cognition', 0.9735600352287292), ('euthymic', 0.9728654026985168), ('hallucination', 0.9723251461982727), ('denie', 0.9708217978477478), ('anhedonia', 0.9706878066062927)]\n",
      "\n",
      "WORD: MANIC\n",
      "[('overdose', 0.9894253611564636), ('anxiety', 0.9888057708740234), ('spell', 0.9865015745162964), ('cocaine', 0.9849120378494263), ('functioning', 0.9838874936103821), ('stand', 0.983627200126648), ('general', 0.9825940728187561), ('chronic', 0.9821550846099854), ('symptom', 0.982040524482727), ('poor', 0.9817485213279724), ('significant', 0.981639564037323), ('adjust', 0.9812541007995605), ('pain', 0.9809674620628357), ('relate', 0.9808318614959717), ('drink', 0.980075478553772), ('observe', 0.9800745844841003), ('never', 0.9799044728279114), ('stressor', 0.9796327352523804), ('Insight', 0.9793073534965515), ('panic', 0.9791831970214844), ('consistent', 0.9789094924926758), ('escape', 0.9781464338302612), ('revolve', 0.9781099557876587), ('nevertheless', 0.9778153896331787), ('multiple', 0.9778121709823608)]\n",
      "no instance of manic found\n",
      "\n",
      "\n",
      "no instance of manic found\n",
      "\n",
      "\n",
      "\n",
      "WORD: SUICIDAL\n",
      "[('ideation', 0.9982768297195435), ('intent', 0.9957656860351562), ('cite', 0.9923304915428162), ('deny', 0.9904907941818237), ('thought', 0.9897366762161255), ('suicide', 0.9882060885429382), ('intention', 0.9872226119041443), ('protective', 0.9845187067985535), ('low', 0.9837467670440674), ('presently', 0.9799814820289612), ('worthless', 0.9771108031272888), ('self', 0.9717992544174194), ('improved', 0.9706202745437622), ('act', 0.9701476097106934), ('formal', 0.9686080813407898), ('mood', 0.9678362607955933), ('withdrawal', 0.9677945375442505), ('Impression', 0.96772700548172), ('fleeting', 0.9653435349464417), ('attempt', 0.9645476937294006), ('swing', 0.9619354605674744), ('escape', 0.9608607292175293), ('PLN', 0.9607922434806824), ('historic', 0.960750162601471), ('factor', 0.96037757396698)]\n",
      "\n",
      "WORD: DELUSIONS\n",
      "no instance of delusions found\n",
      "\n",
      "\n",
      "no instance of delusions found\n",
      "\n",
      "\n",
      "no instance of delusions found\n",
      "\n",
      "\n",
      "no instance of delusions found\n",
      "\n",
      "\n",
      "\n",
      "WORD: MANIA\n",
      "no instance of mania found\n",
      "\n",
      "\n",
      "no instance of mania found\n",
      "\n",
      "\n",
      "no instance of mania found\n",
      "\n",
      "\n",
      "\n",
      "WORD: SUICIDE\n",
      "[('cite', 0.9931370615959167), ('deny', 0.9931109547615051), ('ideation', 0.9916179776191711), ('suicidal', 0.9882059097290039), ('intent', 0.9879286885261536), ('protective', 0.9866752624511719), ('intention', 0.983260452747345), ('superficial', 0.9796317219734192), ('thought', 0.9783680438995361), ('withdrawal', 0.9773577451705933), ('formal', 0.9766992926597595), ('Impression', 0.9765498042106628), ('historic', 0.9765321016311646), ('EXAMINATION', 0.9756289720535278), ('low', 0.9742621183395386), ('act', 0.9735757112503052), ('presently', 0.9730775356292725), ('factor', 0.9727582335472107), ('alcohol', 0.9715031981468201), ('forensic', 0.9714231491088867), ('attempt', 0.9701108932495117), ('psychotic', 0.9686878323554993), ('mildly', 0.9677788615226746), ('assault', 0.9674708843231201), ('drug', 0.9672975540161133)]\n",
      "\n",
      "WORD: DISORDERED\n",
      "no instance of disordered found\n",
      "\n",
      "\n",
      "no instance of disordered found\n",
      "\n",
      "\n",
      "no instance of disordered found\n",
      "\n",
      "\n",
      "no instance of disordered found\n",
      "\n",
      "\n",
      "\n",
      "WORD: EATING\n",
      "[('breast', 0.9951406121253967), ('nausea', 0.9921257495880127), ('heroin', 0.9888512492179871), ('testicular', 0.9885569214820862), ('disturbance', 0.9882275462150574), ('fluctuation', 0.9878246188163757), ('vomiting', 0.9878222346305847), ('negatively', 0.9876652359962463), ('depress', 0.9876285195350647), ('occasional', 0.9875238537788391), ('injury', 0.9873453378677368), ('quiet', 0.9871735572814941), ('rapid', 0.9868693351745605), ('element', 0.9868571758270264), ('major', 0.986503541469574), ('heavily', 0.9864648580551147), ('stone', 0.9862445592880249), ('motivation', 0.9860573410987854), ('prominent', 0.9858400225639343), ('reaction', 0.985775887966156), ('Activation', 0.985389232635498), ('actually', 0.9852990508079529), ('reasonably', 0.9851410984992981), ('visual', 0.985001802444458), ('poorly', 0.9847586750984192)]\n",
      "\n",
      "WORD: INSOMNIA\n",
      "[('small', 0.9924135208129883), ('feed', 0.9910083413124084), ('vomiting', 0.9909257292747498), ('adverse', 0.9899359345436096), ('disagreement', 0.989448606967926), ('milk', 0.9892652630805969), ('meal', 0.9889111518859863), ('possibly', 0.9887514114379883), ('healthy', 0.9883267879486084), ('cycle', 0.9883058071136475), ('driver', 0.9878172874450684), ('contemplate', 0.9877244830131531), ('considerable', 0.9875277876853943), ('burden', 0.9872803092002869), ('reasonably', 0.9869658946990967), ('dream', 0.9869558811187744), ('vomit', 0.9864625930786133), ('minor', 0.9862340688705444), ('heavily', 0.986120879650116), ('relieve', 0.9861140251159668), ('Energy', 0.9859646558761597), ('motivate', 0.9859251976013184), ('undergo', 0.9855756759643555), ('diarrhoea', 0.9853532910346985), ('depress', 0.9853454232215881)]\n",
      "no instance of insomnia found\n",
      "\n",
      "\n",
      "\n",
      "WORD: ADDICTION\n",
      "[('harmful', 0.9852969646453857), ('suicidality', 0.9828599691390991), ('Low', 0.9821246862411499), ('withdrawal', 0.9816339015960693), ('isolation', 0.9791980981826782), ('PLN', 0.979122519493103), ('elicit', 0.9790201783180237), ('intake', 0.9782445430755615), ('spell', 0.9778486490249634), ('UCLH', 0.977050244808197), ('socially', 0.9761530160903931), ('disorder', 0.9761289954185486), ('underlying', 0.9759507179260254), ('MHLT', 0.9758362770080566), ('phobia', 0.9754258990287781), ('formal', 0.9749158620834351), ('insightful', 0.9747107028961182), ('morbid', 0.9746231436729431), ('behaviour', 0.9738596081733704), ('fluid', 0.9731167554855347), ('reported', 0.9724298119544983), ('factor', 0.9722411632537842), ('precipitate', 0.9712318778038025), ('cannabis', 0.9705622792243958), ('mild', 0.9702104926109314)]\n",
      "no instance of addiction found\n",
      "\n",
      "\n",
      "\n",
      "WORD: ADDICTED\n",
      "no instance of addicted found\n",
      "\n",
      "\n",
      "no instance of addicted found\n",
      "\n",
      "\n",
      "no instance of addicted found\n",
      "\n",
      "\n",
      "\n",
      "WORD: WEIGHT\n",
      "[('particularly', 0.9955679774284363), ('breastmilk', 0.993441641330719), ('perhaps', 0.9925010204315186), ('healthy', 0.9924080967903137), ('loss', 0.9917526245117188), ('complain', 0.9916998147964478), ('disrupt', 0.9915676712989807), ('worried', 0.9913210272789001), ('trigger', 0.9912716150283813), ('great', 0.9912662506103516), ('clearly', 0.9911237359046936), ('appear', 0.9909593462944031), ('discontinue', 0.9907238483428955), ('undergo', 0.9902881383895874), ('feeding', 0.9901522994041443), ('overall', 0.9901402592658997), ('recover', 0.9901228547096252), ('control', 0.9900736808776855), ('source', 0.9895265698432922), ('occur', 0.9894137978553772), ('tearful', 0.9892591834068298), ('picture', 0.9890843033790588), ('significantly', 0.9890434741973877), ('cause', 0.9890381693840027), ('throughout', 0.9888712167739868)]\n",
      "\n",
      "WORD: PREGNANT\n",
      "no instance of pregnant found\n",
      "\n",
      "\n",
      "\n",
      "WORD: PREGNANCY\n",
      "\n",
      "WORD: SCHIZOPHRENIA\n",
      "no instance of schizophrenia found\n",
      "\n",
      "\n",
      "\n",
      "WORD: DEPRESSION\n",
      "\n",
      "WORD: ANXIETY\n",
      "\n",
      "WORD: SCHIZOPHRENIC\n",
      "no instance of schizophrenic found\n",
      "\n",
      "\n",
      "no instance of schizophrenic found\n",
      "\n",
      "\n",
      "no instance of schizophrenic found\n",
      "\n",
      "\n",
      "\n",
      "WORD: DEPRESSED\n",
      "\n",
      "WORD: ANXIOUS\n",
      "\n",
      "WORD: BIPOLAR_DISORDER\n",
      "no instance of bipolar_disorder found\n",
      "\n",
      "\n",
      "no instance of bipolar_disorder found\n",
      "\n",
      "\n",
      "no instance of bipolar_disorder found\n",
      "\n",
      "\n",
      "no instance of bipolar_disorder found\n",
      "\n",
      "\n",
      "\n",
      "WORD: BIPOLAR\n",
      "no instance of bipolar found\n",
      "\n",
      "\n",
      "\n",
      "WORD: AUTISM\n",
      "no instance of autism found\n",
      "\n",
      "\n",
      "no instance of autism found\n",
      "\n",
      "\n",
      "\n",
      "WORD: AUTISTIC\n",
      "no instance of autistic found\n",
      "\n",
      "\n",
      "no instance of autistic found\n",
      "\n",
      "\n",
      "no instance of autistic found\n",
      "\n",
      "\n",
      "\n",
      "WORD: HANDICAPPED\n",
      "no instance of handicapped found\n",
      "\n",
      "\n",
      "no instance of handicapped found\n",
      "\n",
      "\n",
      "no instance of handicapped found\n",
      "\n",
      "\n",
      "no instance of handicapped found\n",
      "\n",
      "\n",
      "\n",
      "WORD: HANDICAP\n",
      "no instance of handicap found\n",
      "\n",
      "\n",
      "no instance of handicap found\n",
      "\n",
      "\n",
      "no instance of handicap found\n",
      "\n",
      "\n",
      "no instance of handicap found\n",
      "\n",
      "\n",
      "\n",
      "WORD: LEARNING\n",
      "\n",
      "WORD: DIABETES\n",
      "no instance of diabetes found\n",
      "\n",
      "\n",
      "\n",
      "WORD: OBESE\n",
      "no instance of obese found\n",
      "\n",
      "\n",
      "no instance of obese found\n",
      "\n",
      "\n",
      "no instance of obese found\n",
      "\n",
      "\n",
      "no instance of obese found\n",
      "\n",
      "\n",
      "\n",
      "WORD: OBESITY\n",
      "no instance of obesity found\n",
      "\n",
      "\n",
      "no instance of obesity found\n",
      "\n",
      "\n",
      "no instance of obesity found\n",
      "\n",
      "\n",
      "no instance of obesity found\n",
      "\n",
      "\n",
      "\n",
      "WORD: ALCOHOLISM\n",
      "no instance of alcoholism found\n",
      "\n",
      "\n",
      "no instance of alcoholism found\n",
      "\n",
      "\n",
      "no instance of alcoholism found\n",
      "\n",
      "\n",
      "no instance of alcoholism found\n",
      "\n",
      "\n",
      "\n",
      "WORD: ALCOHOL\n",
      "\n",
      "WORD: FAMILY\n",
      "[('parent', 0.9940630197525024), ('separate', 0.9860102534294128), ('hearing', 0.9781791567802429), ('understanding', 0.9749893546104431), ('verbal', 0.9740662574768066), ('responsibility', 0.974039614200592), ('employment', 0.9717771410942078), ('parenting', 0.9714587330818176), ('relationship', 0.9709880352020264), ('young', 0.9703620076179504), ('tense', 0.9699500203132629), ('normally', 0.9697185754776001), ('ever', 0.9689014554023743), ('employ', 0.9688127040863037), ('financial', 0.968331515789032), ('baby', 0.9680671691894531), ('partner', 0.96799635887146), ('Eastbourne', 0.967731773853302), ('adoptive', 0.9674772024154663), ('proceeding', 0.9674516916275024), ('motherhood', 0.966135561466217), ('retire', 0.9658829569816589), ('supportive', 0.9643649458885193), ('member', 0.9643086791038513), ('wall', 0.9640917181968689)]\n",
      "\n",
      "WORD: RELATIONSHIP\n",
      "[('partner', 0.9963681101799011), ('mother', 0.9944827556610107), ('sister', 0.9890130162239075), ('baby', 0.988926351070404), ('father', 0.9885748028755188), ('boy', 0.9838584065437317), ('employ', 0.983578622341156), ('daughter', 0.9825702905654907), ('business', 0.9818934798240662), ('split', 0.9817482829093933), ('degree', 0.981209933757782), ('proceeding', 0.9802138805389404), ('sibling', 0.9799150824546814), ('motherhood', 0.9798930287361145), ('natural', 0.979181170463562), ('clean', 0.9786019921302795), ('estranged', 0.9767403602600098), ('brother', 0.9766753315925598), ('abandon', 0.9766601324081421), ('husband', 0.9765214323997498), ('lonely', 0.9760034680366516), ('flat', 0.9748736023902893), ('financial', 0.9748159646987915), ('bedroom', 0.974707841873169), ('son', 0.9743874073028564)]\n",
      "\n",
      "WORD: MOTHER\n",
      "[('father', 0.994957447052002), ('relationship', 0.9944827556610107), ('partner', 0.9931725859642029), ('sister', 0.9913204312324524), ('abandon', 0.9865366816520691), ('brother', 0.9864259958267212), ('boy', 0.9861961603164673), ('baby', 0.9860159754753113), ('husband', 0.9850685596466064), ('natural', 0.9843874573707581), ('clean', 0.98367840051651), ('sibling', 0.9833522439002991), ('flat', 0.9821110367774963), ('alive', 0.9819649457931519), ('grow', 0.9814771413803101), ('split', 0.981410801410675), ('bedroom', 0.9813670516014099), ('daughter', 0.98098224401474), ('describe', 0.9797077775001526), ('accountant', 0.9797070026397705), ('stuck', 0.9792534112930298), ('business', 0.9791279435157776), ('around', 0.9790259599685669), ('Ciaran', 0.9788120985031128), ('Gavin', 0.9786583781242371)]\n",
      "\n",
      "WORD: FATHER\n",
      "[('mother', 0.9949575066566467), ('sibling', 0.9928183555603027), ('sister', 0.9920324683189392), ('brother', 0.9916887283325195), ('boy', 0.9911456108093262), ('natural', 0.9904245734214783), ('partner', 0.989185631275177), ('daughter', 0.9887923002243042), ('Portugal', 0.988628089427948), ('relationship', 0.9885746836662292), ('husband', 0.9883437156677246), ('flat', 0.9880172610282898), ('Ciaran', 0.9868085980415344), ('clean', 0.9860529899597168), ('abandon', 0.9849668741226196), ('baby', 0.9842824935913086), ('teacher', 0.9838795065879822), ('around', 0.9831805229187012), ('die', 0.9830071330070496), ('grow', 0.9827314615249634), ('ok', 0.9827235341072083), ('business', 0.9820969104766846), ('bedroom', 0.9818173050880432), ('describe', 0.9809353947639465), ('struggle', 0.9806898832321167)]\n",
      "\n",
      "WORD: HUSBAND\n",
      "[('flat', 0.9947040677070618), ('bedroom', 0.9938563108444214), ('son', 0.9905819296836853), ('die', 0.9905793070793152), ('sister', 0.9900605082511902), ('brother', 0.989791214466095), ('business', 0.9895195364952087), ('father', 0.9883436560630798), ('stressful', 0.9876844882965088), ('daughter', 0.9867467880249023), ('teacher', 0.9860849380493164), ('natural', 0.9860314726829529), ('grow', 0.9858627915382385), ('ago', 0.985760509967804), ('Ciaran', 0.9854296445846558), ('clerk', 0.9853425621986389), ('mother', 0.9850684404373169), ('death', 0.9841488003730774), ('clean', 0.9834103584289551), ('boy', 0.9829981923103333), ('Portugal', 0.9823168516159058), ('Mum', 0.9822616577148438), ('man', 0.9821875691413879), ('report', 0.9820882081985474), ('stress', 0.9819364547729492)]\n",
      "\n",
      "WORD: WIFE\n",
      "[('extent', 0.9939972758293152), ('occasion', 0.992820143699646), ('perhaps', 0.9924207925796509), ('worried', 0.992209255695343), ('recognise', 0.9914627075195312), ('occur', 0.9907076954841614), ('recover', 0.9906622767448425), ('accident', 0.9903188943862915), ('particularly', 0.9900548458099365), ('increasingly', 0.9900384545326233), ('kill', 0.989958643913269), ('ex', 0.9887673258781433), ('trigger', 0.9885653853416443), ('operatively', 0.9885573387145996), ('fact', 0.9883930683135986), ('great', 0.9883220791816711), ('furthermore', 0.9882392287254333), ('adopt', 0.9879327416419983), ('struggle', 0.9876521825790405), ('bond', 0.9874910116195679), ('cycle', 0.9871459007263184), ('emotionally', 0.986945629119873), ('weight', 0.9869059324264526), ('essentially', 0.9868708252906799), ('successful', 0.9868671298027039)]\n",
      "no instance of wife found\n",
      "\n",
      "\n",
      "\n",
      "WORD: SON\n",
      "[('death', 0.9948614835739136), ('business', 0.9923225045204163), ('teen', 0.9914149641990662), ('daughter', 0.990815281867981), ('husband', 0.9905819296836853), ('aged', 0.9895780682563782), ('bedroom', 0.9889135360717773), ('die', 0.9880384206771851), ('sister', 0.9878469705581665), ('Briden', 0.9875542521476746), ('ago', 0.9873799085617065), ('impression', 0.9873200058937073), ('essentially', 0.9866431355476379), ('split', 0.9852386713027954), ('marriage', 0.984607994556427), ('man', 0.9842761754989624), ('clerk', 0.9840117692947388), ('flat', 0.9829884171485901), ('clean', 0.982371985912323), ('sibling', 0.9819284081459045), ('stress', 0.9818990230560303), ('drive', 0.9814184904098511), ('significant', 0.9811782836914062), ('report', 0.9811694622039795), ('traumatic', 0.9810556769371033)]\n",
      "\n",
      "WORD: DAUGHTER\n",
      "[('sister', 0.9930816292762756), ('business', 0.9910809397697449), ('son', 0.9908153414726257), ('partner', 0.9891409277915955), ('die', 0.988847017288208), ('father', 0.9887923002243042), ('death', 0.9887433648109436), ('property', 0.9886292219161987), ('sibling', 0.9884777665138245), ('financial', 0.988444447517395), ('clerk', 0.9876103401184082), ('split', 0.9869018197059631), ('husband', 0.9867467284202576), ('boy', 0.986295223236084), ('struggle', 0.9859955906867981), ('proceeding', 0.9843720197677612), ('essentially', 0.9841646552085876), ('flat', 0.9837331771850586), ('Portugal', 0.9835585951805115), ('teacher', 0.9835273623466492), ('motherhood', 0.9832284450531006), ('natural', 0.983135461807251), ('aged', 0.9828115105628967), ('relationship', 0.9825701713562012), ('study', 0.9825050234794617)]\n",
      "\n",
      "WORD: CHILD\n",
      "[('young', 0.9821431040763855), ('age', 0.9711600542068481), ('personal', 0.9668330550193787), ('Attitude', 0.9613867402076721), ('arrest', 0.9572480916976929), ('unsupervise', 0.9550483822822571), ('live', 0.9522327184677124), ('disclose', 0.9520897269248962), ('Eastbourne', 0.9518311619758606), ('FAMILY', 0.9512701034545898), ('woman', 0.9479202032089233), ('etc', 0.9473450779914856), ('parent', 0.9464530944824219), ('responsibility', 0.9440796375274658), ('unborn', 0.9421489238739014), ('female', 0.9403752684593201), ('birth', 0.9403055310249329), ('problem', 0.9390101432800293), ('biographical', 0.938067615032196), ('estranged', 0.9369365572929382), ('Decline', 0.9367222785949707), ('hearing', 0.9338841438293457), ('family', 0.9335946440696716), ('caucasian', 0.9327982664108276), ('proceeding', 0.931401252746582)]\n",
      "\n",
      "WORD: PARENT\n",
      "[('family', 0.994063138961792), ('separate', 0.9862926602363586), ('responsibility', 0.9832555055618286), ('proceeding', 0.9797216653823853), ('understanding', 0.9796959161758423), ('financial', 0.9796087145805359), ('young', 0.9779858589172363), ('hearing', 0.9766322374343872), ('adoptive', 0.9765588045120239), ('employment', 0.975143551826477), ('ever', 0.9751083850860596), ('retire', 0.9744207859039307), ('Eastbourne', 0.9735501408576965), ('parenting', 0.9731070399284363), ('relationship', 0.9702383279800415), ('partner', 0.9696733951568604), ('baby', 0.9692640900611877), ('woman', 0.9684329032897949), ('behavioural', 0.9680408835411072), ('motherhood', 0.9679295420646667), ('employ', 0.9677782654762268), ('tense', 0.9677667021751404), ('link', 0.9668852686882019), ('preparation', 0.9668269753456116), ('social', 0.9665953516960144)]\n",
      "\n",
      "WORD: PARENTS\n",
      "no instance of parents found\n",
      "\n",
      "\n",
      "no instance of parents found\n",
      "\n",
      "\n",
      "no instance of parents found\n",
      "\n",
      "\n",
      "no instance of parents found\n",
      "\n",
      "\n",
      "\n",
      "WORD: FRIENDS\n",
      "no instance of friends found\n",
      "\n",
      "\n",
      "no instance of friends found\n",
      "\n",
      "\n",
      "no instance of friends found\n",
      "\n",
      "\n",
      "no instance of friends found\n",
      "\n",
      "\n",
      "\n",
      "WORD: SUPPORT\n",
      "[('need', 0.9797585010528564), ('worker', 0.9735060334205627), ('briefly', 0.9731459021568298), ('seek', 0.9727421402931213), ('access', 0.9699819684028625), ('input', 0.9694886207580566), ('ensure', 0.9682150483131409), ('appropriate', 0.9670215845108032), ('responsible', 0.9656755924224854), ('additional', 0.965045690536499), ('resource', 0.964935839176178), ('application', 0.9641185402870178), ('assistant', 0.9637144207954407), ('tomorrow', 0.9629592299461365), ('decision', 0.962909460067749), ('integration', 0.9624465107917786), ('case', 0.9622926712036133), ('specific', 0.9617938995361328), ('provision', 0.9609699845314026), ('regard', 0.9601535797119141), ('recommendation', 0.9601138234138489), ('intervention', 0.9590306282043457), ('option', 0.9585223197937012), ('liaise', 0.9584338665008545), ('provide', 0.9567200541496277)]\n",
      "\n",
      "WORD: MARRIAGE\n",
      "[('middle', 0.9941739439964294), ('childhood', 0.9925435781478882), ('stand', 0.9921616315841675), ('attack', 0.99055016040802), ('clean', 0.989332914352417), ('violent', 0.9893084764480591), ('teen', 0.9888843894004822), ('towards', 0.9878738522529602), ('isolate', 0.9874064326286316), ('grow', 0.987368643283844), ('Briden', 0.9873479604721069), ('partially', 0.9868518710136414), ('depressed', 0.9862765073776245), ('abusive', 0.9862034320831299), ('impression', 0.9860193133354187), ('lose', 0.986005961894989), ('sibling', 0.9856805205345154), ('girl', 0.98534095287323), ('inability', 0.9851976037025452), ('never', 0.9850804805755615), ('multiple', 0.9850611090660095), ('fluctuate', 0.9850455522537231), ('loss', 0.9850090742111206), ('ok', 0.9849461317062378), ('drink', 0.984739363193512)]\n",
      "no instance of marriage found\n",
      "\n",
      "\n",
      "\n",
      "WORD: PARTNER\n",
      "[('relationship', 0.9963680505752563), ('mother', 0.9931726455688477), ('sister', 0.9931698441505432), ('baby', 0.9918815493583679), ('split', 0.9893587827682495), ('father', 0.989185631275177), ('daughter', 0.989141047000885), ('boy', 0.9879574775695801), ('employ', 0.984645664691925), ('business', 0.9844698905944824), ('natural', 0.9829884171485901), ('proceeding', 0.9820142984390259), ('motherhood', 0.9819063544273376), ('husband', 0.9817243814468384), ('school', 0.9810309410095215), ('son', 0.9808395504951477), ('financial', 0.9806998372077942), ('around', 0.9801452159881592), ('separate', 0.9799818396568298), ('flat', 0.9788838624954224), ('degree', 0.9786091446876526), ('sibling', 0.9785150289535522), ('clean', 0.9783925414085388), ('death', 0.9782893657684326), ('bedroom', 0.9782421588897705)]\n",
      "\n",
      "WORD: CHILDCARE\n",
      "[('Yorkshire', 0.9258572459220886), ('Reports', 0.9253904223442078), ('gather', 0.9250044226646423), ('allegation', 0.9200435280799866), ('LSL', 0.9176906943321228), ('wet', 0.9019438624382019), ('gnal', 0.9005740284919739), ('Regent', 0.8992121815681458), ('ABH', 0.8978564739227295), ('Sinus', 0.8966923952102661), ('UGS', 0.8965058326721191), ('speaker', 0.894713282585144), ('nad', 0.8941488862037659), ('neat', 0.8909344673156738), ('Maternal', 0.890522301197052), ('albeit', 0.8902112245559692), ('Dene', 0.8894476890563965), ('supervise', 0.8892794251441956), ('QTC', 0.8806131482124329), ('intrinsic', 0.8795239329338074), ('Bakery', 0.8794059753417969), ('commute', 0.8784855008125305), ('youth', 0.8773571848869324), ('accountant', 0.8736535310745239), ('bleseed', 0.8703209757804871)]\n",
      "\n",
      "WORD: MOOD\n",
      "[('low', 0.9919478297233582), ('obvious', 0.9871595501899719), ('swing', 0.9857397675514221), ('anxiety', 0.9855453968048096), ('risky', 0.9807968139648438), ('intent', 0.9801379442214966), ('improved', 0.9799041152000427), ('dip', 0.9787133932113647), ('manic', 0.9764371514320374), ('consistent', 0.975343644618988), ('observe', 0.9740210771560669), ('whole', 0.9738996028900146), ('escape', 0.973698616027832), ('evident', 0.9730408191680908), ('thought', 0.971905529499054), ('temper', 0.9716793894767761), ('Insight', 0.9716442227363586), ('ideation', 0.9715747237205505), ('Impression', 0.971500039100647), ('symptom', 0.9713849425315857), ('never', 0.9708079099655151), ('coincide', 0.9707887172698975), ('cite', 0.9706373810768127), ('panic', 0.9701232314109802), ('deprivation', 0.9701133370399475)]\n",
      "\n",
      "WORD: BEHAVIOUR\n",
      "[('factor', 0.9924232959747314), ('exposure', 0.9909043312072754), ('sexual', 0.988741397857666), ('isolation', 0.9882611036300659), ('harmful', 0.9877246022224426), ('emotional', 0.9869245886802673), ('traffic', 0.9868891835212708), ('elicit', 0.9848676323890686), ('formal', 0.9847937226295471), ('precipitate', 0.9841744899749756), ('PLN', 0.9841130375862122), ('domestic', 0.9819161891937256), ('independence', 0.9818633794784546), ('Darren', 0.9813450574874878), ('threat', 0.9796987175941467), ('vulnerable', 0.9795094728469849), ('Low', 0.9790709018707275), ('HISTORY', 0.9790284037590027), ('girl', 0.9788906574249268), ('withdrawal', 0.9786399602890015), ('MHLT', 0.978389322757721), ('childhood', 0.9783740043640137), ('socially', 0.9782935976982117), ('lack', 0.9781444072723389), ('UCLH', 0.9772034287452698)]\n",
      "\n",
      "WORD: VIOLENT\n",
      "[('male', 0.9936526417732239), ('childhood', 0.9910553097724915), ('heart', 0.9909986853599548), ('middle', 0.9904741048812866), ('separation', 0.9895627498626709), ('towards', 0.9893963932991028), ('marriage', 0.9893085360527039), ('Dad', 0.9890800714492798), ('grandmother', 0.98853999376297), ('prison', 0.9883262515068054), ('argument', 0.9882055521011353), ('grow', 0.9878278970718384), ('abusive', 0.9876790046691895), ('physically', 0.9874772429466248), ('biological', 0.9860727787017822), ('money', 0.9860287308692932), ('isolate', 0.9860062003135681), ('beg', 0.9857750535011292), ('girlfriend', 0.985650897026062), ('threat', 0.985565721988678), ('serve', 0.9849597811698914), ('grief', 0.9845492243766785), ('wrist', 0.9844776391983032), ('exacerbate', 0.9844505190849304), ('aunt', 0.9841801524162292)]\n",
      "no instance of violent found\n",
      "\n",
      "\n",
      "\n",
      "WORD: AGITATED\n",
      "[('commit', 0.9954079985618591), ('sadness', 0.994591474533081), ('dream', 0.994347870349884), ('threaten', 0.9941588640213013), ('confused', 0.9939177632331848), ('belief', 0.9934030771255493), ('possibly', 0.9933356642723083), ('expression', 0.9933304786682129), ('guilty', 0.9927489757537842), ('word', 0.9926448464393616), ('dark', 0.9926313161849976), ('occasionally', 0.9924309849739075), ('small', 0.9920915961265564), ('blame', 0.992023229598999), ('theme', 0.9920212030410767), ('focused', 0.9915856122970581), ('front', 0.991564929485321), ('recognise', 0.9915555119514465), ('body', 0.991503894329071), ('mechanism', 0.9912314414978027), ('persecutory', 0.9911060929298401), ('pleasant', 0.9909073710441589), ('ability', 0.9908243417739868), ('challenge', 0.9907475113868713), ('unhelpful', 0.9905574321746826)]\n",
      "no instance of agitated found\n",
      "\n",
      "\n",
      "no instance of agitated found\n",
      "\n",
      "\n",
      "\n",
      "WORD: MOODY\n",
      "[('logical', 0.9676050543785095), ('Thought', 0.9582432508468628), ('taste', 0.9559973478317261), ('Perception', 0.9544291496276855), ('perceptual', 0.9488523006439209), ('grossly', 0.94704270362854), ('outbursts', 0.9433333277702332), ('spontaneity', 0.9430173635482788), ('psychomotor', 0.9391042590141296), ('coherence', 0.9381017684936523), ('Ideas', 0.9363797903060913), ('tube', 0.935249924659729), ('Abdomen', 0.9350319504737854), ('veryseverely', 0.9348974227905273), ('tv', 0.9344043135643005), ('Linear', 0.9343389868736267), ('spy', 0.9336577653884888), ('lament', 0.9331888556480408), ('Obj', 0.9330974221229553), ('joke', 0.9329745769500732), ('delusional', 0.9323490858078003), ('Content', 0.9315382838249207), ('unexpected', 0.9315353631973267), ('limb', 0.9294243454933167), ('Thoughts', 0.9293916821479797)]\n",
      "no instance of moody found\n",
      "\n",
      "\n",
      "no instance of moody found\n",
      "\n",
      "\n",
      "no instance of moody found\n",
      "\n",
      "\n",
      "\n",
      "WORD: WITHDRAWN\n",
      "[('beforehand', 0.9208171963691711), ('downstairs', 0.9185900688171387), ('emphasise', 0.9175026416778564), ('negotiate', 0.9156307578086853), ('regrettably', 0.9150896668434143), ('endeavour', 0.9122086763381958), ('translate', 0.9113630652427673), ('volunteer', 0.9100825190544128), ('organise', 0.9089876413345337), ('carefully', 0.9082164168357849), ('induction', 0.9080783724784851), ('vacancy', 0.9077677130699158), ('mumble', 0.9051230549812317), ('hence', 0.9022433161735535), ('effectively', 0.9021227359771729), ('pursue', 0.9013245105743408), ('prepare', 0.9006687998771667), ('ignore', 0.9004631638526917), ('demanding', 0.8998322486877441), ('clarify', 0.8996479511260986), ('openly', 0.8992462754249573), ('prioritise', 0.8988961577415466), ('shrug', 0.8987324237823486), ('gap', 0.8971938490867615), ('advocate', 0.8970739245414734)]\n",
      "no instance of withdrawn found\n",
      "\n",
      "\n",
      "no instance of withdrawn found\n",
      "\n",
      "\n",
      "\n",
      "WORD: AGGRESSIVE\n",
      "[('image', 0.9966635704040527), ('money', 0.9944908618927002), ('euthymic', 0.9910612106323242), ('negative', 0.9910434484481812), ('strong', 0.9910028576850891), ('objectively', 0.988691508769989), ('wrist', 0.9886077642440796), ('serve', 0.988288938999176), ('underlying', 0.9880214929580688), ('paranoia', 0.9879670143127441), ('threat', 0.9871280789375305), ('physically', 0.9868291020393372), ('visual', 0.9867945313453674), ('rumination', 0.9867299199104309), ('hallucination', 0.9865155816078186), ('mostly', 0.9864778518676758), ('cognition', 0.9864280819892883), ('suspect', 0.985958993434906), ('perception', 0.9857004284858704), ('obsession', 0.9856593012809753), ('Dad', 0.9855259656906128), ('hopelessness', 0.9853847026824951), ('anhedonia', 0.9849450588226318), ('reactive', 0.9845327138900757), ('Childhood', 0.9842287302017212)]\n",
      "no instance of aggressive found\n",
      "\n",
      "\n",
      "\n",
      "WORD: SUBMISSIVE\n",
      "no instance of submissive found\n",
      "\n",
      "\n",
      "no instance of submissive found\n",
      "\n",
      "\n",
      "no instance of submissive found\n",
      "\n",
      "\n",
      "no instance of submissive found\n",
      "\n",
      "\n",
      "\n",
      "WORD: QUIET\n",
      "[('stone', 0.9915010929107666), ('motivation', 0.988946795463562), ('nausea', 0.9887125492095947), ('flashback', 0.9875795841217041), ('appetite', 0.9875004887580872), ('concentration', 0.9872341156005859), ('body', 0.9872252345085144), ('eating', 0.9871735572814941), ('prominent', 0.9871174693107605), ('housework', 0.9870918989181519), ('regret', 0.9859727621078491), ('physically', 0.9853402972221375), ('strong', 0.9851706624031067), ('occasional', 0.9844826459884644), ('whereas', 0.9840501546859741), ('appearance', 0.9834834933280945), ('guilt', 0.9834249019622803), ('suspect', 0.9832994341850281), ('discord', 0.9828430414199829), ('Appetite', 0.9828156232833862), ('mostly', 0.9825464487075806), ('reasonably', 0.9824523329734802), ('root', 0.9822964668273926), ('loss', 0.982240617275238), ('troubled', 0.9819421172142029)]\n",
      "no instance of quiet found\n",
      "\n",
      "\n",
      "\n",
      "WORD: SPEECH\n",
      "[('tone', 0.9946715235710144), ('Speech', 0.9939325451850891), ('spontaneous', 0.9910368919372559), ('coherent', 0.9898227453231812), ('movement', 0.9857312440872192), ('helpless', 0.9805268049240112), ('mask', 0.9776972532272339), ('sound', 0.9771789908409119), ('disturb', 0.9757727980613708), ('energy', 0.9756727814674377), ('rhythm', 0.9753510355949402), ('smell', 0.9745592474937439), ('rate', 0.9735842347145081), ('relaxed', 0.9724541902542114), ('sad', 0.9712099432945251), ('Heart', 0.9697842001914978), ('lacking', 0.9691653847694397), ('abnormality', 0.9688307642936707), ('retardation', 0.9680877923965454), ('Cognition', 0.9671038389205933), ('shirt', 0.9663551449775696), ('bruise', 0.9651978015899658), ('hopelessness', 0.9636285901069641), ('hobby', 0.9628506302833557), ('psychomotor', 0.9626076817512512)]\n",
      "\n",
      "WORD: LISTEN\n",
      "[('resentful', 0.9834760427474976), ('sit', 0.9834036827087402), ('trip', 0.9805517196655273), ('stick', 0.9803088903427124), ('relax', 0.9793559312820435), ('judge', 0.979231059551239), ('exhausted', 0.9790460467338562), ('wrong', 0.9784905910491943), ('giggle', 0.9773838520050049), ('something', 0.9772936105728149), ('stain', 0.976962685585022), ('everyone', 0.9766916632652283), ('fairly', 0.976544201374054), ('frustrated', 0.9765416383743286), ('emotion', 0.9735150337219238), ('really', 0.972912073135376), ('asleep', 0.9726600050926208), ('differently', 0.9721624851226807), ('Sept', 0.9721276760101318), ('bed', 0.971459150314331), ('shake', 0.9713013768196106), ('laugh', 0.971023440361023), ('hand', 0.9708834886550903), ('scheduling', 0.9706580638885498), ('upset', 0.9703469276428223)]\n",
      "no instance of listen found\n",
      "\n",
      "\n",
      "\n",
      "WORD: SLEEP\n",
      "[('eat', 0.9910821318626404), ('disturbed', 0.9893445372581482), ('dip', 0.9877254366874695), ('bad', 0.9850688576698303), ('entirely', 0.9849802851676941), ('riding', 0.9836660623550415), ('average', 0.9819851517677307), ('Appetite', 0.9811707735061646), ('significantly', 0.9807699918746948), ('head', 0.9802678227424622), ('irritable', 0.980018675327301), ('temper', 0.9796713590621948), ('appetite', 0.9796204566955566), ('although', 0.9794375896453857), ('distract', 0.979407548904419), ('little', 0.9792513847351074), ('tend', 0.978984534740448), ('less', 0.9784379005432129), ('fatigued', 0.9784069657325745), ('though', 0.9782727956771851), ('never', 0.9775508642196655), ('life', 0.9771345853805542), ('security', 0.9765639305114746), ('cook', 0.9764009714126587), ('effort', 0.9761438965797424)]\n",
      "\n",
      "WORD: IRRITABLE\n",
      "[('appetite', 0.9894981384277344), ('Appetite', 0.9888818860054016), ('angry', 0.986860990524292), ('effort', 0.9858524203300476), ('smile', 0.9852781295776367), ('dip', 0.9844954609870911), ('security', 0.9844375848770142), ('motivation', 0.9844285845756531), ('strong', 0.984326183795929), ('obvious', 0.982752799987793), ('nevertheless', 0.9822058081626892), ('fight', 0.9820513129234314), ('objectively', 0.9818238615989685), ('average', 0.9811961650848389), ('whole', 0.9806118607521057), ('bright', 0.980414867401123), ('though', 0.9800651669502258), ('sleep', 0.980018675327301), ('urge', 0.9799895882606506), ('mostly', 0.9799038767814636), ('temper', 0.9798157811164856), ('trap', 0.979519784450531), ('terrible', 0.9792938828468323), ('concentrate', 0.9790937304496765), ('flashback', 0.9788482189178467)]\n",
      "no instance of irritable found\n",
      "\n",
      "\n",
      "\n",
      "WORD: APPOINTMENT\n",
      "BLACK CBOW: \n",
      "\n",
      "WORD: ASSESSMENT\n",
      "BLACK CBOW: \n",
      "\n",
      "WORD: CANCEL\n",
      "BLACK CBOW: \n",
      "no instance of cancel found\n",
      "\n",
      "\n",
      "\n",
      "WORD: RESCHEDULE\n",
      "BLACK CBOW: \n",
      "no instance of reschedule found\n",
      "\n",
      "\n",
      "\n",
      "WORD: TREATMENT\n",
      "BLACK CBOW: \n",
      "\n",
      "WORD: SCHEDULE\n",
      "BLACK CBOW: \n",
      "\n",
      "WORD: INTERVENTION\n",
      "BLACK CBOW: \n",
      "\n",
      "WORD: MEDICATION\n",
      "BLACK CBOW: \n",
      "\n",
      "WORD: MEDICATE\n",
      "BLACK CBOW: \n",
      "no instance of medicate found\n",
      "\n",
      "\n",
      "\n",
      "WORD: MEDICINE\n",
      "BLACK CBOW: \n",
      "no instance of medicine found\n",
      "\n",
      "\n",
      "\n",
      "WORD: DOCTOR\n",
      "BLACK CBOW: \n",
      "\n",
      "WORD: THERAPIST\n",
      "BLACK CBOW: \n",
      "no instance of therapist found\n",
      "\n",
      "\n",
      "\n",
      "WORD: CLINICIAN\n",
      "BLACK CBOW: \n",
      "no instance of clinician found\n",
      "\n",
      "\n",
      "\n",
      "WORD: ARTICULATE\n",
      "[('useless', 0.9693368673324585), ('destructive', 0.9677491784095764), ('sugary', 0.966090977191925), ('race', 0.9646081328392029), ('underweight', 0.9645174145698547), ('story', 0.9638139605522156), ('likelihood', 0.9633933305740356), ('smell', 0.9632132053375244), ('outburst', 0.9631890654563904), ('retardation', 0.9626052975654602), ('identical', 0.95969158411026), ('restore', 0.9596200585365295), ('questioning', 0.9596125483512878), ('tv', 0.9595299959182739), ('sad', 0.9592733383178711), ('deep', 0.9591575860977173), ('felt', 0.9587385058403015), ('movement', 0.9587274193763733), ('seemingly', 0.9575421214103699), ('posture', 0.9574972987174988), ('guitar', 0.9567550420761108), ('cloud', 0.9564197659492493), ('nightmare', 0.9563822746276855), ('wakening', 0.9552247524261475), ('world', 0.9548219442367554)]\n",
      "no instance of articulate found\n",
      "\n",
      "\n",
      "\n",
      "WORD: ENGLISH\n",
      "[('tutor', 0.841474711894989), ('para', 0.8274040222167969), ('increased', 0.8225483298301697), ('Alpes', 0.8065099120140076), ('Inna', 0.7874365448951721), ('Ville', 0.7855231761932373), ('region', 0.7749454379081726), ('Grand', 0.7665656208992004), ('Ugandan', 0.7661552429199219), ('french', 0.7650849223136902), ('HASSETT', 0.7598893046379089), ('La', 0.7527604699134827), ('Traumatic', 0.7498565912246704), ('madeline', 0.7489339709281921), ('Hyperkinetic', 0.7484830021858215), ('ç', 0.7478001713752747), ('Republic', 0.7442207336425781), ('Rhone', 0.7401198744773865), ('Mani', 0.7391173243522644), ('Pancholi', 0.7342362999916077), ('50years', 0.7330215573310852), ('bedwette', 0.7309842109680176), ('ABT', 0.7277080416679382), ('4bn', 0.726772665977478), ('heritage', 0.7156555652618408)]\n",
      "no instance of english found\n",
      "\n",
      "\n",
      "\n",
      "WORD: NATIVE\n",
      "[('mentor', 0.9799107313156128), ('encourage', 0.9796101450920105), ('stimulation', 0.9791690111160278), ('pertinent', 0.9769136309623718), ('eviction', 0.9754148125648499), ('ahead', 0.9747228622436523), ('app', 0.9741665720939636), ('Situation', 0.9731564521789551), ('practical', 0.9725155830383301), ('housing', 0.971350908279419), ('barrier', 0.9705223441123962), ('SHARP', 0.9695616364479065), ('midst', 0.9692867398262024), ('neither', 0.9690660834312439), ('shower', 0.9687601327896118), ('slip', 0.9687519669532776), ('limited', 0.9684687256813049), ('big', 0.9681396484375), ('addition', 0.9679600596427917), ('short', 0.9674199819564819), ('prioritise', 0.9668614268302917), ('entitlement', 0.9667562246322632), ('blow', 0.9667182564735413), ('nearby', 0.9660738706588745), ('Chelmsford', 0.9657829403877258)]\n",
      "no instance of native found\n",
      "\n",
      "\n",
      "no instance of native found\n",
      "\n",
      "\n",
      "\n",
      "WORD: FOREIGN\n",
      "no instance of foreign found\n",
      "\n",
      "\n",
      "no instance of foreign found\n",
      "\n",
      "\n",
      "no instance of foreign found\n",
      "\n",
      "\n",
      "\n",
      "WORD: IMMIGRANT\n",
      "no instance of immigrant found\n",
      "\n",
      "\n",
      "no instance of immigrant found\n",
      "\n",
      "\n",
      "no instance of immigrant found\n",
      "\n",
      "\n",
      "no instance of immigrant found\n",
      "\n",
      "\n",
      "\n",
      "WORD: IMMIGRATION\n",
      "[('Father', 0.9765193462371826), ('conviction', 0.9694895148277283), ('existing', 0.9533924460411072), ('none', 0.9519711136817932), ('learning', 0.9506889581680298), ('Sister', 0.9489091038703918), ('Admissions', 0.9481326341629028), ('neurological', 0.9448460936546326), ('parental', 0.943554162979126), ('Decline', 0.9429187774658203), ('environmental', 0.9403196573257446), ('Debts', 0.9399191737174988), ('Homelessness', 0.9390567541122437), ('offend', 0.93422931432724), ('Circumstances', 0.9334536194801331), ('xxxx', 0.9294058680534363), ('Guardian', 0.9290060997009277), ('pose', 0.928361713886261), ('Asylum', 0.9283380508422852), ('Aunt', 0.9275702238082886), ('MAPPA', 0.9236000180244446), ('seperate', 0.9197290539741516), ('rented', 0.9193152785301208), ('Veteran', 0.9190514087677002), ('hostage', 0.9175342917442322)]\n",
      "\n",
      "WORD: VISA\n",
      "[('landlord', 0.9681450128555298), ('landmark', 0.9555723071098328), ('prioritise', 0.9548066854476929), ('timing', 0.9542630314826965), ('independent', 0.9535638689994812), ('employeeclientpatientpersonal', 0.9533939957618713), ('entitle', 0.9522872567176819), ('mentor', 0.9517534375190735), ('big', 0.950977623462677), ('account', 0.9508347511291504), ('Kadin', 0.9504228830337524), ('barrier', 0.9499883055686951), ('housing', 0.9493737816810608), ('chart', 0.9473035931587219), ('accuracy', 0.9470054507255554), ('cultural', 0.9468218684196472), ('navigation', 0.9466567635536194), ('library', 0.945112943649292), ('frequently', 0.9450336694717407), ('diagnosos', 0.9449107050895691), ('stimulation', 0.9438550472259521), ('creative', 0.9434438347816467), ('formalise', 0.9431235194206238), ('gross', 0.9422852396965027), ('layer', 0.9419898390769958)]\n",
      "no instance of visa found\n",
      "\n",
      "\n",
      "no instance of visa found\n",
      "\n",
      "\n",
      "\n",
      "WORD: ETHNIC\n",
      "[('childrtn', 0.9203981757164001), ('origin', 0.9133614301681519), ('Language', 0.9067301750183105), ('Sex', 0.8674792647361755), ('Marital', 0.8669480085372925), ('mandatory', 0.8639859557151794), ('Professionals', 0.8610369563102722), ('NoSource', 0.857590913772583), ('Sibling', 0.8559646010398865), ('Loss', 0.8492231369018555), ('widow', 0.8471889495849609), ('Interpreter', 0.8416404724121094), ('census', 0.840735137462616), ('spoken', 0.8303574919700623), ('status', 0.8191012144088745), ('Method', 0.8149904608726501), ('ana', 0.808616578578949), ('T1', 0.8070017695426941), ('uh', 0.7995669841766357), ('abdomen', 0.7977533340454102), ('unr', 0.793410062789917), ('GTT', 0.7832121849060059), ('Parity', 0.7817426323890686), ('Electrician', 0.7814627289772034), ('CARER', 0.7813519239425659)]\n",
      "no instance of ethnic found\n",
      "\n",
      "\n",
      "no instance of ethnic found\n",
      "\n",
      "\n",
      "\n",
      "WORD: CULTURAL\n",
      "[('barrier', 0.9796395301818848), ('translate', 0.9756117463111877), ('library', 0.9734234809875488), ('conclusion', 0.9732241630554199), ('clarify', 0.9732042551040649), ('big', 0.9723360538482666), ('effectively', 0.9720880389213562), ('overwhelming', 0.9719706773757935), ('style', 0.9717019200325012), ('fresh', 0.9704645276069641), ('entitle', 0.9703457355499268), ('frequently', 0.9703359007835388), ('enough', 0.9700160026550293), ('shaky', 0.9697299003601074), ('downstairs', 0.9691609144210815), ('limited', 0.9691534638404846), ('face', 0.96767657995224), ('discomfort', 0.9674853682518005), ('justify', 0.9670816659927368), ('unsafe', 0.9664863348007202), ('useful', 0.9662831425666809), ('desire', 0.966122567653656), ('perspective', 0.9659132361412048), ('saving', 0.9657146334648132), ('among', 0.965566873550415)]\n",
      "no instance of cultural found\n",
      "\n",
      "\n",
      "\n",
      "WORD: CULTURE\n",
      "[('negotiate', 0.9367688298225403), ('expense', 0.9356594085693359), ('volunteer', 0.934876561164856), ('beforehand', 0.9319169521331787), ('regrettably', 0.9317375421524048), ('downstairs', 0.9316182732582092), ('translate', 0.9306316375732422), ('funeral', 0.928821325302124), ('advocate', 0.9285482168197632), ('concise', 0.9280105829238892), ('effectively', 0.9257981181144714), ('emphasise', 0.9217228889465332), ('Nandos', 0.9210367202758789), ('assure', 0.9205389618873596), ('carefully', 0.9204900860786438), ('underwear', 0.9201839566230774), ('accuracy', 0.9200818538665771), ('mumble', 0.9194868803024292), ('hesitant', 0.9184496402740479), ('shrug', 0.9171956181526184), ('clothe', 0.915588915348053), ('abrupt', 0.9148151278495789), ('prioritise', 0.9137275218963623), ('sometime', 0.9134585857391357), ('vacancy', 0.9133305549621582)]\n",
      "no instance of culture found\n",
      "\n",
      "\n",
      "\n",
      "WORD: STUBBORN\n",
      "no instance of stubborn found\n",
      "\n",
      "\n",
      "no instance of stubborn found\n",
      "\n",
      "\n",
      "no instance of stubborn found\n",
      "\n",
      "\n",
      "\n",
      "WORD: FRUSTRATING\n",
      "[('State', 0.9490572214126587), ('Appearance', 0.9485750794410706), ('universal', 0.9231655597686768), ('Examination', 0.9078212976455688), ('Maternal', 0.9061160683631897), ('Study', 0.9038587808609009), ('allegation', 0.9035352468490601), ('Previous', 0.9007375240325928), ('History', 0.8954014182090759), ('smart', 0.8946360349655151), ('Exam', 0.8929855823516846), ('MDMA', 0.8880090713500977), ('slim', 0.8844940662384033), ('Haemorrhoids', 0.8831663131713867), ('manageress', 0.8776439428329468), ('substance', 0.8764285445213318), ('Alcohol', 0.8746150732040405), ('Past', 0.8745037317276001), ('disclocation', 0.8737818598747253), ('mortgage', 0.8715248703956604), ('Circumstances', 0.8710734844207764), ('Drugs', 0.8708395957946777), ('hostel', 0.8702108263969421), ('history', 0.8693701028823853), ('manifestation', 0.869320809841156)]\n",
      "no instance of frustrating found\n",
      "\n",
      "\n",
      "no instance of frustrating found\n",
      "\n",
      "\n",
      "no instance of frustrating found\n",
      "\n",
      "\n",
      "\n",
      "WORD: DEFIANT\n",
      "no instance of defiant found\n",
      "\n",
      "\n",
      "no instance of defiant found\n",
      "\n",
      "\n",
      "no instance of defiant found\n",
      "\n",
      "\n",
      "\n",
      "WORD: OBSTINATE\n",
      "no instance of obstinate found\n",
      "\n",
      "\n",
      "no instance of obstinate found\n",
      "\n",
      "\n",
      "no instance of obstinate found\n",
      "\n",
      "\n",
      "no instance of obstinate found\n",
      "\n",
      "\n",
      "\n",
      "WORD: UNWILLING\n",
      "[('carefully', 0.9633731842041016), ('endeavour', 0.9623608589172363), ('drowsy', 0.9601472020149231), ('simply', 0.9569762349128723), ('emphasise', 0.9568336009979248), ('assure', 0.95643550157547), ('activation', 0.954241931438446), ('organise', 0.9531474709510803), ('minded', 0.9514181017875671), ('recommence', 0.9510020613670349), ('texte', 0.9495872855186462), ('subsequent', 0.947412371635437), ('translate', 0.9455254077911377), ('hesitant', 0.943419873714447), ('encourage', 0.9427511096000671), ('sometime', 0.9422405362129211), ('principle', 0.9406815767288208), ('gap', 0.9399308562278748), ('openly', 0.9387677311897278), ('clarify', 0.938490092754364), ('short', 0.9375297427177429), ('remit', 0.9365707039833069), ('distance', 0.936368465423584), ('eviction', 0.935617983341217), ('shrug', 0.9352244138717651)]\n",
      "no instance of unwilling found\n",
      "\n",
      "\n",
      "no instance of unwilling found\n",
      "\n",
      "\n",
      "\n",
      "WORD: RISK\n",
      "[('dangerous', 0.9721856713294983), ('environmental', 0.9632182121276855), ('vulnerability', 0.958717942237854), ('unborn', 0.9555711150169373), ('pose', 0.9516520500183105), ('warrant', 0.9503606557846069), ('harm', 0.9449389576911926), ('MAPPA', 0.9434583187103271), ('postpartum', 0.9425777196884155), ('etc', 0.9422991275787354), ('self', 0.9416474103927612), ('learning', 0.941378116607666), ('Risks', 0.9403015375137329), ('Hide', 0.9387089014053345), ('evidence', 0.9375304579734802), ('vague', 0.933796226978302), ('medium', 0.9334527254104614), ('violence', 0.9310716986656189), ('exploitation', 0.9293796420097351), ('indicator', 0.9211561679840088), ('Nil', 0.9208309054374695), ('aggression', 0.9206711649894714), ('achievement', 0.9204959869384766), ('factual', 0.9170511364936829), ('domestic', 0.9150817394256592)]\n",
      "\n",
      "WORD: PERSONALITY\n",
      "[('diagnose', 0.9899123311042786), ('disorder', 0.9889276027679443), ('background', 0.9843658208847046), ('PTSD', 0.9834790229797363), ('epilepsy', 0.9802495837211609), ('suffer', 0.9797006249427795), ('borderline', 0.9795534610748291), ('agoraphobia', 0.9792706966400146), ('stroke', 0.9789896011352539), ('dependence', 0.9781869649887085), ('suicidality', 0.977424681186676), ('feature', 0.9771581888198853), ('functioning', 0.9768946170806885), ('socially', 0.9761151671409607), ('morbid', 0.9761127829551697), ('mild', 0.9757276773452759), ('menopausal', 0.9742006659507751), ('cannabis', 0.9739158749580383), ('overdose', 0.9736424684524536), ('forensic', 0.9733415842056274), ('depression', 0.9726365208625793), ('arthritis', 0.9725834727287292), ('trauma', 0.9725682139396667), ('recurrent', 0.9718297123908997), ('psychotic', 0.9718281626701355)]\n",
      "\n",
      "WORD: OPINION\n",
      "[('enquire', 0.9866542816162109), ('ms', 0.9846842288970947), ('early', 0.9837794303894043), ('formulate', 0.9824503660202026), ('whilst', 0.9819972515106201), ('long', 0.9810390472412109), ('stage', 0.9809739589691162), ('however', 0.977438747882843), ('ward', 0.9770020842552185), ('initially', 0.9754860997200012), ('keen', 0.9729340076446533), ('regularly', 0.9728710055351257), ('inclusion', 0.9728184938430786), ('happy', 0.972739577293396), ('PA', 0.9718273282051086), ('explain', 0.9716356992721558), ('end', 0.9716005921363831), ('careful', 0.971104085445404), ('upcoming', 0.9708254337310791), ('six', 0.9706314206123352), ('begin', 0.9702632427215576), ('remember', 0.970207691192627), ('finish', 0.9693672060966492), ('transport', 0.969285786151886), ('decide', 0.9686408638954163)]\n",
      "no instance of opinion found\n",
      "\n",
      "\n",
      "\n",
      "WORD: BELIEVE\n",
      "[('positive', 0.9991530776023865), ('moment', 0.9965777397155762), ('worry', 0.9961215853691101), ('meditation', 0.9960269927978516), ('clear', 0.9952438473701477), ('bond', 0.9950656890869141), ('seem', 0.9948503375053406), ('activity', 0.9948254227638245), ('build', 0.9944844245910645), ('situation', 0.9939992427825928), ('less', 0.9939862489700317), ('deal', 0.9937589764595032), ('express', 0.9936974048614502), ('debt', 0.9930437803268433), ('particularly', 0.993038535118103), ('mean', 0.9928841590881348), ('properly', 0.9927529096603394), ('great', 0.9927265048027039), ('absence', 0.9927029609680176), ('extent', 0.9926024079322815), ('fact', 0.9923993945121765), ('become', 0.9920803904533386), ('struggle', 0.9920660853385925), ('occasion', 0.9918318390846252), ('quickly', 0.9916646480560303)]\n",
      "\n",
      "WORD: SEEM\n",
      "[('less', 0.9972625970840454), ('tearful', 0.9960810542106628), ('little', 0.9957805871963501), ('meditation', 0.9956659078598022), ('believe', 0.9948503375053406), ('significantly', 0.9948201179504395), ('particularly', 0.9945243000984192), ('cause', 0.994045615196228), ('become', 0.9938659071922302), ('positive', 0.993854820728302), ('overall', 0.9937812685966492), ('tend', 0.9937728643417358), ('though', 0.9936673641204834), ('extremely', 0.9930110573768616), ('wear', 0.9929267168045044), ('throughout', 0.9928974509239197), ('worry', 0.9928451180458069), ('occasion', 0.9928014874458313), ('pattern', 0.9926710724830627), ('great', 0.9925515651702881), ('clear', 0.9924693703651428), ('eat', 0.9924682974815369), ('head', 0.9923824667930603), ('extent', 0.9923529028892517), ('fact', 0.9920665621757507)]\n",
      "\n",
      "WORD: CLAIM\n",
      "[('PIP', 0.9949132204055786), ('within', 0.9875674843788147), ('pende', 0.983916699886322), ('inpatient', 0.9827841520309448), ('accommodation', 0.980364203453064), ('save', 0.9790937304496765), ('view', 0.9785270690917969), ('legal', 0.9781607389450073), ('seek', 0.9781580567359924), ('aspiration', 0.9775980710983276), ('role', 0.9771425724029541), ('brief', 0.9770926237106323), ('already', 0.9753795862197876), ('route', 0.9748888611793518), ('comment', 0.9744191765785217), ('application', 0.972408652305603), ('person', 0.9715835452079773), ('Mention', 0.9707461595535278), ('concern', 0.9689462184906006), ('involve', 0.9688912630081177), ('complex', 0.9681316018104553), ('engagement', 0.9677672982215881), ('wellbeing', 0.9675933122634888), ('share', 0.9673689603805542), ('refuse', 0.9671205282211304)]\n",
      "no instance of claim found\n",
      "\n",
      "\n",
      "LENGTH OF DICTIONARY: 367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75993"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = {}\n",
    "for word in symptoms:\n",
    "    print(f\"\\nWORD: {word.upper()}\")\n",
    "    word_file = open(f\"results/symptoms_25/{word}.txt\", \"a\")\n",
    "    try:\n",
    "        key = f\"{word}_white_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------WHITE (CBOW)---------\\n\")\n",
    "        print(white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25))\n",
    "        for entry in white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_black_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------BLACK (CBOW)---------\\n\")\n",
    "        for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_mixed_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------MIXED (CBOW)---------\\n\")\n",
    "        for entry in mixed_cbow_models[mixed_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_asian_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------ASIAN (CBOW)---------\\n\")\n",
    "        for entry in asian_cbow_models[asian_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_white_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------WHITE (SG)---------\\n\")\n",
    "        # for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_black_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------BLACK (SG)---------\\n\")\n",
    "        # for entry in black_sg_models[black_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_mixed_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------MIXED (SG)---------\\n\")\n",
    "        # for entry in mixed_sg_models[mixed_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_asian_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------ASIAN (SG)---------\\n\")\n",
    "        # for entry in asian_sg_models[asian_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "        # word_file.close()\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "for word in conditions:\n",
    "    print(f\"\\nWORD: {word.upper()}\")\n",
    "    word_file = open(f\"results/conditions_25/{word}.txt\", \"a\")\n",
    "    try:\n",
    "        key = f\"{word}_white_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------WHITE (CBOW)---------\\n\")\n",
    "        for entry in white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_black_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------BLACK (CBOW)---------\\n\")\n",
    "        for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_mixed_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------MIXED (CBOW)---------\\n\")\n",
    "        for entry in mixed_cbow_models[mixed_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_asian_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------ASIAN (CBOW)---------\\n\")\n",
    "        for entry in asian_cbow_models[asian_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_white_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------WHITE (SG)---------\\n\")\n",
    "        # for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_black_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------BLACK (SG)---------\\n\")\n",
    "        # for entry in black_sg_models[black_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_mixed_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------MIXED (SG)---------\\n\")\n",
    "        # for entry in mixed_sg_models[mixed_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_asian_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------ASIAN (SG)---------\\n\")\n",
    "        # for entry in asian_sg_models[asian_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "        # word_file.close()\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "for word in interpersonal:\n",
    "    print(f\"\\nWORD: {word.upper()}\")\n",
    "    word_file = open(f\"results/interpersonal_25/{word}.txt\", \"a\")\n",
    "    try:\n",
    "        key = f\"{word}_white_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------WHITE (CBOW)---------\\n\")\n",
    "        print(white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25))\n",
    "        for entry in white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_black_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------BLACK (CBOW)---------\\n\")\n",
    "        for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_mixed_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------MIXED (CBOW)---------\\n\")\n",
    "        for entry in mixed_cbow_models[mixed_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_asian_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------ASIAN (CBOW)---------\\n\")\n",
    "        for entry in asian_cbow_models[asian_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_white_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------WHITE (SG)---------\\n\")\n",
    "        # for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_black_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------BLACK (SG)---------\\n\")\n",
    "        # for entry in black_sg_models[black_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_mixed_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------MIXED (SG)---------\\n\")\n",
    "        # for entry in mixed_sg_models[mixed_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_asian_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------ASIAN (SG)---------\\n\")\n",
    "        # for entry in asian_sg_models[asian_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "        # word_file.close()\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "for word in behaviour:\n",
    "    print(f\"\\nWORD: {word.upper()}\")\n",
    "    word_file = open(f\"results/behaviour_25/{word}.txt\", \"a\")\n",
    "    try:\n",
    "        key = f\"{word}_white_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------WHITE (CBOW)---------\\n\")\n",
    "        print(white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25))\n",
    "        for entry in white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_black_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------BLACK (CBOW)---------\\n\")\n",
    "        for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_mixed_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------MIXED (CBOW)---------\\n\")\n",
    "        for entry in mixed_cbow_models[mixed_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_asian_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------ASIAN (CBOW)---------\\n\")\n",
    "        for entry in asian_cbow_models[asian_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_white_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------WHITE (SG)---------\\n\")\n",
    "        # for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_black_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------BLACK (SG)---------\\n\")\n",
    "        # for entry in black_sg_models[black_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_mixed_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------MIXED (SG)---------\\n\")\n",
    "        # for entry in mixed_sg_models[mixed_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_asian_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------ASIAN (SG)---------\\n\")\n",
    "        # for entry in asian_sg_models[asian_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "        # word_file.close()\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "for word in clinical:\n",
    "    print(f\"\\nWORD: {word.upper()}\")\n",
    "    word_file = open(f\"results/clinical_25/{word}.txt\", \"a\")\n",
    "    try:\n",
    "        key = f\"{word}_white_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------WHITE (CBOW)---------\\n\")\n",
    "        for entry in white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        print(\"BLACK CBOW: \")\n",
    "        word_file.write(\"\\n---------BLACK (CBOW)---------\\n\")\n",
    "        for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_mixed_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------MIXED (CBOW)---------\\n\")\n",
    "        for entry in mixed_cbow_models[mixed_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_asian_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------ASIAN (CBOW)---------\\n\")\n",
    "        for entry in asian_cbow_models[asian_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_white_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------WHITE (SG)---------\\n\")\n",
    "        # for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_black_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------BLACK (SG)---------\\n\")\n",
    "        # for entry in black_sg_models[black_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_mixed_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------MIXED (SG)---------\\n\")\n",
    "        # for entry in mixed_sg_models[mixed_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_asian_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------ASIAN (SG)---------\\n\")\n",
    "        # for entry in asian_sg_models[asian_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "        # word_file.close()\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "for word in stereotypes:\n",
    "    print(f\"\\nWORD: {word.upper()}\")\n",
    "    word_file = open(f\"results/stereotypes_25/{word}.txt\", \"a\")\n",
    "    try:\n",
    "        key = f\"{word}_white_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------WHITE (CBOW)---------\\n\")\n",
    "        print(white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25))\n",
    "        for entry in white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_black_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------BLACK (CBOW)---------\\n\")\n",
    "        for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_mixed_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------MIXED (CBOW)---------\\n\")\n",
    "        for entry in mixed_cbow_models[mixed_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_asian_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------ASIAN (CBOW)---------\\n\")\n",
    "        for entry in asian_cbow_models[asian_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_white_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------WHITE (SG)---------\\n\")\n",
    "        # for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_black_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------BLACK (SG)---------\\n\")\n",
    "        # for entry in black_sg_models[black_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_mixed_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------MIXED (SG)---------\\n\")\n",
    "        # for entry in mixed_sg_models[mixed_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_asian_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------ASIAN (SG)---------\\n\")\n",
    "        # for entry in asian_sg_models[asian_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "        # word_file.close()\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "for word in subjective:\n",
    "    print(f\"\\nWORD: {word.upper()}\")\n",
    "    word_file = open(f\"results/subjective_25/{word}.txt\", \"a\")\n",
    "    try:\n",
    "        key = f\"{word}_white_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------WHITE (CBOW)---------\\n\")\n",
    "        print(white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25))\n",
    "        for entry in white_cbow_models[white_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_black_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------BLACK (CBOW)---------\\n\")\n",
    "        for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_mixed_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"\\n---------MIXED (CBOW)---------\\n\")\n",
    "        for entry in mixed_cbow_models[mixed_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        key = f\"{word}_asian_cbow\"\n",
    "        word_embeddings[key] = []\n",
    "        word_file.write(\"---------ASIAN (CBOW)---------\\n\")\n",
    "        for entry in asian_cbow_models[asian_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            try:\n",
    "                word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                word_embeddings[key].append(entry[0].lower())\n",
    "            except Exception as error:\n",
    "                print(f\"Could not write to file: {error}\")\n",
    "        word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    except Exception as error:\n",
    "        print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_white_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------WHITE (SG)---------\\n\")\n",
    "        # for entry in black_cbow_models[black_cbow_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_black_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------BLACK (SG)---------\\n\")\n",
    "        # for entry in black_sg_models[black_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_mixed_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------MIXED (SG)---------\\n\")\n",
    "        # for entry in mixed_sg_models[mixed_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "    # try:\n",
    "        # key = f\"{word}_asian_sg\"\n",
    "        # word_embeddings[key] = []\n",
    "        # word_file.write(\"\\n---------ASIAN (SG)---------\\n\")\n",
    "        # for entry in asian_sg_models[asian_sg_model_chosen].wv.similar_by_word(word, 25):\n",
    "            # try:\n",
    "                # word_file.write(f\"\\nWORD: {entry[0].lower()}\\tSCORE: {entry[1]}\")\n",
    "                # word_embeddings[key].append(entry[0].lower())\n",
    "            # except Exception as error:\n",
    "                # print(f\"Could not write to file: {error}\")\n",
    "        # word_file.write(\"\\n----------------------------------------------\\n\")\n",
    "        # word_file.close()\n",
    "    # except Exception as error:\n",
    "        # print(f\"no instance of {word} found\\n\\n\")\n",
    "\n",
    "print(f\"LENGTH OF DICTIONARY: {len(word_embeddings.keys())}\")\n",
    "embeddings_file = open(f\"results/word_embeddings_15.txt\", \"w\")\n",
    "embeddings_file.write(json.dumps(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a1ebf99-c4d9-4d36-a7ac-70d3385feccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['brcid', 'Gender', 'dob', 'Ethnicity', 'Date_Of_Death', 'Age',\n",
      "       'Diagnosis_Date', 'Document_Date', 'unique_id', 'Date',\n",
      "       'Patient_Summary', 'cn_doc_id', 'Summary', 'BRCID',\n",
      "       'Patient_Summary_sp', 'tokens', 'lemmas', 'lemmas_no_stop'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# concordances\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "print(white_data.columns)\n",
    "white_summaries = []\n",
    "for i in range(white_data.shape[0]):\n",
    "    toks = (\" \".join([t.lower() for t in white_data['tokens'].iloc[i]])).split(\" \")\n",
    "    # print(toks[:10])\n",
    "    white_summaries.extend(toks)\n",
    "white_text = Text(white_summaries)\n",
    "for word in symptoms:\n",
    "    curr_list = white_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/white_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in subjective:\n",
    "    curr_list = white_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/white_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in stereotypes:\n",
    "    curr_list = white_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/white_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in interpersonal:\n",
    "    curr_list = white_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/white_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in conditions:\n",
    "    curr_list = white_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/white_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in clinical:\n",
    "    curr_list = white_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/white_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in behaviour:\n",
    "    curr_list = white_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/white_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "\n",
    "black_summaries = []\n",
    "for i in range(black_data.shape[0]):\n",
    "    toks = (\" \".join([t.lower() for t in black_data['tokens'].iloc[i]])).split(\" \")\n",
    "    # print(toks[:10])\n",
    "    black_summaries.extend(toks)\n",
    "black_text = Text(black_summaries)\n",
    "for word in symptoms:\n",
    "    curr_list = black_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/black_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in subjective:\n",
    "    curr_list = black_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/black_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in stereotypes:\n",
    "    curr_list = black_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/black_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in interpersonal:\n",
    "    curr_list = black_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/black_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in conditions:\n",
    "    curr_list = black_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/black_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in clinical:\n",
    "    curr_list = black_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/black_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in behaviour:\n",
    "    curr_list = black_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/black_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "\n",
    "asian_summaries = []\n",
    "for i in range(asian_data.shape[0]):\n",
    "    toks = (\" \".join([t.lower() for t in asian_data['tokens'].iloc[i]])).split(\" \")\n",
    "    # print(toks[:10])\n",
    "    asian_summaries.extend(toks)\n",
    "asian_text = Text(asian_summaries)\n",
    "for word in symptoms:\n",
    "    curr_list = asian_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/asian_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in subjective:\n",
    "    curr_list = asian_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/asian_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in stereotypes:\n",
    "    curr_list = asian_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/asian_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in interpersonal:\n",
    "    curr_list = asian_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/asian_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in conditions:\n",
    "    curr_list = asian_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/asian_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in clinical:\n",
    "    curr_list = asian_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/asian_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in behaviour:\n",
    "    curr_list = asian_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/asian_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "\n",
    "mixed_summaries = []\n",
    "for i in range(mixed_data.shape[0]):\n",
    "    toks = (\" \".join([t.lower() for t in mixed_data['tokens'].iloc[i]])).split(\" \")\n",
    "    # print(toks[:10])\n",
    "    mixed_summaries.extend(toks)\n",
    "mixed_text = Text(mixed_summaries)\n",
    "for word in symptoms:\n",
    "    curr_list = mixed_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/mixed_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in subjective:\n",
    "    curr_list = mixed_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/mixed_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in stereotypes:\n",
    "    curr_list = mixed_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/mixed_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in interpersonal:\n",
    "    curr_list = mixed_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/mixed_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in conditions:\n",
    "    curr_list = mixed_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/mixed_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in clinical:\n",
    "    curr_list = mixed_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/mixed_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()\n",
    "for word in behaviour:\n",
    "    curr_list = mixed_text.concordance_list(word)\n",
    "    word_file = open(f\"concordances/mixed_{word}.txt\", \"a\")\n",
    "    for entry in curr_list:\n",
    "        word_file.write(str(entry))\n",
    "    word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8aff2-54a8-4a68-a7c6-d2414973dd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0e1ff-676d-4616-a7e6-513c9077b9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
